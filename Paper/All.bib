
@incollection{2015737,
  title = {Bibliography},
  booktitle = {Doing {{Bayesian Data Analysis}} ({{Second Edition}})},
  editor = {Kruschke, John K.},
  year = {2015},
  edition = {Second Edition},
  pages = {737--745},
  publisher = {{Academic Press}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-405888-0.10000-5},
  isbn = {978-0-12-405888-0}
}

@article{abadiTensorFlowLargeScaleMachine2016,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  month = mar,
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  archivePrefix = {arXiv},
  eprint = {1603.04467},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F9K265EC\\Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf;C\:\\Users\\devan\\Zotero\\storage\\S2TSQUSM\\Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf},
  journal = {arXiv:1603.04467 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@misc{Adler2018,
  title = {Rgl: {{3D Visualization Using OpenGL}}},
  author = {Adler, Daniel and Murdoch, Duncan},
  year = {2018}
}

@article{ainsworthApproximateInferenceDisease2006,
  title = {Approximate Inference for Disease Mapping},
  author = {Ainsworth, L.M. and Dean, C. B.},
  year = {2006},
  month = jun,
  volume = {50},
  pages = {2552--2570},
  issn = {01679473},
  doi = {10.1016/j.csda.2005.05.001},
  abstract = {Disease mapping is an important area of statistical research. Contributions to the area over the last twenty years have been instrumental in helping to pinpoint potential causes of mortality and to provide a strategy for effective allocation of health funding. Because of the complexity of spatial analyses, new developments in methodology have not generally found application at Vital Statistics agencies. Inference for spatio-temporal analyses remains computationally prohibitive, for routine preparation of mortality atlases. This paper considers whether approximate methods of inference are reliable for mapping studies, especially in terms of providing accurate estimates of relative risks, ranks of regions and standard errors of risks. These approximate methods lie in the broader realm of approximate inference for generalized linear mixed models. Penalized quasi-likelihood is specifically considered here. The main focus is on assessing how close the penalized quasi-likelihood estimates are to target values, by comparison with the more rigorous and widespread Bayesian Markov Chain Monte Carlo methods. No previous studies have compared these two methods. The quantities of prime interest are small-area relative risks and the estimated ranks of the risks which are often used for ordering the regions. It will be shown that penalized quasi-likelihood is a reasonably accurate method of inference and can be recommended as a simple, yet quite precise method for initial exploratory studies.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MRPPVSXC\\Ainsworth and Dean - 2006 - Approximate inference for disease mapping.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {10}
}

@article{ainsworthDetectionLocalGlobal2008,
  title = {Detection of Local and Global Outliers in Mapping Studies},
  author = {Ainsworth, L. M. and Dean, C. B.},
  year = {2008},
  month = feb,
  volume = {19},
  pages = {21--37},
  issn = {11804009, 1099095X},
  doi = {10.1002/env.851},
  abstract = {In mapping studies, extreme risk areas may arise in proximity to one another in a smooth spatial surface. They may also arise as isolated `hotspots' or `lowspots', which are quite distinct from those of neighbouring sites. In this paper, we develop spatial methods which encompass both types of extreme risks. The former is modelled by a spatially smooth surface using a conditional autoregressive model; the latter is addressed with the addition of a discrete clustering component, which offers the flexibility of accommodating extreme isolated risks and is not limited by spatial smoothness. The autoregressive component incorporates the spatially correlated risk as a baseline surface, acknowledging that environmental activity, often spatially correlated, influences risk responses. The discrete component identifies hotspots/lowspots of activity beyond the spatially correlated baseline risk surface. Both types of extreme risk are important, but isolated extremes may provide insight into areas with potential of being a centre for future spatially correlated extreme risks. Hence these may be particularly important in terms of surveillance. A Bayesian approach to inference is employed and graphical techniques for isolating extremes are illustrated. Model assessment is conducted via cross-validation posterior predictive checks. Three examples demonstrate the utility of the methods and case studies show the procedures to be useful for pinpointing extreme risks. In addition, sensitivity to priors is investigated. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2YPB9KUF\\Ainsworth and Dean - 2008 - Detection of local and global outliers in mapping .pdf},
  journal = {Environmetrics},
  language = {en},
  number = {1}
}

@article{aitkin1991posterior,
  title = {Posterior Bayes Factors},
  author = {Aitkin, Murray},
  year = {1991},
  pages = {111--142},
  publisher = {{JSTOR}},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)}
}

@article{aitkinPosteriorBayesFactors1991,
  title = {Posterior {{Bayes Factors}}},
  author = {Aitkin, Murray},
  year = {1991},
  month = sep,
  volume = {53},
  pages = {111--128},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1991.tb01812.x},
  abstract = {A general procedure for computing Bayes factors for the comparison of arbitrary models is described, based on the use of the posterior mean of the likelihood under each model rather than the usual prior mean. The use of the posterior mean has several advantages, including reduced sensitivity to variations in the prior and the avoidance of the Lindley paradox in testing point null hypotheses. The frequency properties of the new procedure are evaluated in standard examples, and a non-standard example is analysed to show the considerable differences possible between prior and posterior means of the likelihood. Several different justifications of the procedure are given, and a non-Bayesian direct likelihood interpretation is described.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EN6SPQRJ\\Aitkin - 1991 - Posterior Bayes Factors.pdf;C\:\\Users\\devan\\Zotero\\storage\\MGWH2ZZY\\Aitkin - 1991 - Posterior Bayes Factors.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  language = {en},
  number = {1}
}

@article{albert-greenHierarchicalPointProcess2019,
  title = {A Hierarchical Point Process with Application to Storm Cell Modelling},
  author = {Albert-Green, Alisha and Braun, W. John and Dean, Charmaine B. and Miller, Craig},
  year = {2019},
  volume = {47},
  pages = {46--64},
  issn = {1708-945X},
  doi = {10.1002/cjs.11485},
  abstract = {In environmetrics, interest often centres around the development of models and methods for making inference on observed point patterns assumed to be generated by latent spatial or spatio-temporal processes, which may have a hierarchical structure. In this research, motivated by the analysis of spatio-temporal storm cell data, we generalize the Neyman\textendash Scott parent\textendash child process to account for hierarchical clustering. This is accomplished by allowing the parents to follow a log-Gaussian Cox process thereby incorporating correlation and facilitating inference at all levels of the hierarchy. This approach is applied to monthly storm cell data from the Bismarck, North Dakota radar station from April through August 2003 and we compare these results to simpler cluster processes to demonstrate the advantages of accounting for both levels of correlation present in these hierarchically clustered point patterns. The Canadian Journal of Statistics 47: 46\textendash 64; 2019 \textcopyright{} 2019 Statistical Society of Canada},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DFPVRAGW\\Albert‐Green et al. - 2019 - A hierarchical point process with application to s.pdf;C\:\\Users\\devan\\Zotero\\storage\\7E2ZRBLZ\\cjs.html},
  journal = {Canadian Journal of Statistics},
  keywords = {Cluster process,log-Gaussian Cox process,minimum contrast estimation,Neyman–Scott process,spatio-temporal point process},
  language = {en},
  number = {1}
}

@article{albert-greenHowDoubleUnders2020,
  title = {How to {{Do Double Unders}} in {{Under}} 5 {{Minutes}}},
  author = {{Albert-Green}, Alisha},
  year = {2020},
  volume = {7},
  journal = {Journal of Catalyst Training},
  number = {43}
}

@phdthesis{albert-greenJointModelsSpatial2016,
  title = {Joint {{Models}} for {{Spatial}} and {{Spatio}}-{{Temporal Point Processes}}},
  author = {{Albert-Green}, Alisha},
  year = {2016},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QFA33YHI\\Albert-Green - 2016 - Joint Models for Spatial and Spatio-Temporal Point.pdf;C\:\\Users\\devan\\Zotero\\storage\\56ATM45Y\\4306.html},
  keywords = {Application,Theory},
  school = {The University of Western Ontario}
}

@article{albert-greenMethodologyInvestigatingTrends2013,
  title = {A Methodology for Investigating Trends in Changes in the Timing of the Fire Season with Applications to Lightning-Caused Forest Fires in {{Alberta}} and {{Ontario}}, {{Canada}}},
  author = {{Albert-Green}, Alisha and Dean, C.B. and Martell, David L. and Woolford, Douglas G.},
  year = {2013},
  month = jan,
  volume = {43},
  pages = {39--45},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/cjfr-2011-0432},
  abstract = {Lightning-caused fires account for approximately 45\% of ignitions and 80\% of area burned by forest fires in Canada. Investigating the seasonality of these fires and the extent to which it may be changing over time is of interest to both fire managers and researchers. In this project, we develop flexible models for describing the temporal variation in the risk of lightning-caused fires. Generalized additive models are first used to obtain smooth estimates of fire risk by Julian day for each year. Inverse calculations are then employed to obtain point and interval estimates of the start and end of the fire season annually; these are defined by the crossing of fire risk thresholds. Finally, permutation-based methods are used to test for significant linear trends in the start and end of the fire season. This methodology is applied to historical forest fire records in Alberta, Canada, and the western and eastern subregions of Ontario, Canada. Our results suggest significant changes to both the start and end of the fire season in Alberta and a significant change to the end of the fire season in western and eastern Ontario.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UYSFSPP5\\Albert-Green et al. - 2013 - A methodology for investigating trends in changes .pdf},
  journal = {Canadian Journal of Forest Research},
  language = {en},
  number = {1}
}

@article{AlbertGreen2014,
  title = {Visualization Tools for Assessing the {{Markov}} Property: {{Sojourn}} Times in the Forest Fire Weather Index in {{Ontario}}},
  author = {{Albert-Green}, Alisha and Braun, W. John and Martell, David L. and Woolford, Douglas G.},
  year = {2014},
  volume = {25},
  pages = {417--430},
  issn = {1099095X},
  doi = {10.1002/env.2237},
  abstract = {In Canada, the Fire Weather Index (FWI) provides forest fire managers with an overall measure of fire danger. Specifically, the FWI is a numerical rating of the potential intensity of a forest fire based on its potential spread rate and the amount of vegetation available for combustion. In our analyses, we consider daily FWI time series, recorded over 42 fire seasons from a sample of fire-weather stations in Ontario, Canada. Graphical exploratory analyses of the data, including stalagmite plots (a new interactive, three-dimensional visualization tool), show that the FWI switches between epochs of nil and non-nil behaviour. This paper develops a framework for assessing sojourn times in these two phases. At some sites, the FWI process appears to begin each year as an approximate Markov process before gradually losing its Markovian character. However, a time-homogeneous discrete time Markov chain model is insufficient overall, because those sojourn times are not found to be geometrically distributed. Instead, the duration of epochs in each of these phases can be more accurately modelled using beta-geometric random variables which incorporate seasonality of phase-specific run length behaviour using local likelihood methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\L7T5VUYN\\Albert-Green et al. - 2014 - Visualization tools for assessing the Markov prope.pdf},
  isbn = {1180-4009},
  journal = {Environmetrics},
  keywords = {Beta-geometric,Fire danger,Local likelihood,Rainbow plot,Stalagmite plot},
  number = {6}
}

@article{altieriBayesianPsplinesAdvanced2016,
  title = {Bayesian {{P}}-Splines and Advanced Computing in {{R}} for a Changepoint Analysis on Spatio-Temporal Point Processes},
  author = {Altieri, L. and Cocchi, D. and Greco, F. and Illian, Janine B. and Scott, E.M.},
  year = {2016},
  month = sep,
  volume = {86},
  pages = {2531--2545},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2016.1146280},
  abstract = {This work presents advanced computational aspects of a new method for changepoint detection on spatio-temporal point process data. We summarize the methodology, based on building a Bayesian hierarchical model for the data and declaring prior conjectures on the number and positions of the changepoints, and show how to take decisions regarding the acceptance of potential changepoints. The focus of this work is about choosing an approach that detects the correct changepoint and delivers smooth reliable estimates in a feasible computational time; we propose Bayesian P-splines as a suitable tool for managing spatial variation, both under a computational and a model fitting performance perspective. The main computational challenges are outlined and a solution involving parallel computing in R is proposed and tested on a simulation study. An application is also presented on a data set of seismic events in Italy over the last 20 years.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\59GV9N39\\Altieri et al. - 2016 - Bayesian P-splines and advanced computing in R for.pdf;C\:\\Users\\devan\\Zotero\\storage\\CPGMNKS5\\Altieri et al. - 2016 - Bayesian P-splines and advanced computing in R for.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {13}
}

@article{altieriChangepointAnalysisSpatiotemporal2015,
  title = {A Changepoint Analysis of Spatio-Temporal Point Processes},
  author = {Altieri, Linda and Scott, E. Marian and Cocchi, Daniela and Illian, Janine B.},
  year = {2015},
  month = nov,
  volume = {14},
  pages = {197--207},
  issn = {22116753},
  doi = {10.1016/j.spasta.2015.05.005},
  abstract = {This work introduces a Bayesian approach to detecting multiple unknown changepoints over time in the inhomogeneous intensity of a spatio-temporal point process with spatial and temporal dependence within segments. We propose a new method for detecting changes by fitting a spatio-temporal log-Gaussian Cox process model using the computational efficiency and flexibility of integrated nested Laplace approximation, and by studying the posterior distribution of the potential changepoint positions. In this paper, the context of the problem and the research questions are introduced, then the methodology is presented and discussed in detail. A simulation study assesses the validity and properties of the proposed methods. Lastly, questions are addressed concerning potential unknown changepoints in the intensity of radioactive particles found on Sandside beach, Dounreay, Scotland.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JT93YVJ7\\Altieri et al. - 2015 - A changepoint analysis of spatio-temporal point pr.pdf;C\:\\Users\\devan\\Zotero\\storage\\MNFX2NUU\\Altieri et al. - 2015 - A changepoint analysis of spatio-temporal point pr.pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@article{altieriLookingChangepointsSpatiotemporal2015,
  title = {Looking for Changepoints in Spatio-Temporal Earthquake Data},
  author = {Altieri, L and Cocchi, D and Greco, F and Illian, Janine B. and Scott, E M},
  year = {2015},
  pages = {4},
  abstract = {This work presents an application of a new method for changepoint detection on spatiotemporal point process data. We summarise the methodology, based on building a Bayesian hierarchical model for the data and priors on the number and positions of the changepoints, and introduce two approaches to taking decisions on the acceptance of potential changepoints. We present the dataset collecting Italian seismic events over 30 years and show results for multiple changepoint detection. Finally, concluding comments and suggestions for further work are provided.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7TEQJVPI\\Altieri et al. - 2015 - Looking for changepoints in spatio-temporal earthq.pdf;C\:\\Users\\devan\\Zotero\\storage\\DHE9PSJX\\Altieri et al. - 2015 - Looking for changepoints in spatio-temporal earthq.pdf},
  language = {en}
}

@article{alwan1988time,
  title = {Time-Series Modeling for Statistical Process Control},
  author = {Alwan, Layth C and Roberts, Harry V},
  year = {1988},
  volume = {6},
  pages = {87--95},
  publisher = {{Taylor {{\&}} Francis Group}},
  journal = {Journal of Business {{\&}} Economic Statistics},
  number = {1}
}

@article{amaral-turkmanHierarchicalSpacetimeModels2011,
  title = {Hierarchical Space-Time Models for Fire Ignition and Percentage of Land Burned by Wildfires},
  author = {{Amaral-Turkman}, M. A. and Turkman, K. F. and Le Page, Y. and Pereira, J. M. C.},
  year = {2011},
  month = dec,
  volume = {18},
  pages = {601--617},
  issn = {1573-3009},
  doi = {10.1007/s10651-010-0153-9},
  abstract = {Policy responses for local and global fire management as well as international green-gas inventories depend heavily on the proper understanding of the annual fire extend as well as its spatial variation across any given study area. Proper statistical models are important tools in quantifying these fire risks. We propose Bayesian methods to model jointly the probability of ignition and fire sizes in Australia and New Zeland. The data set on which we base our model and results consists of annual observations of several meteorological and topographical explanatory variables, together with the percentage of land burned over a grid with resolution of 1\textdegree{} across Austalia and New Zealand. Our model and conclusions bring improvements on the results reported by Russell-Smith et~al. in Int J Wildland Fire, 16:361\textendash 377 (2007) based on a similar data set.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4LJGEFD4\\Amaral-Turkman et al. - 2011 - Hierarchical space-time models for fire ignition a.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {4}
}

@misc{amazon,
  title = {Amazon {{Mechanical Turk}}},
  author = {{Amazon}},
  year = {2008},
  howpublished = {https://www.mturk.com}
}

@article{amiroFireWeatherIndex2004,
  title = {Fire Weather Index System Components for Large Fires in the {{Canadian}} Boreal Forest},
  author = {Amiro, B. D. and Logan, K. A. and Wotton, B. M. and Flannigan, M. D. and Todd, J. B. and Stocks, B. J. and Martell, D. L.},
  year = {2004},
  volume = {13},
  pages = {391},
  issn = {1049-8001},
  doi = {10.1071/WF03066},
  abstract = {Canadian Fire Weather Index (FWI) System components and head fire intensities were calculated for fires greater than 2 km2 in size for the boreal and taiga ecozones of Canada from 1959 to 1999. The highest noonhour values were analysed that occurred during the first 21 days of each of 9333 fires. Depending on ecozone, the means of the FWI System parameters ranged from: fine fuel moisture code (FFMC), 90 to 92 (82 to 96 for individual fires); duff moisture code (DMC), 38 to 78 (10 to 140 for individual fires); drought code (DC), 210 to 372 (50 to 600 for individual fires); and fire weather index, 20 to 33 (5 to 60 for individual fires). Fine fuel moisture code decreased, DMC had a mid-season peak, and DC increased through the fire season. Mean head fire intensities ranged from 10 to 28 MW m-1 in the boreal spruce fuel type, showing that most large fires exhibit crown fire behaviour. Intensities of individual fires can exceed 60 MW m-1. Most FWI System parameters did not show trends over the 41-year period because of large inter-annual variability. A changing climate is expected to create future weather conditions more conducive to fire throughout much of Canada but clear changes have not yet occurred.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6SCE867E\\Amiro et al. - 2004 - Fire weather index system components for large fir.pdf},
  journal = {International Journal of Wildland Fire},
  keywords = {Size},
  language = {en},
  number = {4}
}

@article{AnalyzingSpatiotemporalData2017,
  title = {Analyzing Spatio-Temporal Data with {{R}}: {{Everything}} You Always Wanted to Know \textendash{} but Were Afraid to Ask},
  year = {2017},
  volume = {158},
  pages = {35},
  abstract = {We present an overview of (geo-)statistical models, methods and techniques for the analysis and prediction of continuous spatio-temporal processes residing in continuous space. Various approaches exist for building statistical models for such processes, estimating their parameters and performing predictions. We cover the Gaussian process approach, very common in spatial statistics and geostatistics, and we focus on R-based implementations of numerical procedures. To illustrate and compare the use of some of the most relevant packages, we treat a real-world application with high-dimensional data. The target variable is the daily mean PM10 concentration predicted thanks to a chemistrytransport model and observation series collected at monitoring stations across France in 2014. We give R code covering the full work-flow from importing data sets to the prediction of PM10 concentrations with a fitted parametric model, including the visualization of data, estimation of the parameters of the spatio-temporal covariance function and model selection. We conclude with some elements of comparison between the packages that are available today and some discussion for future developments.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\947GUDTC\\2017 - Analyzing spatio-temporal data with R Everything .pdf},
  language = {en},
  number = {3}
}

@article{Apley2002,
  title = {The {{Autoregressive T}}{$^2$} {{Chart}} for {{Monitoring Univariate Autocorrelated Processes}}},
  author = {Apley, Daniel W. and Tsung, Fugee},
  year = {2002},
  volume = {34},
  pages = {80--96},
  issn = {0022-4065},
  doi = {10.1080/00224065.2002.11980131},
  abstract = {In this paper we investigate the autoregressive T-2 control chart for statistical process control of autocorrelated processes. The method involves the monitoring, using Hotelling's T-2 statistic, of a vector formed from a moving window of observations of the univariate autocorrelated process, It is shown that the T-2 statistic can be decomposed into the sum of the squares of the residual errors for various order autoregressive time series models fit to the process data, Guidelines for designing the autoregressive T-2 chart are presented, and its performance is compared to that of residual-based CUSUM and Shewhart individual control charts. The autoregressive T-2 chart has a number of characteristics, including some level of robustness with respect to modeling errors, that make it an attractive alternative to residual-based control charts for autocorrelated processes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FJ7PEUEK\\Apley and Tsung - 2002 - The Autoregressive T² Chart for Monitoring Univari.pdf},
  journal = {Journal of Quality Technology},
  number = {1}
}

@article{apleyAutoregressiveChartMonitoring2002,
  title = {The {{Autoregressive}} {{{\emph{T}}}} 2 {{Chart}} for {{Monitoring Univariate Autocorrelated Processes}}},
  author = {Apley, Daniel W. and Tsung, Fugee},
  year = {2002},
  month = jan,
  volume = {34},
  pages = {80--96},
  issn = {0022-4065, 2575-6230},
  doi = {10.1080/00224065.2002.11980131},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QLVQJLZR\\Apley and Tsung - 2002 - The Autoregressive iTi 2 Chart for Monitoring.pdf;C\:\\Users\\devan\\Zotero\\storage\\XXMBVX42\\Apley and Tsung - 2002 - The Autoregressive iTi 2 Chart for Monitoring.pdf},
  journal = {Journal of Quality Technology},
  language = {en},
  number = {1}
}

@misc{Articles2018,
  title = {Articles},
  year = {2018},
  month = dec,
  abstract = {Articles Please complete our submission form in order to suggest additions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Y4UCKVKT\\articles.html},
  journal = {meta\_hockey},
  language = {en-US}
}

@misc{arvindExaminingDistributionScoring2016,
  title = {Examining the {{Distribution}} of {{Scoring}}},
  author = {Arvind},
  year = {2016},
  month = jun,
  abstract = {What we can learn from examining how scoring is distributed on the player level in the NHL.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2STV4YRU\\examining-the-distribution-of-scoring.html},
  howpublished = {https://www.pensionplanpuppets.com/2016/6/6/11862496/examining-the-distribution-of-scoring},
  journal = {Pension Plan Puppets}
}

@article{assuncaoNoteTestingSeparability2007,
  title = {A {{Note}} on {{Testing Separability}} in {{Spatial}}-{{Temporal Marked Point Processes}}},
  author = {Assun{\c c}{\~a}o, Renato and Maia, Alexandra},
  year = {2007},
  month = mar,
  volume = {63},
  pages = {290--294},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2007.00737_1.x},
  abstract = {In environmental risk analysis, it is common to assume the stochastic independence (or separability) between the marks associated with the random events of a spatial-temporal point process. Schoenberg (2004, Biometrics 60, 471\textendash 481) proposed several test statistics for this hypothesis and used simulated data to evaluate their performance. He found that a Cram\textasciiacute er-von Mises-type test is powerful to detect gradual departures from separability although it is not uniformly powerful over a large class of alternative models. We present a semiparametric approach to model alternative hypotheses to separability and derive a score test statistic. We show that there is a relationship between this score test and some of the test statistics proposed by Schoenberg. Specifically, all are different versions of weighted Cram\textasciiacute er-von Mises-type statistics. This gives some insight into the reasons for the similarities and differences between the test statistics' performance. We also point out some difficulties in controlling the type I error probability in Schoenberg's residual test.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MLMZTNDL\\Assunção and Maia - 2007 - A Note on Testing Separability in Spatial-Temporal.pdf},
  journal = {Biometrics},
  keywords = {Dependence},
  language = {en},
  number = {1}
}

@article{bachlInlabruPackageBayesian2019,
  title = {Inlabru: An {{R}} Package for {{Bayesian}} Spatial Modelling from Ecological Survey Data},
  shorttitle = {Inlabru},
  author = {Bachl, Fabian E. and Lindgren, Finn and Borchers, David L. and Illian, Janine B.},
  year = {2019},
  volume = {10},
  pages = {760--766},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13168},
  abstract = {Spatial processes are central to many ecological processes, but fitting models that incorporate spatial correlation to data from ecological surveys is computationally challenging. This is particularly true of point pattern data (in which the primary data are the locations at which target species are found), but also true of gridded data, and of georeferenced samples from continuous spatial fields. We describe here the R package inlabru that builds on the widely used RINLA package to provide easier access to Bayesian inference from spatial point process, spatial count, gridded, and georeferenced data, using integrated nested Laplace approximation (INLA, Rue et al., 2009). The package provides methods for fitting spatial density surfaces and estimating abundance, as well as for plotting and prediction. It accommodates data that are points, counts, georeferenced samples, or distance sampling data. This paper describes the main features of the package, illustrated by fitting models to the gorilla nest data contained in the package spatstat (Baddeley, \& Turner, 2005), a line transect survey dataset contained in the package dsm (Miller, Rexstad, Burt, Bravington, \& Hedley, 2018), and to a georeferenced sample from a simulated continuous spatial field.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YFBJJK88\\Bachl et al. - 2019 - inlabru an R package for Bayesian spatial modelli.pdf;C\:\\Users\\devan\\Zotero\\storage\\EEQHUGVW\\2041-210X.html},
  journal = {Methods in Ecology and Evolution},
  keywords = {Application,Bayesian inference,georeferenced data,point process,spatial count,spatial modelling},
  language = {en},
  number = {6}
}

@article{baddeleyAnalysingSpatialPoint2008,
  title = {Analysing Spatial Point Patterns in {{R}}},
  author = {Baddeley, Adrian},
  year = {2008},
  abstract = {This is a detailed set of notes for a workshop on Analysing spatial point patterns that has been held several times in Australia and New Zealand in 2006\textendash 2008. It covers statistical methods that are currently feasible in practice and available in public domain software. Some of these techniques are well established in the applications literature, while some are very recent developments.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RQW792R7\\Baddeley - 2008 - Analysing spatial point patterns in R.pdf},
  journal = {Workshop notes version 3},
  language = {en}
}

@article{baddeleyAnalysingSpatialPoint2010,
  title = {Analysing Spatial Point Patterns in {{R}}},
  author = {Baddeley, Adrian},
  year = {2010},
  pages = {232},
  abstract = {This is a detailed set of notes for a workshop on Analysing spatial point patterns in R, presented by the author in Australia and New Zealand since 2006. The goal of the workshop is to equip researchers with a range of practical techniques for the statistical analysis of spatial point patterns. Some of the techniques are well established in the applications literature, while some are very recent developments. The workshop is based on spatstat, a contributed library for the statistical package R, which is free open source software.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YRTH7ZAF\\Baddeley - 2010 - Analysing spatial point patterns in R.pdf},
  language = {en}
}

@article{baddeleyNonparametricEstimationDependence2012,
  title = {Nonparametric Estimation of the Dependence of a Spatial Point Process on Spatial Covariates},
  author = {Baddeley, Adrian and Chang, Ya-Mei and Song, Yong and Turner, Rolf},
  year = {2012},
  volume = {5},
  pages = {221--236},
  issn = {19387989, 19387997},
  doi = {10.4310/SII.2012.v5.n2.a7},
  file = {C\:\\Users\\devan\\Zotero\\storage\\M7W6IEEF\\Baddeley et al. - 2012 - Nonparametric estimation of the dependence of a sp.pdf},
  journal = {Statistics and Its Interface},
  language = {en},
  number = {2}
}

@article{baddeleyNonSemiParametric2000,
  title = {Non- and Semi-parametric Estimation of Interaction in Inhomogeneous Point Patterns},
  author = {Baddeley, A. J. and M{\o}ller, J. and Waagepetersen, R.},
  year = {2000},
  month = nov,
  volume = {54},
  pages = {329--350},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/1467-9574.00144},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PXV4IVQZ\\Baddeley et al. - 2000 - Non‐ and semi‐parametric estimation of interaction.pdf},
  journal = {Statistica Neerlandica},
  language = {en},
  number = {3}
}

@article{baddeleyPRACTICALMAXIMUMPSEUDOLIKELIHOOD2000,
  title = {{{PRACTICAL MAXIMUM PSEUDOLIKELIHOOD FOR SPATIAL POINT PATTERNS}}},
  author = {Baddeley, Adrian and Turner, Rolf},
  year = {2000},
  pages = {40},
  abstract = {This paper describes a technique for computing approximate maximum pseudolikelihood estimates of the parameters of a spatial point process. The method is an extension of Berman \& Turner's (1992) device for maximizing the likelihoods of inhomogeneous spatial Poisson processes. For a very wide class of spatial point process models the likelihood is intractable, while the pseudolikelihood is known explicitly, except for the computation of an integral over the sampling region. Approximation of this integral by a finite sum in a special way yields an approximate pseudolikelihood which is formally equivalent to the (weighted) likelihood of a loglinear model with Poisson responses. This can be maximized using standard statistical software for generalized linear or additive models, provided the conditional intensity of the process takes an `exponential family' form. Using this approach a wide variety of spatial point process models of Gibbs type can be fitted rapidly, incorporating spatial trends, interaction between points, dependence on spatial covariates, and mark information.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F8K4INR6\\Baddeley and Turner - 2000 - PRACTICAL MAXIMUM PSEUDOLIKELIHOOD FOR SPATIAL POI.pdf;C\:\\Users\\devan\\Zotero\\storage\\WJ8NXAGS\\Baddeley and Turner - 2000 - PRACTICAL MAXIMUM PSEUDOLIKELIHOOD FOR SPATIAL POI.pdf;C\:\\Users\\devan\\Zotero\\storage\\Z7DA96N6\\Baddeley and Turner - 2000 - PRACTICAL MAXIMUM PSEUDOLIKELIHOOD FOR SPATIAL POI.pdf},
  language = {en}
}

@article{baddeleyPropertiesResidualsSpatial2008,
  title = {Properties of Residuals for Spatial Point Processes},
  author = {Baddeley, A. and M{\o}ller, J. and Pakes, A. G.},
  year = {2008},
  month = sep,
  volume = {60},
  pages = {627--649},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-007-0116-6},
  abstract = {For any point process in Rd that has a Papangelou conditional intensity {$\lambda$}, we define a random measure of `innovations' which has mean zero. When the point process model parameters are estimated from data, there is an analogous random measure of `residuals'. We analyse properties of the innovations and residuals, including first and second moments, conditional independence, a martingale property, lack of correlation, and marginal distributions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F3ZAJZVN\\Baddeley et al. - 2008 - Properties of residuals for spatial point processe.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {3}
}

@article{baddeleyResidualAnalysisSpatial2005,
  title = {Residual Analysis for Spatial Point Processes (with Discussion)},
  author = {Baddeley, A. and Turner, R. and M{\o}ller, J. and Hazelton, M.},
  year = {2005},
  volume = {67},
  pages = {617--666},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2005.00519.x},
  abstract = {Summary. We define residuals for point process models fitted to spatial point pattern data, and we propose diagnostic plots based on them. The residuals apply to any point process model that has a conditional intensity; the model may exhibit spatial heterogeneity, interpoint interaction and dependence on spatial covariates. Some existing ad hoc methods for model checking (quadrat counts, scan statistic, kernel smoothed intensity and Berman's diagnostic) are recovered as special cases. Diagnostic tools are developed systematically, by using an analogy between our spatial residuals and the usual residuals for (non-spatial) generalized linear models. The conditional intensity {$\lambda$} plays the role of the mean response. This makes it possible to adapt existing knowledge about model validation for generalized linear models to the spatial point process context, giving recommendations for diagnostic plots. A plot of smoothed residuals against spatial location, or against a spatial covariate, is effective in diagnosing spatial trend or co-variate effects. Q\textendash Q-plots of the residuals are effective in diagnosing interpoint interaction.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6YHNN9LH\\Baddeley et al. - 2005 - Residual analysis for spatial point processes (wit.pdf;C\:\\Users\\devan\\Zotero\\storage\\J3GAMHDI\\j.1467-9868.2005.00519.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Berman–Turner device,Berman's diagnostic,Estimating equations,Exponential energy marks,Generalized linear models,Georgii–Nguyen–Zessin formula,K-function,Kernel smoothing,Ogata residual,Papangelou conditional intensity,Pearson residuals,Pseudolikelihood,Q–Q-plots,Quadrat counts,Residual plots,Scan statistic,Space–time point processes},
  language = {en},
  number = {5}
}

@article{baddeleyResidualAnalysisSpatial2005a,
  title = {Residual {{Analysis}} for {{Spatial Point Processes}}},
  author = {Baddeley, A. and Turner, R. and M{\o}ller, J. and Hazelton, M.},
  year = {2005},
  volume = {67},
  pages = {617--666},
  abstract = {We define residuals for point process models fitted to spatial point pattern data, and we propose diagnostic plots based on them.The residuals apply to any point process model that has a conditional intensity; the model may exhibit spatial heterogeneity, interpoint interaction and dependence on spatial covariates. Some existing ad hoc methods for model checking (quadrat counts, scan statistic, kernel smoothed intensity and Berman's diagnostic) are recovered as special cases. Diagnostic tools are developed systematically, by using an analogy between our spatial residuals and the usual residuals for (non-spatial) generalized linear models. The conditional intensity A plays the role of the mean response. This makes it possible to adapt existing knowledge about model validation for generalized linear models to the spatial point process context, giving recommendations for diagnostic plots. A plot of smoothed residuals against spatial location, or against a spatial covariate, is effective in diagnosing spatial trend or covariate effects. Q-Q-plots of the residuals are effective in diagnosing interpoint interaction.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WZSQ9FKH\\Baddeley et al. - 2005 - Residual Analysis for Spatial Point Processes.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  language = {en},
  number = {5}
}

@article{baddeleySpatialPointProcesses,
  title = {Spatial {{Point Processes}} and Their {{Applications}}},
  author = {Baddeley, Adrian},
  pages = {75},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZJ8I4ULG\\Baddeley - Spatial Point Processes and their Applications.pdf},
  language = {en}
}

@article{baddeleySpatstatPackageAnalyzing2005,
  title = {Spatstat: {{An R Package}} for {{Analyzing Spatial Point Patterns}}},
  shorttitle = {{\textbf{Spatstat}}},
  author = {Baddeley, Adrian and Turner, Rolf},
  year = {2005},
  volume = {12},
  issn = {1548-7660},
  doi = {10.18637/jss.v012.i06},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3CQZYKMZ\\Baddeley and Turner - 2005 - bspatstatb  An iRi Package for Analyzin.pdf;C\:\\Users\\devan\\Zotero\\storage\\C9EZ27UV\\Baddeley and Turner - 2005 - bspatstatb  An iRi Package for Analyzin.pdf;C\:\\Users\\devan\\Zotero\\storage\\P52Q2ZHW\\Baddeley and Turner - 2005 - bspatstatb  An iRi Package for Analyzin.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {6}
}

@article{bakkaNonstationaryGaussianModels2016,
  title = {Non-Stationary {{Gaussian}} Models with Physical Barriers},
  author = {Bakka, Haakon and Vanhatalo, Jarno and Illian, Janine and Simpson, Daniel and Rue, H{\aa}vard},
  year = {2016},
  month = aug,
  abstract = {This paper is motivated by a study on the spatial distribution of fish larvae in reproduction habitats of three commercially important fish species in the Finnish archipelago. Species distribution modeling (SDM) requires realistic models that account for spatial dependence in observation data that cannot be explained by environmental factors alone.},
  archivePrefix = {arXiv},
  eprint = {1608.03787},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8RG4BMAN\\Bakka et al. - 2016 - Non-stationary Gaussian models with physical barri.pdf},
  journal = {arXiv:1608.03787 [stat]},
  keywords = {Statistics - Applications,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{bakkaSpatialModelingRINLA2018,
  title = {Spatial Modeling with {{R}}-{{INLA}}: {{A}} Review},
  shorttitle = {Spatial Modeling with {{R}}-{{INLA}}},
  author = {Bakka, Haakon and Rue, H{\aa}vard and Fuglstad, Geir-Arne and Riebler, Andrea and Bolin, David and Illian, Janine and Krainski, Elias and Simpson, Daniel and Lindgren, Finn},
  year = {2018},
  volume = {10},
  pages = {e1443},
  issn = {1939-0068},
  doi = {10.1002/wics.1443},
  abstract = {Coming up with Bayesian models for spatial data is easy, but performing inference with them can be challenging. Writing fast inference code for a complex spatial model with realistically-sized datasets from scratch is time-consuming, and if changes are made to the model, there is little guarantee that the code performs well. The key advantages of R-INLA are the ease with which complex models can be created and modified, without the need to write complex code, and the speed at which inference can be done even for spatial problems with hundreds of thousands of observations. R-INLA handles latent Gaussian models, where fixed effects, structured and unstructured Gaussian random effects are combined linearly in a linear predictor, and the elements of the linear predictor are observed through one or more likelihoods. The structured random effects can be both standard areal model such as the Besag and the BYM models, and geostatistical models from a subset of the Mat\'ern Gaussian random fields. In this review, we discuss the large success of spatial modeling with R-INLA and the types of spatial models that can be fitted, we give an overview of recent developments for areal models, and we give an overview of the stochastic partial differential equation (SPDE) approach and some of the ways it can be extended beyond the assumptions of isotropy and separability. In particular, we describe how slight changes to the SPDE approach leads to straight-forward approaches for nonstationary spatial models and nonseparable space\textendash time models. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Bayesian Methods and Theory Statistical Models {$>$} Bayesian Models Data: Types and Structure {$>$} Massive Data},
  copyright = {\textcopyright{} 2018 Wiley Periodicals, Inc.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ERQTQZ86\\Bakka et al. - 2018 - Spatial modeling with R-INLA A review.pdf;C\:\\Users\\devan\\Zotero\\storage\\RZYMNPU6\\wics.html},
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  keywords = {approximate Bayesian inference,Gaussian Markov random fields,Laplace approximations,sparse matrices,spatial statistics,stochastic partial differential equations},
  language = {en},
  number = {6}
}

@article{barAccountingHeapingRetrospectively2012,
  title = {Accounting for {{Heaping}} in {{Retrospectively Reported Event Data}} \textendash{} {{A Mixture}}-{{Model Approach}}},
  author = {Bar, Haim Y. and Lillard, Dean R.},
  year = {2012},
  month = nov,
  volume = {31},
  pages = {3347--3365},
  issn = {0277-6715},
  doi = {10.1002/sim.5419},
  abstract = {When event data are retrospectively reported, more temporally distal events tend to get ``heaped'' on even multiples of reporting units. Heaping may introduce a type of attenuation bias because it causes researchers to mismatch time-varying right-hand side variables. We develop a model-based approach to estimate the extent of heaping in the data, and how it affects regression parameter estimates. We use smoking cessation data as a motivating example, but our method is general. It facilitates the use of retrospective data from the multitude of cross-sectional and longitudinal studies worldwide that collect and potentially could collect event data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Q8JQZEDK\\Bar and Lillard - 2012 - Accounting for Heaping in Retrospectively Reported.pdf},
  journal = {Statistics in medicine},
  number = {27},
  pmcid = {PMC3518030},
  pmid = {22733577}
}

@article{barbozaEfficientReconstructionsCommon2019,
  title = {Efficient {{Reconstructions}} of {{Common Era Climate}} via {{Integrated Nested Laplace Approximations}}},
  author = {Barboza, Luis A. and {Emile-Geay}, Julien and Li, Bo and He, Wan},
  year = {2019},
  month = sep,
  volume = {24},
  pages = {535--554},
  issn = {1537-2693},
  doi = {10.1007/s13253-019-00372-4},
  abstract = {Paleoclimate reconstruction on the Common Era (1\textendash 2000 AD) provides critical context for recent warming trends. This work leverages integrated nested Laplace approximations (INLA) to conduct inference under a Bayesian hierarchical model using data from three sources: a state-of-the-art proxy database (PAGES 2k), surface temperature observations (HadCRUT4), and latest estimates of external forcings. INLA's computational efficiency allows to explore several model formulations (with or without forcings, explicitly modeling internal variability or not), as well as five data reduction techniques. Two different validation exercises find a small impact of data reduction choices, but a large impact for model choice, with best results for the two models that incorporate external forcings. These models confirm that man-made greenhouse gas emissions are the largest contributor to temperature variability over the Common Era, followed by volcanic forcing. Solar effects are indistinguishable from zero. INLA provide an efficient way to estimate the posterior mean, comparable with the much costlier Monte Carlo Markov Chain procedure, but with wider uncertainty bounds. We recommend using it for exploration of model designs, but full MCMC solutions should be used for proper uncertainty quantification.Supplementary materials accompanying this paper appear online.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9YZQAAS8\\Barboza et al. - 2019 - Efficient Reconstructions of Common Era Climate vi.pdf},
  journal = {Journal of Agricultural, Biological and Environmental Statistics},
  keywords = {Hierarchical Bayesian model,INLA,Paleoclimate reconstruction},
  language = {en},
  number = {3}
}

@article{barrRandomEffectsStructure2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  volume = {68},
  pages = {255--278},
  issn = {0749596X},
  doi = {10.1016/j.jml.2012.11.001},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LGQAQF5T\\Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf;C\:\\Users\\devan\\Zotero\\storage\\R82DS4S7\\Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf},
  journal = {Journal of Memory and Language},
  language = {en},
  number = {3}
}

@article{bartolucciFiniteMixtureLatent2015,
  title = {A Finite Mixture Latent Trajectory Model for Modeling Ultrarunners' Behavior in a 24-Hour Race},
  author = {Bartolucci, Francesco and Murphy, Thomas Brendan},
  year = {2015},
  month = jan,
  volume = {11},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2014-0060},
  abstract = {A finite mixture latent trajectory model is developed to study the performance and strategy of runners in a 24-h long ultra running race. The model facilitates clustering of runners based on their speed and propensity to rest and thus reveals the strategies used in the race. Inference for the adopted latent trajectory model is achieved using an expectation-maximization algorithm. Fitting the model to data from the 2013 World Championships reveals three clearly separated clusters of runners who exhibit different strategies throughout the race. The strategies show that runners can be grouped in terms of their average moving speed and their propensity to rest during the race. The effect of age and gender on the probability of belonging to each cluster is also investigated.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NAT4339P\\Bartolucci and Murphy - 2015 - A finite mixture latent trajectory model for model.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {4}
}

@article{bastidaBayesianRobustnessCompound2009,
  title = {Bayesian Robustness of the Compound {{Poisson}} Distribution under Bidimensional Prior: An Application to the Collective Risk Model},
  shorttitle = {Bayesian Robustness of the Compound {{Poisson}} Distribution under Bidimensional Prior},
  author = {Bastida, Agust{\'i}n Hern{\'a}ndez and D{\'e}niz, Emilio G{\'o}mez and S{\'a}nchez, Jos{\'e} Mar{\'i}a P{\'e}rez},
  year = {2009},
  month = aug,
  volume = {36},
  pages = {853--869},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/02664760802510059},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H22EJ9H4\\Bastida et al. - 2009 - Bayesian robustness of the compound Poisson distri.pdf},
  journal = {Journal of Applied Statistics},
  language = {en},
  number = {8}
}

@article{baumerOpenWAROpenSource2015,
  title = {{{openWAR}}: {{An}} Open Source System for Evaluating Overall Player Performance in Major League Baseball},
  shorttitle = {{{openWAR}}},
  author = {Baumer, Benjamin S. and Jensen, Shane T. and Matthews, Gregory J.},
  year = {2015},
  month = jan,
  volume = {11},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2014-0098},
  abstract = {Within sports analytics, there is substantial interest in comprehensive statistics intended to capture overall player performance. In baseball, one such measure is wins above replacement (WAR), which aggregates the contributions of a player in each facet of the game: hitting, pitching, baserunning, and fielding. However, current versions of WAR depend upon proprietary data, ad hoc methodology, and opaque calculations. We propose a competitive aggregate measure, openWAR, that is based on public data, a methodology with greater rigor and transparency, and a principled standard for the nebulous concept of a ``replacement'' player. Finally, we use simulation-based techniques to provide interval estimates for our openWAR measure that are easily portable to other domains.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WBKFPFKP\\Baumer et al. - 2015 - openWAR An open source system for evaluating over.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {2}
}

@article{BayesianGeoadditiveModelling,
  title = {Bayesian Geoadditive Modelling of Climate Extremes with Nonparametric Spatially Varying Temporal Effects - {{Yang}} - 2016 - {{International Journal}} of {{Climatology}} - {{Wiley Online Library}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YE2DZE6J\\Bayesian geoadditive modelling of climate extremes.pdf;C\:\\Users\\devan\\Zotero\\storage\\SD2UXMAA\\joc.html}
}

@book{BayesianStatisticsAction2017,
  title = {Bayesian Statistics in Action},
  year = {2017},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8JN5CCSE\\2017 - Bayesian statistics in action.pdf;C\:\\Users\\devan\\Zotero\\storage\\99FNPEL3\\2017 - Bayesian statistics in action.pdf},
  isbn = {978-3-319-54083-2},
  language = {en}
}

@article{bayesLIIEssaySolving1763,
  title = {{{LII}}. {{An}} Essay towards Solving a Problem in the Doctrine of Chances. {{By}} the Late {{Rev}}. {{Mr}}. {{Bayes}}, {{F}}. {{R}}. {{S}}. Communicated by {{Mr}}. {{Price}}, in a Letter to {{John Canton}}, {{A}}. {{M}}. {{F}}. {{R}}. {{S}}},
  author = {Bayes, Thomas and Price, Richard},
  year = {1763},
  month = jan,
  volume = {53},
  pages = {370--418},
  doi = {10.1098/rstl.1763.0053},
  abstract = {Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SLQUTW6A\\Bayes and Price - 1763 - LII. An essay towards solving a problem in the doc.pdf;C\:\\Users\\devan\\Zotero\\storage\\GZSYXFQC\\rstl.1763.html},
  journal = {Philosophical Transactions of the Royal Society of London}
}

@article{beckerAssessingDependenceFrequency,
  title = {Assessing {{Dependence Between Frequency}} and {{Severity Through Shared Random Effects}}},
  author = {Becker, Devan G. and Dean, Charmaine B. and Woolford, Douglas G.},
  journal = {Working Paper}
}

@inproceedings{beckerCanProfileMonitoring2017,
  title = {Can {{Profile Monitoring}} of {{Onboard Flight Data Detect Airtanker Pilot Fatigue}}?},
  booktitle = {{{CASI AERO}} 2017: 63rd {{Candian Aeronautics Conference}}},
  author = {Becker, Devan G. and Braun, W. John},
  year = {2017},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FMUJL798\\Becker and Braun - 2017 - Can Profile Monitoring of Onboard Flight Data Dete.pdf}
}

@inproceedings{beckerSpaceOtherThings2017,
  title = {Space and {{Some Other Things}}: {{Point Process Models}} for {{Hockey Data}}},
  shorttitle = {Space and {{Some Other Things}}},
  booktitle = {Ottawa {{Hockey Analytics Conference}}},
  author = {Becker, Devan},
  year = {2017},
  month = apr,
  address = {{Carleton University}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QLHZTFYG\\Becker - 2017 - Space and Some Other Things Point Process Models .pdf}
}

@article{beckerVisualizingMultivariateTime,
  title = {Visualizing {{Multivariate Time Series}} of {{Aerial Fire Fighting Data}}},
  author = {Becker, Devan G and Braun, W John and Dean, C B and Woolford, Douglas G},
  abstract = {Aerial wildland fire fighters have a unique challenge. They are able to fill their tank with water via a nearby body of water, drop this water on a fire, then return to repeat this process. For a given fire, the replications of the fills and drops are multivariate time series measured in space, and this data structure allows us to compare replications over space and time. We use control chart methodologies to determine which time series were unlike the others, then examine the data from both a univariate and a multivariate viewpoint to determine potential causes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VKB9BBR8\\Becker et al. - Visualizing Multivariate Time Series of Aerial Fir.pdf},
  language = {en}
}

@article{bedfordCrypticTransmissionSARSCoV22020,
  title = {Cryptic Transmission of {{SARS}}-{{CoV}}-2 in {{Washington}} State},
  author = {Bedford, Trevor and Greninger, Alexander L. and Roychoudhury, Pavitra and Starita, Lea M. and Famulare, Michael and Huang, Meei-Li and Nalla, Arun and Pepper, Gregory and Reinhardt, Adam and Xie, Hong and Shrestha, Lasata and Nguyen, Truong N. and Adler, Amanda and Brandstetter, Elisabeth and Cho, Shari and Giroux, Danielle and Han, Peter D. and Fay, Kairsten and Frazar, Chris D. and Ilcisin, Misja and Lacombe, Kirsten and Lee, Jover and Kiavand, Anahita and Richardson, Matthew and Sibley, Thomas R. and Truong, Melissa and Wolf, Caitlin R. and Nickerson, Deborah A. and Rieder, Mark J. and Englund, Janet A. and Investigators{\textdaggerdbl}, The Seattle Flu Study and Hadfield, James and Hodcroft, Emma B. and Huddleston, John and Moncla, Louise H. and M{\"u}ller, Nicola F. and Neher, Richard A. and Deng, Xianding and Gu, Wei and Federman, Scot and Chiu, Charles and Duchin, Jeff S. and Gautom, Romesh and Melly, Geoff and Hiatt, Brian and Dykema, Philip and Lindquist, Scott and Queen, Krista and Tao, Ying and Uehara, Anna and Tong, Suxiang and MacCannell, Duncan and Armstrong, Gregory L. and Baird, Geoffrey S. and Chu, Helen Y. and Shendure, Jay and Jerome, Keith R.},
  year = {2020},
  month = sep,
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abc0523},
  abstract = {Following its emergence in Wuhan, China, in late November or early December 2019, the SARS-CoV-2 virus has rapidly spread globally. Genome sequencing of SARS-CoV-2 allows reconstruction of its transmission history, although this is contingent on sampling. We have analyzed 453 SARS-CoV-2 genomes collected between 20 February and 15 March 2020 from infected patients in Washington State, USA. We find that most SARS-CoV-2 infections sampled during this time derive from a single introduction in late January or early February 2020 which subsequently spread locally before active community surveillance was implemented.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).. https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ESJM3BU3\\Bedford et al. - 2020 - Cryptic transmission of SARS-CoV-2 in Washington s.pdf;C\:\\Users\\devan\\Zotero\\storage\\Q6WLAIE5\\science.html},
  journal = {Science},
  language = {en},
  pmid = {32913002}
}

@phdthesis{benetreau-dupinProbabilisticReasoningCosmology,
  title = {Probabilistic {{Reasoning}} in {{Cosmology}}},
  author = {{Ben{\'e}treau-Dupin}, Yann},
  abstract = {Cosmology raises novel philosophical questions regarding the use of probabilities in inference. This work aims at identifying and assessing lines of arguments and problematic principles in probabilistic reasoning in cosmology. The first, second, and third papers deal with the intersection of two distinct problems: accounting for selection effects, and representing ignorance or indifference in probabilistic inferences. These two problems meet in the cosmology literature when anthropic considerations are used to predict cosmological parameters by conditionalizing the distribution of, e.g., the cosmological constant on the number of observers it allows for. However, uniform probability distributions usually appealed to in such arguments are an inadequate representation of indifference, and lead to unfounded predictions. It has been argued that this inability to represent ignorance is a fundamental flaw of any inductive framework using additive measures. In the first paper, I examine how imprecise probabilities fare as an inductive framework and avoid such unwarranted inferences. In the second paper, I detail how this framework allows us to successfully avoid the conclusions of Doomsday arguments in a way no Bayesian approach that represents credal states by single credence functions could. There are in the cosmology literature several kinds of arguments referring to selflocating uncertainty. In the multiverse framework, different ``pocket-universes'' may have different fundamental physical parameters. We don't know if we are typical observers and if we can safely assume that the physical laws we draw from our observations hold elsewhere. The third paper examines the validity of the appeal to the ``Sleeping Beauty problem'' and assesses the nature and role of typicality assumptions often endorsed to handle such questions. A more general issue for the use of probabilities in cosmology concerns the inadequacy of Bayesian and statistical model selection criteria in the absence of well-motivated measures for different cosmological models. The criteria for model selection commonly used tend to focus on optimizing the number of free parameters, but they can select physically implausible models. The fourth paper examines the possibility for Bayesian model selection to circumvent the lack of well-motivated priors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3KHY7NZV\\Benétreau-Dupin - Probabilistic Reasoning in Cosmology.pdf},
  language = {en}
}

@article{Berger1987,
  title = {Testing a {{Point Null Hypothesis}}: {{The Irreconcilability}} of {{P Values}} and {{Evidence}}},
  author = {Berger, James O. and Sellke, Thomas},
  year = {1987},
  volume = {82},
  pages = {112--122},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {01621459},
  abstract = {The problem of testing a point null hypothesis (or a "small interval" null hypothesis) is considered. Of interest is the relationship between the P value (or observed significance level) and conditional and Bayesian measures of evidence against the null hypothesis. Although one might presume that a small P value indicates the presence of strong evidence against the null, such is not necessarily the case. Expanding on earlier work [especially Edwards, Lindman, and Savage (1963) and Dickey (1977)], it is shown that actual evidence against a null (as measured, say, by posterior probability or comparative likelihood) can differ by an order of magnitude from the P value. For instance, data that yield a P value of .05, when testing a normal mean, result in a posterior probability of the null of at least .30 for any objective prior distribution. ("Objective" here means that equal prior weight is given the two hypotheses and that the prior is symmetric and nonincreasing away from the null; other definitions of "objective" will be seen to yield qualitatively similar results.) The overall conclusion is that P values can be highly misleading measures of the evidence provided by the data against the null hypothesis.},
  journal = {Journal of the American Statistical Association},
  number = {397}
}

@article{bermanTestingSpatialAssociation1986,
  title = {Testing for {{Spatial Association Between}} a {{Point Process}} and {{Another Stochastic Process}}},
  author = {Berman, Mark},
  year = {1986},
  volume = {35},
  pages = {54},
  issn = {00359254},
  doi = {10.2307/2347865},
  abstract = {Motivated by a problem in geology, this paper proposes some tests of the spatial association between a point process and some other stochastic process of geometric structures, G. All the tests are performed conditionally on the realization of G. Under the null hypothesis that the point process is a stationary Poisson process independent of G, some of these statistics have well-known distributional properties, even in small samples. The Poisson assumption is relaxed using a conditional Monte Carlo test suggested by Lotwick and Silverman (1982). The tests are applied to a geological data set.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X6TV3ANW\\Berman - 1986 - Testing for Spatial Association Between a Point Pr.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {1}
}

@article{berridgeAPPLICATIONMARKEDPOINT,
  title = {{{AN APPLICATION OF A MARKED POINT PROCESS IN PRE}}-{{CLINICAL MEDICINE}}},
  author = {Berridge, Damon M},
  pages = {12},
  abstract = {Several authors have employed marked point processes to model complex survival time data. In this paper it is proposed to illustrate further the flexibility of marked point processes by using such a framework to model pre-clinical data which comprise survival times accompanied by ordinal outcomes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\58P6VFL8\\Berridge - AN APPLICATION OF A MARKED POINT PROCESS IN PRE-CL.pdf;C\:\\Users\\devan\\Zotero\\storage\\GGH4BVH5\\Berridge - AN APPLICATION OF A MARKED POINT PROCESS IN PRE-CL.pdf},
  journal = {CLINICAL MEDICINE},
  language = {en}
}

@article{berthelsenNONPARAMETRICBAYESIANINFERENCE2008,
  title = {{{NON}}-{{PARAMETRIC BAYESIAN INFERENCE FOR INHOMOGENEOUS MARKOV POINT PROCESSES}}},
  author = {Berthelsen, Kasper K. and M{\o}ller, Jesper},
  year = {2008},
  month = sep,
  volume = {50},
  pages = {257--272},
  issn = {13691473, 1467842X},
  doi = {10.1111/j.1467-842X.2008.00516.x},
  abstract = {With reference to a specific data set, we consider how to perform a flexible non-parametric Bayesian analysis of an inhomogeneous point pattern modelled by a Markov point process, with a location dependent first order term and pairwise interaction only. A priori we assume that the first order term is a shot noise process, and the interaction function for a pair of points depends only on the distance between the two points and is a piecewise linear function modelled by a marked Poisson process. Simulation of the resulting posterior using a Metropolis-Hastings algorithm in the ``conventional'' way involves evaluating ratios of unknown normalising constants. We avoid this problem by applying a new auxiliary variable technique introduced by M\o ller, Pettitt, Reeves \& Berthelsen (2006). In the present setting the auxiliary variable used is an example of a partially ordered Markov point process model.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZVE5TC9F\\Berthelsen and Møller - 2008 - NON-PARAMETRIC BAYESIAN INFERENCE FOR INHOMOGENEOU.pdf},
  journal = {Australian \& New Zealand Journal of Statistics},
  language = {en},
  number = {3}
}

@article{berthelsenPrimerPerfectSimulation2002,
  title = {A Primer on Perfect Simulation for Spatial Point Processes},
  author = {Berthelsen, Kasper K. and M{\o}ller, Jesper},
  year = {2002},
  month = nov,
  volume = {33},
  pages = {351--367},
  issn = {1678-7544, 1678-7714},
  doi = {10.1007/s005740200019},
  abstract = {This primer provides a self-contained exposition of the case where spatial birth-and-death processes are used for perfect simulation of locally stable point processes. Particularly, a simple dominating coupling from the past (CFTP) algorithm and the CFTP algorithms introduced in [13], [14], and [5] are studied. Some empirical results for the algorithms are discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MQ7TJAMB\\Berthelsen and M�ller - 2002 - A primer on perfect simulation for spatial point p.pdf},
  journal = {Bulletin of the Brazilian Mathematical Society},
  language = {en},
  number = {3}
}

@article{bilderbeekBabetteBEAUtiBEAST22018,
  title = {Babette: {{BEAUti}} 2, {{BEAST2}} and {{Tracer}} for {{R}}},
  shorttitle = {Babette},
  author = {Bilderbeek, Rich{\`e}l J. C. and Etienne, Rampal S.},
  year = {2018},
  volume = {9},
  pages = {2034--2040},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13032},
  abstract = {In the field of phylogenetics, BEAST2 is one of the most widely used software tools. It comes with the graphical user interfaces BEAUti 2, DensiTree and Tracer, to create BEAST2 configuration files and to interpret BEAST2's output files. However, when many different alignments or model setups are required, a workflow of graphical user interfaces is cumbersome. Here, we present a free, libre and open-source package, babette: `BEAUti 2, BEAST2 and Tracer for R', for the R programming language. babette creates BEAST2 input files, runs BEAST2 and parses its results, all from an R function call. We describe babette's usage and the novel functionality it provides compared to the original tools and we give some examples. As babette is designed to be of high quality and extendable, we conclude by describing the further development of the package.},
  annotation = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13032},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JAUYG3JU\\Bilderbeek and Etienne - 2018 - babette BEAUti 2, BEAST2 and Tracer for R.pdf;C\:\\Users\\devan\\Zotero\\storage\\M8TI33QI\\2041-210X.html},
  journal = {Methods in Ecology and Evolution},
  keywords = {BEAST2,computational biology,evolution,phylogenetics,R},
  language = {en},
  number = {9}
}

@article{bivandSpatialDataAnalysis2015,
  title = {Spatial {{Data Analysis}} with {{{\emph{R}}}} - {{{\textbf{INLA}}}} with {{Some Extensions}}},
  author = {Bivand, Roger S. and {G{\'o}mez-Rubio}, Virgilio and Rue, H{\aa}vard},
  year = {2015},
  volume = {63},
  issn = {1548-7660},
  doi = {10.18637/jss.v063.i20},
  abstract = {The integrated nested Laplace approximation (INLA) provides an interesting way of approximating the posterior marginals of a wide range of Bayesian hierarchical models. This approximation is based on conducting a Laplace approximation of certain functions and numerical integration is extensively used to integrate some of the models parameters out.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GH7VCE69\\Bivand et al. - 2015 - Spatial Data Analysis with iRi - bINLAb .pdf;C\:\\Users\\devan\\Zotero\\storage\\WEG9W9NK\\Bivand et al. - 2015 - Spatial Data Analysis with iRi - bINLAb .pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {20}
}

@article{blangiardoSpatialSpatioTemporalModels,
  title = {Spatial and {{Spatio}}-{{Temporal}} Models with {{R}}-{{INLA}}},
  author = {Blangiardo, Marta and Cameletti, Michela and Baio, Gianluca},
  pages = {38},
  abstract = {During the last three decades, Bayesian methods have developed greatly in the field of epidemiology. Their main challenge focusses around computation, but the advent of Markov Chain Monte Carlo methods (MCMC) and in particular of the WinBUGS software has opened the doors of Bayesian modelling to the wide research community. However model complexity and database dimension still remain a constraint.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MT99856H\\Blangiardo et al. - 2013 - Spatial and spatio-temporal models with R-INLA.pdf;C\:\\Users\\devan\\Zotero\\storage\\W5A5UR9A\\Blangiardo et al. - Spatial and Spatio-Temporal models with R-INLA.pdf},
  language = {en}
}

@article{boychukAssemblingCustomizingMultiple2020,
  title = {Assembling and {{Customizing Multiple Fire Weather Forecasts}} for {{Burn Probability}} and {{Other Fire Management Applications}} in {{Ontario}}, {{Canada}}},
  author = {Boychuk, Den and McFayden, Colin B. and Evens, Jordan and Shields, Jerry and Stacey, Aaron and Woolford, Douglas G. and Wotton, Mike and Johnston, Dan and Leonard, Dan and McLarty, Darren},
  year = {2020},
  month = jun,
  volume = {3},
  pages = {16},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/fire3020016},
  abstract = {Weather forecasts are needed in fire management to support risk-based decision-making that considers both the probability of an outcome and its potential impact. These decisions are complicated by the large amount of uncertainty surrounding many aspects of the decision, such as weather forecasts. Wildland fires in Ontario, Canada can burn and actively spread for days, weeks, or even months, or be naturally limited or extinguished by rain. Conventional fire weather forecasts have typically been a single scenario for a period of one to five days. These forecasts have two limitations: they are not long enough to inform some fire management decisions, and they do not convey any uncertainty to inform risk-based decision-making. We present an overview of a method for the assembly and customization of forecasts that (1) combines short-, medium-, and long-term forecasts of different types, (2) calculates Fire Weather Indices and Fire Behaviour Predictions, including modelling seasonal weather station start-up and shutdown, (3) resolves differing spatial resolutions, and (4) communicates forecasts. It is used for burn probability modelling and other fire management applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\devan\\Zotero\\storage\\322SDGGB\\Boychuk et al. - 2020 - Assembling and Customizing Multiple Fire Weather F.pdf;C\:\\Users\\devan\\Zotero\\storage\\6PD5VSMH\\16.html},
  journal = {Fire},
  keywords = {analogue forecast,decision support,ensemble,forest fire,risk management,wildfire},
  language = {en},
  number = {2}
}

@misc{braunForestFireRisk2010,
  title = {Forest {{Fire Risk Assessment}}: {{An Illustrative Example}} from {{Ontario}}, {{Canada}}},
  shorttitle = {Forest {{Fire Risk Assessment}}},
  author = {Braun, W. John and Jones, Bruce L. and Lee, Jonathan S. W. and Woolford, Douglas G. and Wotton, B. Mike},
  year = {2010},
  doi = {10.1155/2010/823018},
  abstract = {This paper presents an analysis of ignition and burn risk due to wildfire in a region of Ontario, Canada using a methodology which is applicable to the entire boreal forest region. A generalized additive model was employed to obtain ignition risk probabilities and a burn probability map using only historic ignition and fire area data. Constructing fire shapes according to an accurate physical model for fire spread, using a fuel map and realistic weather scenarios is possible with the Prometheus fire growth simulation model. Thus, we applied the Burn-P3 implementation of Prometheus to construct a more accurate burn probability map. The fuel map for the study region was verified and corrected. Burn-P3 simulations were run under the settings (related to weather) recommended in the software documentation and were found to be fairly robust to errors in the fuel map, but simulated fire sizes were substantially larger than those observed in the historic record. By adjusting the input parameters to reflect suppression effects, we obtained a model which gives more appropriate fire sizes. The resulting burn probability map suggests that risk of fire in the study area is much lower than what is predicted by Burn-P3 under its recommended settings.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DBBB5QLE\\Braun et al. - 2010 - Forest Fire Risk Assessment An Illustrative Examp.pdf;C\:\\Users\\devan\\Zotero\\storage\\XPSPFPKK\\abs.html},
  howpublished = {https://www.hindawi.com/journals/jps/2010/823018/abs/},
  journal = {Journal of Probability and Statistics},
  language = {en},
  type = {Research Article}
}

@article{braunFORESTSFIRESSTOCHASTIC,
  title = {{{FORESTS}}, {{FIRES}} and {{STOCHASTIC MODELLING}}},
  author = {Braun, John and Dean, Charmaine and He, Fangliang and Martell, David and Preisler, Haiganoush},
  pages = {11},
  file = {C\:\\Users\\devan\\Zotero\\storage\\J2JJVULN\\Braun et al. - FORESTS, FIRES and STOCHASTIC MODELLING.pdf},
  language = {en}
}

@incollection{brewerColorUseGuidelines1994,
  title = {Color {{Use Guidelines}} for {{Mapping}} and {{Visualization}}},
  booktitle = {Modern {{Cartography Series}}},
  author = {Brewer, Cynthia A.},
  year = {1994},
  volume = {2},
  pages = {123--147},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-08-042415-6.50014-4},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DTJYFZQY\\Brewer - 1994 - Color Use Guidelines for Mapping and Visualization.pdf;C\:\\Users\\devan\\Zotero\\storage\\SSDUR5KA\\Brewer - 1994 - Color Use Guidelines for Mapping and Visualization.pdf},
  isbn = {978-0-08-042415-6},
  language = {en}
}

@article{brillingerExploratoryDataAnalysis2004,
  title = {An Exploratory Data Analysis ({{EDA}}) of the Paths of Moving Animals},
  author = {Brillinger, David R and Preisler, Haiganoush K and Ager, Alan A and Kie, John G},
  year = {2004},
  month = may,
  volume = {122},
  pages = {43--63},
  issn = {03783758},
  doi = {10.1016/j.jspi.2003.06.016},
  abstract = {This work presents an exploratory data analysis of the trajectories of deer and elk moving about in the Starkey Experimental Forest and Range in eastern Oregon. The animals' movements may be a ected by habitat variables and the behavior of the other animals. In the work of this paper a stochastic di erential equation-based model is developed in successive stages. Equations of motion are set down motivated by corresponding equations of physics. Functional parameters appearing in the equations are estimated nonparametrically and plots of vector \"yelds of animal movements are prepared. Residuals are used to look for interactions amongst the movements of the animals. There are exploratory analyses of various sorts. Statistical inferences are based on Fourier transforms of the data, which are unequally spaced. The sections of the paper start with motivating quotes and aphorisms from the writings of John W. Tukey.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8XWTMW9X\\Brillinger et al. - 2004 - An exploratory data analysis (EDA) of the paths of.pdf},
  journal = {Journal of Statistical Planning and Inference},
  language = {en},
  number = {1-2}
}

@article{brillingerExploratoryDataAnalysis2014,
  title = {An Exploratory Data Analysis of the Temperature Fluctuations in a Spreading Fire: {{AN EDA OF ADVANCING FIRE}}},
  shorttitle = {An Exploratory Data Analysis of the Temperature Fluctuations in a Spreading Fire},
  author = {Brillinger, David R. and Finney, Mark A.},
  year = {2014},
  month = sep,
  volume = {25},
  pages = {443--453},
  issn = {11804009},
  doi = {10.1002/env.2279},
  abstract = {A series of fire experiments were carried out in a wind tunnel at the United States Forest Service's Fire Science Laboratory in Missoula, Montana. The experiments involved tines cut out of pieces of cardboard. The pieces were laid out in comb-like strips parallel to each other along a testbed. They were ignited at the windward end of the testbed. The progress of the fire was monitored by thermocouples, recording temperature, set out equidistantly up the middle of the testbed. Goals of the experiment included improved understanding of wildfire spread and the development of practical tools for wild land fire managers to employ. There was to be a search for regular pulsing in the series and any other interesting phenomena. This paper presents the results of a variety of exploratory data analyses meant to elicit information concerning the series before commencing probability modeling.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F7T7DDLA\\Brillinger and Finney - 2014 - An exploratory data analysis of the temperature fl.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZCX7F5XM\\Brillinger and Finney - 2014 - An exploratory data analysis of the temperature fl.pdf},
  journal = {Environmetrics},
  keywords = {EDA,Fire Spread,Size},
  language = {en},
  number = {6}
}

@article{brillingerProbabilisticRiskAssessment2006,
  title = {Probabilistic Risk Assessment for Wildfires},
  author = {Brillinger, David R. and Preisler, H. K. and Benoit, J. W.},
  year = {2006},
  month = sep,
  volume = {17},
  pages = {623--633},
  issn = {1180-4009, 1099-095X},
  doi = {10.1002/env.768},
  abstract = {Forest fires are an important societal problem. They cause extensive damage and substantial funds are spent preparing for and fighting them. This work develops a stochastic model useful for probabilistic risk assessment, specifically to estimate chances of fires at a future time given explanatory variables. Questions of interest include: Are random effects needed in the risk model? and if yes, How is the analysis to be implemented? An exploratory data analysis approach is taken using both fixed and random effects models for data concerning the Federal Lands in the state of California during the period 2000\textendash 2003. Published in 2006 by John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JJ7LW9F5\\Brillinger et al. - 2006 - Probabilistic risk assessment for wildfires.pdf},
  journal = {Environmetrics},
  keywords = {EDA,Ignition Probability,Random Effects},
  language = {en},
  number = {6}
}

@article{brillingerRiskAssessmentForest2003,
  title = {Risk {{Assessment}}: {{A Forest Fire Example}}},
  author = {Brillinger, David R. and Preisler, Haiganoush K. and Benoit, John W.},
  year = {2003},
  volume = {40},
  pages = {177--196},
  abstract = {The concern of this paper is obtaining baseline values for the number of forest fires as a function of time and location and other explanatories. A model is developed and applied to a large data set from Federal lands in the state of Oregon. To proceed the data are grouped into small spatial-temporal cells (voxels). Fires are rare so there are many of these voxels with no fires. In fact there are so many such cells that in the analyses presented a sample is taken to make the work manageable. The paper sets down a likelihood for the sampled data and fits a generalized additive model involving location, elevation and day of the year as explanatories.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Z7TQKZLQ\\Brillinger et al. - 2003 - Risk Assessment A Forest Fire Example.pdf},
  journal = {Lecture Notes-Monograph Series},
  keywords = {Count,Covariates},
  language = {en}
}

@article{brixSpaceTimeMultiType2001,
  title = {Space-{{Time Multi Type Log Gaussian Cox Processes}} with a {{View}} to {{Modelling Weeds}}},
  author = {Brix, Anders and M{\o}ller, Jesper},
  year = {2001},
  volume = {28},
  pages = {471--488},
  abstract = {Log Gaussian Cox processes as introduced in M0ller et al. (1998) are extended to space-time models called log Gaussian Cox birth processes. These processes allow modelling of spatial and temporal heterogeneity in time series of increasing point processes consisting of different types of points. The models are shown to be easy to analyse, yet flexible enough for a detailed statistical analysis of a particular agricultural experiment concerning the development of two weed species on an organic barley field. Particularly, the aspects of estimation, model validation and intensity surface prediction are discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JCYWTNYG\\Brix and Møller - 2001 - Space-Time Multi Type Log Gaussian Cox Processes w.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {3}
}

@article{brixSpatiotemporalPredictionLogGaussian2001,
  title = {Spatiotemporal {{Prediction}} for {{Log}}-{{Gaussian Cox Processes}}},
  author = {Brix, Anders and Diggle, Peter J.},
  year = {2001},
  volume = {63},
  pages = {823--841},
  abstract = {Space-time pointpatterndata have become more widelyavailable as a resultof technologicaldevelopmentsin areas such as geographic informatiosnystems. We descrbe a flexibleclass of space-time pointprocesses. Our models are Cox processes whose stochastic intensityis a space-time Ornstein-Uhlenbeckprocess. We develop moment-basedmethodsof parameterestimations,how howto predictthe underlyinigntensitybyusinga Markovchain Monte Carlo approach and illustrattehe performanceofour methodson a syntheticdata set.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FJHVSHPS\\Brix and Diggle - 2001 - Spatiotemporal Prediction for Log-Gaussian Cox Pro.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  language = {en},
  number = {4}
}

@article{brownComparisonSpatialStatistics,
  title = {Comparison of Spatial Statistics for Identifying Underlying Process in Forest Ecology},
  author = {Brown, Calum and Illian, Janine and Burslem, David},
  pages = {4},
  abstract = {A number of different mechanisms have been suggested to explain species coexistence in diverse communities such as tropical rainforests. Spatial statistics appear to hold great potential for distinguishing the effects of these in empirical data, and a wide range of measures intended to describe spatial structure have been proposed. Using patterns generated by stochastic individual-based models, we examine the relative sensitivity of several of these measures to processes thought to be occurring in tropical rainforests, and so assess the potential for identifying specific coexistence mechanisms from empirical data. We then apply the measures to spatially explicit census data from a number of large-scale tropical rainforest plots in order to investigate the manifestation of ecological processes in forest spatial structure.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\U9F5XNCG\\Brown et al. - Comparison of spatial statistics for identifying u.pdf},
  language = {en}
}

@article{brownFrequencySpatialDistribution2004,
  title = {Frequency and {{Spatial Distribution}} of {{Environmental Campylobacter}} Spp.},
  author = {Brown, P. E. and Christensen, O. F. and Clough, H. E. and Diggle, P. J. and Hart, C. A. and Hazel, S. and Kemp, R. and Leatherbarrow, A. J. H. and Moore, A. and Sutherst, J. and Turner, J. and Williams, N. J. and Wright, E. J. and French, N. P.},
  year = {2004},
  month = nov,
  volume = {70},
  pages = {6501--6511},
  issn = {0099-2240},
  doi = {10.1128/AEM.70.11.6501-6511.2004},
  file = {C\:\\Users\\devan\\Zotero\\storage\\L28IRW6U\\Brown et al. - 2004 - Frequency and Spatial Distribution of Environmenta.pdf;C\:\\Users\\devan\\Zotero\\storage\\TK9KJ578\\Brown et al. - 2004 - Frequency and Spatial Distribution of Environmenta.pdf},
  journal = {Applied and Environmental Microbiology},
  language = {en},
  number = {11}
}

@article{brownLocalEMMismeasuredData2013,
  title = {Local-{{EM}} and Mismeasured Data},
  author = {Brown, Patrick E. and Nguyen, Paul and Stafford, Jamie and Ashta, Shiva},
  year = {2013},
  month = jan,
  volume = {83},
  pages = {135--140},
  issn = {01677152},
  doi = {10.1016/j.spl.2012.08.030},
  abstract = {This note uses the established connection between the local-EM and EMS algorithms to explore methods for smoothing mismeasured data. Implementations of local-EM are developed for several forms of censored and/or mismeasured data and their connections with other methods explored.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C9QAAXMD\\Brown et al. - 2013 - Local-EM and mismeasured data.pdf;C\:\\Users\\devan\\Zotero\\storage\\JFU7L92T\\Brown et al. - 2013 - Local-EM and mismeasured data.pdf},
  journal = {Statistics \& Probability Letters},
  language = {en},
  number = {1}
}

@article{brownMapsCoordinateReference2016,
  title = {Maps, {{Coordinate Reference Systems}} and {{Visualising Geographic Data}} with Mapmisc},
  author = {Brown, E., Patrick},
  year = {2016},
  volume = {8},
  pages = {64},
  issn = {2073-4859},
  doi = {10.32614/RJ-2016-005},
  abstract = {The mapmisc package provides functions for visualising geospatial data, including fetching background map layers, producing colour scales and legends, and adding scale bars and orientation arrows to plots. Background maps are returned in the coordinate reference system of the dataset supplied, and inset maps and direction arrows reflect the map projection being plotted. This is a `light weight' package having an emphasis on simplicity and ease of use.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QGTNE4ZU\\Brown - 2016 - Maps, Coordinate Reference Systems and Visualising.pdf;C\:\\Users\\devan\\Zotero\\storage\\XXAA5F7A\\Brown - 2016 - Maps, Coordinate Reference Systems and Visualising.pdf},
  journal = {The R Journal},
  language = {en},
  number = {1}
}

@article{brownMCMCGeneralizedLinear2010,
  title = {{{MCMC}} for {{Generalized Linear Mixed Models}} with {{glmmBUGS}}},
  author = {Brown, Patrick and Zhou, Lutong},
  year = {2010},
  volume = {2},
  pages = {13},
  issn = {2073-4859},
  doi = {10.32614/RJ-2010-003},
  abstract = {The glmmBUGS package is a bridging tool between Generalized Linear Mixed Models (GLMMs) in R and the BUGS language. It provides a simple way of performing Bayesian inference using Markov Chain Monte Carlo (MCMC) methods, taking a model formula and data frame in R and writing a BUGS model file, data file, and initial values files. Functions are provided to reformat and summarize the BUGS results. A key aim of the package is to provide files and objects that can be modified prior to calling BUGS, giving users a platform for customizing and extending the models to accommodate a wide variety of analyses.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8DT3H4HA\\Brown and Zhou - 2010 - MCMC for Generalized Linear Mixed Models with glmm.pdf},
  journal = {The R Journal},
  language = {en},
  number = {1}
}

@article{brownModelBasedGeostatisticsEasy2015,
  title = {Model-{{Based Geostatistics}} the {{Easy Way}}},
  author = {Brown, Patrick E.},
  year = {2015},
  volume = {63},
  issn = {1548-7660},
  doi = {10.18637/jss.v063.i12},
  abstract = {This paper briefly describes geostatistical models for Gaussian and non-Gaussian data and demonstrates the geostatsp and geostatsinla packages for performing inference using these models. Making use of R's spatial data types, and raster objects in particular, makes spatial analyses using geostatistical models simple and convenient. Examples using real data are shown for Gaussian spatial data, binomially distributed spatial data, a logGaussian Cox process, and an area-level model for case counts.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AI8XPTJ3\\Brown - 2015 - Model-Based Geostatistics the Easy Way.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {12}
}

@article{brownNonGaussianSpatialProcess2003,
  title = {A {{Non}}-{{Gaussian Spatial Process Model}} for {{Opacity}} of {{Flocculated Paper}}},
  author = {Brown, Patrick E. and Diggle, Peter J. and Henderson, Robin},
  year = {2003},
  volume = {30},
  pages = {355--368},
  abstract = {Product quality in the paper-making industry can be assessed by opacity of a linear trace through continuous production sheets, summarized in spectral form. We adopt a class of nonGaussian stochastic models for continuous spatial variation to describe data of this type. The model has flexible covariance structure, physically interpretable parameters and allows several scales of variation for the underlying process. We derive the spectral properties of the model, and develop methods of parameter estimation based on maximum likelihood in the frequency domain. The methods are illustrated using sample data from a UK mill.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XQZHFK6B\\Brown et al. - 2003 - A Non-Gaussian Spatial Process Model for Opacity o.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {2}
}

@article{brownNonparametricSmoothingUsing2001,
  title = {Nonparametric Smoothing Using State Space Techniques},
  author = {Brown, Patrick E. and De Jong, Piet},
  year = {2001},
  month = mar,
  volume = {29},
  pages = {37--50},
  issn = {03195724, 1708945X},
  doi = {10.2307/3316049},
  abstract = {The authors examine the equivalence between penalized least squares and state space smoothing using random vectors with infinite variance. They show that despite infinite variance, many time series techniques for estimation, significance testing, and diagnostics can be used. The Kalman filter can be used to fit penalized least squares models, computing the smoothed quantities and related values. Infinite variance is equivalent to differencing to stationarity, and to adding explanatory variables. The authors examine constructs called "smoothations" which they show to be fundamental in smoothing. Applications illustrate concepts and methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H2JP9J4D\\Brown and De Jong - 2001 - Nonparametric smoothing using state space techniqu.pdf;C\:\\Users\\devan\\Zotero\\storage\\W7UZLB5B\\Brown and De Jong - 2001 - Nonparametric smoothing using state space techniqu.pdf},
  journal = {Canadian Journal of Statistics},
  language = {en},
  number = {1}
}

@article{brownSimulationbasedPowerCalculations2010,
  title = {Simulation-Based Power Calculations for Large Cohort Studies},
  author = {Brown, Patrick and Jiang, Hedy},
  year = {2010},
  month = oct,
  volume = {52},
  pages = {604--615},
  issn = {03233847},
  doi = {10.1002/bimj.200900277},
  abstract = {A large number of factors can affect the statistical power and bias of analyses of data from large cohort studies, including misclassification, correlated data, followup time, prevalence of the risk factor of interest, and prevalence of the outcome. This paper presents a method for simulating cohorts where individual's risk is correlated within communities, recruitment is staggered over time, and outcomes are observed after different followup periods. Covariates and outcomes are misclassified, and Cox proportional hazards models are fit with a community-level frailty term. The Cox proportional hazards model is shown to produce unbiased tests in the presence of the aforementioned factors, and the effect on study power of changing various variables is shown.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CURQFCV5\\Brown and Jiang - 2010 - Simulation-based power calculations for large coho.pdf;C\:\\Users\\devan\\Zotero\\storage\\DVFBLR3J\\Brown and Jiang - 2010 - Simulation-based power calculations for large coho.pdf},
  journal = {Biometrical Journal},
  language = {en},
  number = {5}
}

@article{brownStatisticalInferenceComputational2014,
  title = {Statistical Inference and Computational Efficiency for Spatial Infectious Disease Models with Plantation Data},
  author = {Brown, Patrick E. and Chimard, Florencia and Remorov, Alexander and Rosenthal, Jeffrey S. and Wang, Xin},
  year = {2014},
  month = apr,
  volume = {63},
  pages = {467--482},
  issn = {00359254},
  doi = {10.1111/rssc.12036},
  abstract = {This paper considers data from an aphid infestation on a sugar cane plantation, and illustrates the use of an individual-level infectious disease model for making inference on the biological process underlying these data. The data are interval censored, and the practical issues involved with the use of Markov Chain Monte Carlo algorithms with models of this sort are explored and developed. As inference for spatial infectious disease models is complex and computationally demanding, emphasis is put on a minimal, parsimonious model and speed of code execution.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ICGUF83R\\Brown et al. - 2014 - Statistical inference and computational efficiency.pdf},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  language = {en},
  number = {3}
}

@article{brownSuccessSpatialStatistics2016,
  title = {Success of Spatial Statistics in Determining Underlying Process in Simulated Plant Communities},
  author = {Brown, Calum and Illian, Janine B. and Burslem, David F. R. P.},
  editor = {Sandhu, Lauren},
  year = {2016},
  month = jan,
  volume = {104},
  pages = {160--172},
  issn = {00220477},
  doi = {10.1111/1365-2745.12493},
  file = {C\:\\Users\\devan\\Zotero\\storage\\I8NWLHSD\\Brown et al. - 2016 - Success of spatial statistics in determining under.pdf;C\:\\Users\\devan\\Zotero\\storage\\LFPQJCYA\\Brown et al. - 2016 - Success of spatial statistics in determining under.pdf},
  journal = {Journal of Ecology},
  language = {en},
  number = {1}
}

@article{brunetMetagenesMolecularPattern2004,
  title = {Metagenes and Molecular Pattern Discovery Using Matrix Factorization},
  author = {Brunet, J.P. and Tamayo, P. and Golub, T. R. and Mesirov, J. P.},
  year = {2004},
  month = mar,
  volume = {101},
  pages = {4164--4169},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0308531101},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LVBEJ5HJ\\Brunet et al. - 2004 - Metagenes and molecular pattern discovery using ma.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {12}
}

@article{buja2009statistical,
  title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
  author = {Buja, Andreas and Cook, Dianne and Hofmann, Heike and Lawrence, Michael and Lee, Eun-Kyung and Swayne, Deborah F and Wickham, Hadley},
  year = {2009},
  volume = {367},
  pages = {4361--4383},
  publisher = {{The Royal Society}},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  number = {1906}
}

@article{bujaStatisticalInferenceExploratory2009,
  title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
  author = {Buja, Andreas and Cook, Dianne and Hofmann, Heike and Lawrence, Michael and Lee, Eun-Kyung and Swayne, Deborah F. and Wickham, Hadley},
  year = {2009},
  month = nov,
  volume = {367},
  pages = {4361--4383},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2009.0120},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QRWZSWW3\\Buja et al. - 2009 - Statistical inference for exploratory data analysi.pdf},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {1906}
}

@phdthesis{burkeHistoricFiresCentral2014,
  title = {{Historic fires in the central Western Cascades, Oregon}},
  author = {Burke, Constance J.},
  year = {2014},
  month = feb,
  language = {http://id.loc.gov/vocabulary/iso639-2/eng},
  school = {Oregon State University}
}

@article{busschaertEstimatingDistributionsOut2010,
  title = {Estimating Distributions out of Qualitative and (Semi)Quantitative Microbiological Contamination Data for Use in Risk Assessment},
  author = {Busschaert, P. and Geeraerd, A. H. and Uyttendaele, M. and Van Impe, J. F.},
  year = {2010},
  month = apr,
  volume = {138},
  pages = {260--269},
  issn = {0168-1605},
  doi = {10.1016/j.ijfoodmicro.2010.01.025},
  abstract = {A framework using maximum likelihood estimation (MLE) is used to fit a probability distribution to a set of qualitative (e.g., absence in 25g), semi-quantitative (e.g., presence in 25g and absence in 1g) and/or quantitative test results (e.g., 10CFU/g). Uncertainty about the parameters of the variability distribution is characterized through a non-parametric bootstrapping method. The resulting distribution function can be used as an input for a second order Monte Carlo simulation in quantitative risk assessment. As an illustration, the method is applied to two sets of in silico generated data. It is demonstrated that correct interpretation of data results in an accurate representation of the contamination level distribution. Subsequently, two case studies are analyzed, namely (i) quantitative analyses of Campylobacter spp. in food samples with nondetects, and (ii) combined quantitative, qualitative, semiquantitative analyses and nondetects of Listeria monocytogenes in smoked fish samples. The first of these case studies is also used to illustrate what the influence is of the limit of quantification, measurement error, and the number of samples included in the data set. Application of these techniques offers a way for meta-analysis of the many relevant yet diverse data sets that are available in literature and (inter)national reports of surveillance or baseline surveys, therefore increases the information input of a risk assessment and, by consequence, the correctness of the outcome of the risk assessment.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VB777QN4\\Busschaert et al. - 2010 - Estimating distributions out of qualitative and (s.pdf;C\:\\Users\\devan\\Zotero\\storage\\CYM6QMGN\\S0168160510000425.html},
  journal = {International Journal of Food Microbiology},
  keywords = {Bootstrapping method,Maximum likelihood estimation (MLE),Quantitative microbiological risk assessment,Second order Monte Carlo simulation,Survival analysis},
  number = {3}
}

@article{camelettiSpatiotemporalModelingParticulate2013,
  title = {Spatio-Temporal Modeling of Particulate Matter Concentration through the {{SPDE}} Approach},
  author = {Cameletti, Michela and Lindgren, Finn and Simpson, Daniel and Rue, H{\aa}vard},
  year = {2013},
  month = apr,
  volume = {97},
  pages = {109--131},
  issn = {1863-8171, 1863-818X},
  doi = {10.1007/s10182-012-0196-3},
  file = {C\:\\Users\\devan\\Zotero\\storage\\STCGBZ22\\Cameletti et al. - 2013 - Spatio-temporal modeling of particulate matter con.pdf},
  journal = {AStA Advances in Statistical Analysis},
  language = {en},
  number = {2}
}

@misc{caneUsingShotLocation2014,
  title = {Using {{Shot Location Data}} for {{Team}} and {{Player Strategy}}},
  author = {Cane, Matt},
  year = {2014},
  address = {{Pittsburgh}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8E4YE8MV\\Cane - Using Shot Location Data for Team and Player Strat.pdf},
  language = {en}
}

@article{catalanoBivariateLatentVariable,
  title = {Bivariate {{Latent Variable Models}} for {{Clustered Discrete}} and {{Continuous Outcomes}}},
  author = {Catalano, Paul J and Ryan, Louise M},
  pages = {9},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CA4I8S6M\\Catalano and Ryan - Bivariate Latent Variable Models for Clustered Dis.pdf;C\:\\Users\\devan\\Zotero\\storage\\G7PBH4X6\\Catalano and Ryan - Bivariate Latent Variable Models for Clustered Dis.pdf},
  language = {en}
}

@article{celeuxDevianceInformationCriteria2006,
  title = {Deviance Information Criteria for Missing Data Models},
  author = {Celeux, G. and Forbesy, F. and Robertz, C.P. and Titteringtonx, D.M.},
  year = {2006},
  volume = {1},
  pages = {651--674},
  doi = {10.1214/06-BA122},
  abstract = {The deviance information criterion (DIC) introduced by Spiegelhalter et al. (2002) for model assessment and model comparison is directly inspired by linear and generalised linear models, but it is open to different possible variations in the setting of missing data models, depending in particular on whether or not the missing variables are treated as parameters. In this paper, we reassess the criterion for such models and compare different DIC constructions, testing the behaviour of these various extensions in the cases of mixtures of distributions and random effect models. \textcopyright{} 2006 International Society for Bayesian Analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GQ3R9ITK\\Celeux et al. - 2006 - Deviance information criteria for missing data mod.pdf;C\:\\Users\\devan\\Zotero\\storage\\WGLJMP5Q\\display.html},
  journal = {Bayesian Analysis},
  keywords = {Completion,Deviance,DIC,EM algorithm,MAP,Mixture model,Model comparison,Random effect model},
  number = {4}
}

@article{cemgilBayesianInferenceNonnegative2009,
  title = {Bayesian {{Inference}} for {{Nonnegative Matrix Factorisation Models}}},
  author = {Cemgil, Ali Taylan},
  year = {2009},
  volume = {2009},
  issn = {1687-5265},
  doi = {10.1155/2009/785152},
  abstract = {We describe nonnegative matrix factorisation (NMF) with a Kullback-Leibler (KL) error measure in a statistical framework, with a hierarchical generative model consisting of an observation and a prior component. Omitting the prior leads to the standard KL-NMF algorithms as special cases, where maximum likelihood parameter estimation is carried out via the Expectation-Maximisation (EM) algorithm. Starting from this view, we develop full Bayesian inference via variational Bayes or Monte Carlo. Our construction retains conjugacy and enables us to develop more powerful models while retaining attractive features of standard NMF such as monotonic convergence and easy implementation. We illustrate our approach on model order selection and image reconstruction.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\I8FXDTDQ\\Cemgil - 2009 - Bayesian Inference for Nonnegative Matrix Factoris.pdf},
  journal = {Computational Intelligence and Neuroscience},
  pmcid = {PMC2688815},
  pmid = {19536273}
}

@article{cervoneMultiresolutionStochasticProcess2016,
  title = {A {{Multiresolution Stochastic Process Model}} for {{Predicting Basketball Possession Outcomes}}},
  author = {Cervone, Daniel and D'Amour, Alex and Bornn, Luke and Goldsberry, Kirk},
  year = {2016},
  month = apr,
  volume = {111},
  pages = {585--599},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2016.1141685},
  abstract = {Basketball games evolve continuously in space and time as players constantly interact with their teammates, the opposing team, and the ball. However, current analyses of basketball outcomes rely on discretized summaries of the game that reduce such interactions to tallies of points, assists, and similar events. In this paper, we propose a framework for using optical player tracking data to estimate, in real time, the expected number of points obtained by the end of a possession. This quantity, called expected possession value (EPV), derives from a stochastic process model for the evolution of a basketball possession. We model this process at multiple levels of resolution, differentiating between continuous, infinitesimal movements of players, and discrete events such as shot attempts and turnovers. Transition kernels are estimated using hierarchical spatiotemporal models that share information across players while remaining computationally tractable on very large data sets. In addition to estimating EPV, these models reveal novel insights on players' decision-making tendencies as a function of their spatial strategy. A data sample and R code for further exploration of our model/results are available in the repository https://github.com/dcervone/EPVDemo.},
  archivePrefix = {arXiv},
  eprint = {1408.0777},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5PVNTTRQ\\1408.0777.pdf},
  journal = {Journal of the American Statistical Association},
  keywords = {Statistics - Applications,Statistics - Computation},
  language = {en},
  number = {514}
}

@article{chaliseIntegrativeClusteringMultilevel2017,
  title = {Integrative Clustering of Multi-Level `omic Data Based on Non-Negative Matrix Factorization Algorithm},
  author = {Chalise, Prabhakar and Fridley, Brooke L.},
  editor = {Peddada, Shyamal D},
  year = {2017},
  month = may,
  volume = {12},
  pages = {e0176278},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0176278},
  abstract = {Integrative analyses of high-throughput `omic data, such as DNA methylation, DNA copy number alteration, mRNA and protein expression levels, have created unprecedented opportunities to understand the molecular basis of human disease. In particular, integrative analyses have been the cornerstone in the study of cancer to determine molecular subtypes within a given cancer. As malignant tumors with similar morphological characteristics have been shown to exhibit entirely different molecular profiles, there has been significant interest in using multiple `omic data for the identification of novel molecular subtypes of disease, which could impact treatment decisions. Therefore, we have developed intNMF, an integrative approach for disease subtype classification based on non-negative matrix factorization. The proposed approach carries out integrative clustering of multiple high dimensional molecular data in a single comprehensive analysis utilizing the information across multiple biological levels assessed on the same individual. As intNMF does not assume any distributional form for the data, it has obvious advantages over other model based clustering methods which require specific distributional assumptions. Application of intNMF is illustrated using both simulated and real data from The Cancer Genome Atlas (TCGA).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NP6ZLKDW\\Chalise and Fridley - 2017 - Integrative clustering of multi-level ‘omic data b.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {5}
}

@article{chanFastComputationDeviance2016,
  title = {Fast Computation of the Deviance Information Criterion for Latent Variable Models},
  author = {Chan, Joshua C. C. and Grant, Angelia L.},
  year = {2016},
  month = aug,
  volume = {100},
  pages = {847--859},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2014.07.018},
  abstract = {The deviance information criterion (DIC) has been widely used for Bayesian model comparison. However, recent studies have cautioned against the use of certain variants of the DIC for comparing latent variable models. For example, it has been argued that the conditional DIC\textendash based on the conditional likelihood obtained by conditioning on the latent variables\textendash is sensitive to transformations of latent variables and distributions. Further, in a Monte Carlo study that compares various Poisson models, the conditional DIC almost always prefers an incorrect model. In contrast, the observed-data DIC\textendash calculated using the observed-data likelihood obtained by integrating out the latent variables\textendash seems to perform well. It is also the case that the conditional DIC based on the maximum a posteriori (MAP) estimate might not even exist, whereas the observed-data DIC does not suffer from this problem. In view of these considerations, fast algorithms for computing the observed-data DIC for a variety of high-dimensional latent variable models are developed. Through three empirical applications it is demonstrated that the observed-data DICs have much smaller numerical standard errors compared to the conditional DICs. The corresponding Matlab code is available upon request.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DRDMNUMD\\Chan and Grant - 2016 - Fast computation of the deviance information crite.pdf;C\:\\Users\\devan\\Zotero\\storage\\S5NE6DJV\\S0167947314002321.html},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {Bayesian model comparison,Factor model,Semiparametric model,State space,Vector autoregression}
}

@misc{Chang2018,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Xie, Yihui and McPherson, Jonathan},
  year = {2018}
}

@article{changMonitoringLinearityMeasurement2006,
  title = {Monitoring Linearity of Measurement Gauges},
  author = {Chang, Tee-Chin and Gan, Fah-Fatt},
  year = {2006},
  month = oct,
  volume = {76},
  pages = {889--911},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/10629360500282171},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CZU4DDHM\\Chang and Gan - 2006 - Monitoring linearity of measurement gauges.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {10}
}

@phdthesis{changSeparabilityTestingPoint,
  title = {Separability {{Testing}} for {{Point Processes}} with {{Covariates}} and {{An Application}} to {{Wildfire Hazard Assessment}}},
  author = {Chang, Chien-Hsun},
  year = {2007},
  address = {{Los Angeles}},
  abstract = {In modeling marked point processes, it is typically assumed that marks are sep- arable from the spatial-temporal coordinates. Tests have been proposed in the simple marked point process case to investigate the separability of the mark dis- tribution. These tests are here extended to the case of a marked point process with covariates. The extension is not trivial, and covariates must be treated in a fundamentally different way than marks and coordinates of the process, espe- cially when covariates are not uniformly distributed. Solutions are proposed to the problem of how to proceed when the separability hypothesis is rejected. An application of separable marked point process models with covariates is given to the assessment of the Burning Index in predicting wildfire activity. An exami- nation of the Los Angeles County data reveals that the Burning Index predicts poorly compared to simple alternatives using just a few weather variables.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PYLT7JFC\\Chang - 2007 - Separability Testing for Point Processes with Covariates and An Application to Wildfire Hazard Assessment.pdf},
  keywords = {Covariates,Dependence,Ignition Probability,Point Process},
  school = {University of California}
}

@article{changTestingSeparabilityMarked2011,
  title = {Testing Separability in Marked Multidimensional Point Processes with Covariates},
  author = {Chang, Chien-Hsun and Schoenberg, Frederic Paik},
  year = {2011},
  month = dec,
  volume = {63},
  pages = {1103--1122},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-010-0284-7},
  abstract = {In modeling marked point processes, it is convenient to assume a separable or multiplicative form for the conditional intensity, as this assumption typically allows one to estimate each component of the model individually. Tests have been proposed in the simple marked point process case, to investigate whether the mark distribution is separable from the spatial\textendash temporal characteristics of the point process. Here, we extend these tests to the case of a marked point process with covariates, and where one is interested in testing the separability of each of the covariates, as well as the mark and the coordinates of the point process. The extension is not at all trivial, and covariates must be treated in a fundamentally different way than marks and coordinates of the process, especially when the covariates are not uniformly distributed. An application is given to point process models for forecasting wildfire hazard in Los Angeles County, California, and solutions are proposed to the problem of how to proceed when the separability hypothesis is rejected.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AM93BQL7\\Chang and Schoenberg - 2011 - Testing separability in marked multidimensional po.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {6}
}

@article{chatoPublicHealthGenetic2020,
  title = {Public Health in Genetic Spaces: A Statistical Framework to Optimize Cluster-Based Outbreak Detection},
  shorttitle = {Public Health in Genetic Spaces},
  author = {Chato, Connor and Kalish, Marcia L and Poon, Art F Y},
  year = {2020},
  month = jan,
  volume = {6},
  pages = {veaa011},
  issn = {2057-1577},
  doi = {10.1093/ve/veaa011},
  abstract = {Genetic clustering is a popular method for characterizing variation in transmission rates for rapidly evolving viruses, and could potentially be used to detect outbreaks in `near real time'. However, the statistical properties of clustering are poorly understood in this context, and there are no objective guidelines for setting clustering criteria. Here, we develop a new statistical framework to optimize a genetic clustering method based on the ability to forecast new cases. We analysed the pairwise Tamura-Nei (TN93) genetic distances for anonymized HIV-1 subtype B pol sequences from Seattle (n {$\frac{1}{4}$} 1,653) and Middle Tennessee, USA (n {$\frac{1}{4}$} 2,779), and northern Alberta, Canada (n {$\frac{1}{4}$} 809). Under varying TN93 thresholds, we fit two models to the distributions of new cases relative to clusters of known cases: 1, a null model that assumes cluster growth is strictly proportional to cluster size, i.e. no variation in transmission rates among individuals; and 2, a weighted model that incorporates individual-level covariates, such as recency of diagnosis. The optimal threshold maximizes the difference in information loss between models, where covariates are used most effectively. Optimal TN93 thresholds varied substantially between data sets, e.g. 0.0104 in Alberta and 0.016 in Seattle and Tennessee, such that the optimum for one population would potentially misdirect prevention efforts in another. For a given population, the range of thresholds where the weighted model conferred greater predictive accuracy tended to be narrow (60.005 units), and the optimal threshold tended to be stable over time. Our framework also indicated that variation in the recency of HIV diagnosis among clusters was significantly more predictive of new cases than sample collection dates (DAIC {$>$} 50). These results suggest that one cannot rely on historical precedence or convention to configure genetic clustering methods for public health applications, especially when translating methods between settings of low-level and generalized epidemics. Our framework not only enables investigators to calibrate a clustering method to a specific public health setting, but also provides a variable selection procedure to evaluate different predictive models of cluster growth.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TYVAC2SU\\Chato et al. - 2020 - Public health in genetic spaces a statistical fra.pdf},
  journal = {Virus Evolution},
  language = {en},
  number = {1}
}

@book{chen2012monte,
  title = {Monte {{Carlo}} Methods in {{Bayesian}} Computation},
  author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G},
  year = {2012},
  publisher = {{Springer Science \& Business Media}}
}

@article{chenEMDemystifiedExpectationMaximization,
  title = {{{EM Demystified}}: {{An Expectation}}-{{Maximization Tutorial}}},
  author = {Chen, Yihua and Gupta, Maya R},
  pages = {17},
  abstract = {After a couple of disastrous experiments trying to teach EM, we carefully wrote this tutorial to give you an intuitive and mathematically rigorous understanding of EM and why it works. We explain the standard applications of EM to learning Gaussian mixture models (GMMs) and hidden Markov models (HMMs), and prepare you to apply EM to new problems. This tutorial assumes you have an advanced undergraduate understanding of probability and statistics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7XEMBYVV\\Chen and Gupta - EM Demystiﬁed An Expectation-Maximization Tutoria.pdf;C\:\\Users\\devan\\Zotero\\storage\\V9Y6HSJY\\Chen and Gupta - EM Demystiﬁed An Expectation-Maximization Tutoria.pdf},
  language = {en}
}

@article{chenThinningAlgorithmsSimulating,
  title = {Thinning {{Algorithms}} for {{Simulating Point Processes}}},
  author = {Chen, Yuanda},
  pages = {14},
  abstract = {In this talk we will discuss the algorithms for simulating point processes. The simplest point process is the (homogeneous) Poisson process, which has an intensity function of a constant value {$\lambda$}. It can be simulated by the sum of its interarrival times. By allowing the intensity to vary, taking values given by a deterministic function {$\lambda$}(t), we can extend the Poisson process to the inhomogeneous case. As proposed by (Lewis and Shedler, 1979), inhomogeneous Poisson processes can be simulated by ``thinning'' the points from the homogeneous versions. Further by allowing the intensity at time t to depend on all information prior to t, we get the self-exciting processes such as the Hawkes processes. The simulation of such processes can also be performed by thinning, as given by (Ogata, 1981).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4ILFIK3K\\Chen - Thinning Algorithms for Simulating Point Processes.pdf;C\:\\Users\\devan\\Zotero\\storage\\VQGPNLCH\\Chen - Thinning Algorithms for Simulating Point Processes.pdf},
  language = {en}
}

@article{chetwyndEstimatingReducedSecond1998,
  title = {On {{Estimating}} the {{Reduced Second Moment Measure}} of a {{Stationary Spatial Point Process}}},
  author = {Chetwynd, A.G. and Diggle, P.J.},
  year = {1998},
  month = mar,
  volume = {40},
  pages = {11--15},
  issn = {1369-1473, 1467-842X},
  doi = {10.1111/1467-842X.00002},
  abstract = {A method is proposed for estimating the covariance structure of a nonparametric estimator for the reduced second moment measure, K(s), of a homogeneous planar Poisson process. The method relies on the invariance of the reduced second moment measure to random thinning, and the known covariance structure of the estimator under random sampling from a fixed set of points. The possible extension of the method to stationary Cox processes is discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\P4GPBPZJ\\Chetwynd and Diggle - 1998 - On Estimating the Reduced Second Moment Measure of.pdf},
  journal = {Australian \& New Zealand Journal of Statistics},
  language = {en},
  number = {1}
}

@article{chiuCorrigendumGeneralizedCramer2010,
  title = {Corrigendum to ``{{Generalized Cram\'er}}\textendash von {{Mises}} Goodness-of-Fit Tests for Multivariate Distributions'' [{{Comput}}. {{Statist}}. {{Data Anal}}. 53 (2009) 3817\textendash 3834]},
  author = {Chiu, Sung Nok and Liu, Kwong Ip},
  year = {2010},
  month = feb,
  volume = {54},
  pages = {607},
  issn = {01679473},
  doi = {10.1016/j.csda.2009.09.013},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KD6R9NK4\\Chiu and Liu - 2010 - Corrigendum to “Generalized Cramér–von Mises goodn.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {2}
}

@article{chiuGeneralizedCramerMises2009,
  title = {Generalized {{Cram\'er}}\textendash von {{Mises}} Goodness-of-Fit Tests for Multivariate Distributions},
  author = {Chiu, Sung Nok and Liu, Kwong Ip},
  year = {2009},
  month = sep,
  volume = {53},
  pages = {3817--3834},
  issn = {01679473},
  doi = {10.1016/j.csda.2009.04.004},
  abstract = {A class of statistics for testing the goodness-of-fit for any multivariate continuous distribution is proposed. These statistics consider not only the goodness-of-fit of the joint distribution but also the goodness-of-fit of all marginal distributions, and can be regarded as generalizations of the multivariate Cram\'er\textendash von Mises statistic. Simulation shows that these generalizations, using the Monte Carlo test procedure to approximate their finitesample p-values, are more powerful than the multivariate Kolmogorov\textendash Smirnov statistic. \textcopyright{} 2009 Elsevier B.V. All rights reserved.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\M3TX8KNZ\\Chiu and Liu - 2009 - Generalized Cramér–von Mises goodness-of-fit tests.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {11}
}

@phdthesis{chongApplicationsCreditScoring,
  title = {Applications of {{Credit Scoring Models}}},
  author = {Chong, Mimi Mei Ling},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ESKJULVG\\Chong - Applications of Credit Scoring Models.pdf},
  language = {en}
}

@article{christensenBayesianPredictionSpatial2002,
  title = {Bayesian {{Prediction}} of {{Spatial Count Data Using Generalized Linear Mixed Models}}},
  author = {Christensen, Ole F. and Waagepetersen, Rasmus},
  year = {2002},
  volume = {58},
  pages = {280--286},
  abstract = {Spatial weed count data are modeled and predicted using a generalized linear mixed model combined with a Bayesian approach and Markov chain Monte Carlo. Informative priors for a data set with sparse sampling are elicited using a previously collected data set with extensive sampling. Furthermore, we demonstrate that so-called Langevin-Hastings updates are useful for efficient simulation of the posterior distributions, and we discuss computational issues concerning prediction.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4CQKRXQE\\Christensen and Waagepetersen - 2002 - Bayesian Prediction of Spatial Count Data Using Ge.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{chuModifiedKellyCriteria2018,
  title = {Modified {{Kelly}} Criteria},
  author = {Chu, Dani and Wu, Yifan and Swartz, Tim B.},
  year = {2018},
  month = mar,
  volume = {14},
  pages = {1--11},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2017-0122},
  abstract = {This paper considers an extension of the Kelly criterion used in sports wagering. By recognizing that the probability p of placing a correct wager is unknown, modified Kelly criteria are obtained that take the uncertainty into account. Estimators are proposed that are developed from a decision theoretic framework. We observe that the resultant betting fractions can differ markedly based on the choice of loss function. In the cases that we study, the modified Kelly fractions are smaller than original Kelly.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GTVVP3A9\\Chu et al. - 2018 - Modified Kelly criteria.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {1}
}

@article{cinlarSuperpositionPointProcesses1968,
  title = {On the {{Superposition}} of {{Point Processes}}},
  author = {Cinlar, E. and Agnew, R. A.},
  year = {1968},
  volume = {30},
  pages = {576--581},
  issn = {0035-9246},
  abstract = {Given point processes N\textsubscript{1}, ..., N\textsubscript{m} their superposition is the point process N defined by N(t) = N\textsubscript{1}(t) + ... + N\textsubscript{m}(t), t {$\geqslant$} 0. An equivalent description of the system (N\textsubscript{1}, ...,N\textsubscript{m}) is by the process (X\textsubscript{n}, T\textsubscript{n}) where the T\textsubscript{n} are the points of N, and X\textsubscript{n} = k if and only if T\textsubscript{n} is a point of N\textsubscript{k}. The use of (X, T) process enables one to study the dependence of N\textsubscript{1}, ..., N\textsubscript{m}. Necessary and sufficient conditions are obtained for N\textsubscript{1}, ..., N\textsubscript{m} to be independent, and for the super-position N to be renewal process. For example, if N\textsubscript{1} and N\textsubscript{2} are renewal processes and X is independent of N, then N is a renewal process only if X is a homogeneous Markov chain.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VYP6M3DJ\\Cinlar and Agnew - 1968 - On the Superposition of Point Processes.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {3}
}

@article{claeskensOnesidedTestsShared2008,
  title = {One-Sided Tests in Shared Frailty Models},
  author = {Claeskens, Gerda and Nguti, Rosemary and Janssen, Paul},
  year = {2008},
  month = may,
  volume = {17},
  pages = {69--82},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/s11749-006-0023-9},
  abstract = {Tests for the presence of heterogeneity in frailty models use an alternative hypothesis in which the heterogeneity parameter is subject to an inequality constraint. As a result, the classical likelihood ratio asymptotic chi-square distribution theory is no longer valid. Our main result states the limiting distribution of the likelihood ratio and score statistic for the one-sided testing problem. The resulting distribution is a mixture of chi-square distributed random variables. The results are shown for gamma and positive stable frailty distributions, and hold when covariate information is present. A data example illustrates the tests. We also assess, in a simulation study, the performance of the tests regarding the significance level and power.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\332PW66Z\\Claeskens et al. - 2008 - One-sided tests in shared frailty models.pdf;C\:\\Users\\devan\\Zotero\\storage\\FKWDR4EK\\Claeskens et al. - 2008 - One-sided tests in shared frailty models.pdf;C\:\\Users\\devan\\Zotero\\storage\\K6D8J9RB\\Claeskens et al. - 2008 - One-sided tests in shared frailty models.pdf},
  journal = {TEST},
  language = {en},
  number = {1}
}

@book{clevelandLocalRegressionModels2017,
  title = {Local {{Regression Models}}},
  author = {Cleveland, William S. and Grosse, Eric and Shyu, William M.},
  editor = {T.J. Hastie},
  year = {2017},
  month = nov,
  publisher = {{Taylor \& Francis Group}},
  doi = {10.1201/9780203738535-8},
  abstract = {Local regression models provide methods for fitting regression functions, or regression surfaces, to data. The chapter describes the specifications of a},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PBAQ3ACY\\9780203738535-8.html},
  language = {en}
}

@article{clevelandRegressionLocalFitting1988,
  title = {Regression by Local Fitting: {{Methods}}, Properties, and Computational Algorithms},
  shorttitle = {Regression by Local Fitting},
  author = {Cleveland, William S. and Devlin, Susan J. and Grosse, Eric},
  year = {1988},
  month = jan,
  volume = {37},
  pages = {87--114},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(88)90077-2},
  abstract = {Local regression is a procedure for estimating regression surfaces by the local fitting of linear or quadratic functions of the independent variables in a moving fashion that is analogous to how a moving average is computed for a time series. The advantage of the methodology over the global fitting of parametric functions of the independent variables by least squares, the current paradigm in regression studies, is that a much wider class of regression functions can be estimated without distortion. In this paper, we discuss the methods, their statistical properties, and computational algorithms.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HAHIS8TI\\Cleveland et al. - 1988 - Regression by local fitting Methods, properties, .pdf;C\:\\Users\\devan\\Zotero\\storage\\4JYWJG3S\\0304407688900772.html},
  journal = {Journal of Econometrics},
  language = {en},
  number = {1}
}

@article{cloughItShouldnHappen2004,
  title = {It Shouldn't Happen to a Statistician},
  author = {Clough, Helen and Brown, Patrick},
  year = {2004},
  month = sep,
  volume = {1},
  pages = {118--120},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2004.00044.x},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NRT6ECK6\\Clough and Brown - 2004 - It shouldn't happen to a statistician.pdf;C\:\\Users\\devan\\Zotero\\storage\\UFWS7AJ5\\Clough and Brown - 2004 - It shouldn't happen to a statistician.pdf},
  journal = {Significance},
  language = {en},
  number = {3}
}

@article{cochranePositiveFeedbacksFire1999,
  title = {Positive {{Feedbacks}} in the {{Fire Dynamic}} of {{Closed Canopy Tropical Forests}}},
  author = {Cochrane, M. A.},
  year = {1999},
  month = jun,
  volume = {284},
  pages = {1832--1835},
  issn = {00368075, 10959203},
  doi = {10.1126/science.284.5421.1832},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8JX6K67X\\Cochrane - 1999 - Positive Feedbacks in the Fire Dynamic of Closed C.pdf;C\:\\Users\\devan\\Zotero\\storage\\97KYBBJG\\Cochrane - 1999 - Positive Feedbacks in the Fire Dynamic of Closed C.pdf},
  journal = {Science},
  keywords = {Fire Good},
  language = {en},
  number = {5421}
}

@article{comasCharacterizingConfigurationsFire2014,
  title = {Characterizing Configurations of Fire Ignition Points through Spatiotemporal Point Processes},
  author = {Comas, C. and {Costafreda-Aumedes}, S. and {Vega-Garcia}, C.},
  year = {2014},
  month = apr,
  volume = {2},
  pages = {2891--2911},
  issn = {2195-9269},
  doi = {10.5194/nhessd-2-2891-2014},
  abstract = {Human-caused forest fires are usually regarded as unpredictable but often exhibit trends towards clustering in certain locations and periods. Characterizing such configurations is crucial for understanding spatiotemporal fire dynamics and implementing 5 preventive actions. Our objectives were to analyse the spatiotemporal point configuration and to test for spatiotemporal interaction. We characterized the spatiotemporal structure of 984 fire ignition points in a study area of Galicia, Spain, during 2007\textendash 2011 by the K-Ripley's function. Our results suggest the presence of spatiotemporal structures for time lags of less than two years and ignition point distances in the range 10 0\textendash 12 km. Ignition centre points at time lags of less than 100 days are aggregated for any inter-event distance. This cluster structure loses strength as the time lag increases, and at time lags of more than 365 days this cluster structure is not significant for any lag distance. Our results also suggest spatiotemporal interdependencies at time lags of less than 100 days and inter-event distances of less than 10 km. At time lags of up 15 to 365 days spatiotemporal components are independent for any point distance. These results suggest that risk conditions occur locally and are short-lived in this study area.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XQF44S7Y\\Comas et al. - 2014 - Characterizing configurations of fire ignition poi.pdf},
  journal = {Natural Hazards and Earth System Sciences Discussions},
  language = {en},
  number = {4}
}

@article{commeauFittingLognormalDistribution2012,
  title = {Fitting a Lognormal Distribution to Enumeration and Absence/Presence Data},
  author = {Commeau, Natalie and Parent, Eric and {Delignette-Muller}, Marie-Laure and Cornu, Marie},
  year = {2012},
  month = apr,
  volume = {155},
  pages = {146--152},
  issn = {0168-1605},
  doi = {10.1016/j.ijfoodmicro.2012.01.023},
  abstract = {To fit a lognormal distribution to a complex set of microbial data, including detection data (e.g. presence or absence in 25g) and enumeration data (e.g. 30cfu/g), we compared two models: a model called MCLD based on data expressed as concentrations (in cfu/g) or censored concentrations (e.g. {$<$}10cfu/g, or {$>$}1cfu/25g) versus a model called MRD that directly uses raw data (presence/absence in test portions, and plate colony counts). We used these two models to simulated data sets, under standard conditions (limit of detection (LOD)=1cfu/25g; limit of quantification (LOQ)=10cfu/g) and used a maximum likelihood estimation method (directly for the model MCLD and via the Expectation\textendash Maximisation (EM) algorithm for the model MRD. The comparison suggests that in most cases estimates provided by the proposed model MRD are similar to those obtained by model MCLD accounting for censorship. Nevertheless, in some cases, the proposed model MRD leads to less biased and more precise estimates than model MCLD.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\K3JCFUDB\\Commeau et al. - 2012 - Fitting a lognormal distribution to enumeration an.pdf;C\:\\Users\\devan\\Zotero\\storage\\ELK7PFPJ\\S0168160512000566.html},
  journal = {International Journal of Food Microbiology},
  keywords = {EM algorithm,Limit of detection,Limit of quantification,Maximum likelihood estimation,Microbial contamination assessment},
  number = {3}
}

@misc{CompoundDistributions,
  title = {Compound {{Distributions}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DBCZLM4A\\9780470012505.tac046.pdf}
}

@article{congdonModelFrameworkMortality2006,
  title = {A {{Model Framework}} for {{Mortality}} and {{Health Data Classified}} by {{Age}}, {{Area}}, and {{Time}}},
  author = {Congdon, Peter},
  year = {2006},
  volume = {62},
  pages = {269--278},
  abstract = {This article sets out a modeling framework for modeling health outcomes over area, age, and time dimensions that takes account of spatial correlation, interactions between dimensions, and cohort as well as age effects. The goals of the framework include parsimony and parameter interpretability. Multivariate extensions may be made allowing interdependent or shared effects between different outcomes (e.g., ill health and mortality). A particular focus is on assessing the proportionality assumption whereby separate age and area effects multiply to produce age-area mortality or illness rates, and age-area interactions are assumed not to exist. A trivariate (mortality-health) application of the framework involves cross-sectional data in the 33 London boroughs, while a longitudinal univariate application involves deaths for the same areas over four 5-year periods starting in 1979.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F53STRV4\\Congdon - 2006 - A Model Framework for Mortality and Health Data Cl.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{congdonModelNonparametricSpatially2006,
  title = {A Model for Non-Parametric Spatially Varying Regression Effects},
  author = {Congdon, Peter},
  year = {2006},
  month = jan,
  volume = {50},
  pages = {422--445},
  issn = {01679473},
  doi = {10.1016/j.csda.2004.08.008},
  abstract = {A method for non-parametric regression effects is applied to spatially configured data, using a generalised additive form that allows regression effects to vary over areas. The focus is on discrete outcomes in disease mapping but can be adapted to metric outcomes. Specifically a mixed model is proposed that combines a local general additive model (GAM) element for each area with a spatially filtered GAM effect. Modifications are discussed that allow for the impact of outliers on the spatial regression. The paper uses a Bayesian approach that places random walk priors on the various smooth functions, Gamma priors on inverse scale parameters and Dirichlet priors on mixing parameters. The model is illustrated with applications to lip cancer mortality in Scottish counties, where there is one predictor with regression impact modelled non-parametrically, and suicide deaths in 32 London boroughs, where two predictors are taken to follow a spatial GAM form.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XGVRVLQX\\Congdon - 2006 - A model for non-parametric spatially varying regre.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {2}
}

@article{copasShrinkagePointScoring1993,
  title = {The {{Shrinkage}} of {{Point Scoring Methods}}},
  author = {Copas, J. B.},
  year = {1993},
  volume = {42},
  pages = {315},
  issn = {00359254},
  doi = {10.2307/2986235},
  abstract = {Point scoring, widely used in criminology and other social sciences, is a simple way of predicting a binary response on the basis of binary explanatory variables. Like all statistical predictors they are liable to shrinkage, working less well on a validation sample than they appear to do on the original data. The paper examines the extent of shrinkage and proposes shrinkage-adjusted predictions. The related 'independence Bayes' method is also considered, and found to shrink more than the basic point scoring method. The results are applied to data from a cohort study in the development of delinquency.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MJH5YIN4\\Copas - 1993 - The Shrinkage of Point Scoring Methods.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {2}
}

@article{coroGibbsSamplingJAGS2017,
  title = {Gibbs {{Sampling}} with {{JAGS}}: {{Behind}} the {{Scenes}}},
  shorttitle = {Gibbs {{Sampling}} with {{JAGS}}},
  author = {Coro, Gianpaolo},
  year = {2017},
  month = feb,
  abstract = {Gibbs sampling is a Bayesian inference technique that is used in various scientific domains to generate samples from a certain posterior probability density function, given experimental data. Several software implementations of Gibbs sampling exist, which generally adopt very different approaches, because it is not easy to make a Gibbs sampling implementation exactly correspond to the theoretical approach. In particular, these implementations may use different approximation algorithms to find solutions to sub-steps of the Gibbs sampling process. Scientists working in different domains often use Gibbs sampling software without knowing the details of the implementation. Nevertheless, it is our experience that understanding the implementation can be crucial to enhance the performance of a model, because a software configuration conceived to help the underlying implementation may end in better approximation of the estimated probabilities functions. JAGS (Just Another Gibbs Sampler) is a widely used open-source implementation of Gibbs sampling. Its installation and user's guide are accurate, but do not indicate how the software really implements Gibbs sampling and it is not easy to infer this information from the source code. The aim of this paper is to give a high-level overview of the JAGS algorithms and its extensions that implement Gibbs sampling. Our target reader is a scientist who may want to understand the basic concepts underlying Bayesian inference and Gibbs sampling and who want to be aware of what happens behind the scenes when building a model.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PS7B7JZS\\Coro - 2017 - Gibbs Sampling with JAGS Behind the Scenes.pdf}
}

@article{costafreda-aumedesSpatioTemporalConfigurationsHumanCaused2016,
  title = {Spatio-{{Temporal Configurations}} of {{Human}}-{{Caused Fires}} in {{Spain}} through {{Point Patterns}}},
  author = {{Costafreda-Aumedes}, Sergi and Comas, Carles and {Vega-Garcia}, Cristina},
  year = {2016},
  month = aug,
  volume = {7},
  pages = {185},
  issn = {1999-4907},
  doi = {10.3390/f7090185},
  abstract = {Human-caused wildfires are often regarded as unpredictable, but usually occur in patterns aggregated over space and time. We analysed the spatio-temporal configuration of 7790 anthropogenic wildfires (2007\textendash 2013) in nine study areas distributed throughout Peninsular Spain by using the Ripley's K-function. We also related these aggregation patterns to weather, population density, and landscape structure descriptors of each study area. Our results provide statistical evidence for spatio-temporal structures around a maximum of 4 km and six months. These aggregations lose strength when the spatial and temporal distances increase. At short time lags after a wildfire ({$<$}1 month), the probability of another fire occurrence is high at any distance in the range of 0\textendash 16 km. When considering larger time lags (up to two years), the probability of fire occurrence is high only at short distances ({$>$}3 km). These aggregated patterns vary depending on location in Spain. Wildfires seem to aggregate within fewer days (heat waves) in warm and dry Mediterranean regions than in milder Atlantic areas (bimodal fire season). Wildfires aggregate spatially over shorter distances in diverse, fragmented landscapes with many small and complex patches. Urban interfaces seem to spatially concentrate fire occurrence, while wildland-agriculture interfaces correlate with larger aggregates.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PSRGU7S2\\Costafreda-Aumedes et al. - 2016 - Spatio-Temporal Configurations of Human-Caused Fir.pdf},
  journal = {Forests},
  language = {en},
  number = {12}
}

@article{cowpertwaitFurtherDevelopmentsNeymanscott1991,
  title = {Further Developments of the Neyman-Scott Clustered Point Process for Modeling Rainfall},
  author = {Cowpertwait, Paul S. P.},
  year = {1991},
  month = jul,
  volume = {27},
  pages = {1431--1438},
  issn = {00431397},
  doi = {10.1029/91WR00479},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DA756EHJ\\Cowpertwait - 1991 - Further developments of the neyman-scott clustered.pdf;C\:\\Users\\devan\\Zotero\\storage\\DH49XIDV\\Cowpertwait - 1991 - Further developments of the neyman-scott clustered.pdf},
  journal = {Water Resources Research},
  language = {en},
  number = {7}
}

@book{coxPointProcesses1980,
  title = {Point {{Processes}}},
  author = {Cox, D.R. and Isham, Valerie},
  year = {1980},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {This book describes the properties of stochastic probabilistic models and develops the applied mathematics of stochastic point processes. It is useful to students and research workers in probability and statistics and also to research workers wishing to apply stochastic point processes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VA6NTXDG\\9780412219108.html},
  language = {en},
  series = {Chapman \& {{Hall}}/{{CRC Monographs}} on {{Statistics}} and {{Applied Probability}}}
}

@article{crainiceanuLikelihoodRatioTests2004,
  title = {Likelihood Ratio Tests in Linear Mixed Models with One Variance Component},
  author = {Crainiceanu, Ciprian M. and Ruppert, David},
  year = {2004},
  volume = {66},
  pages = {165--185},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2004.00438.x},
  abstract = {Summary. We consider the problem of testing null hypotheses that include restrictions on the variance component in a linear mixed model with one variance component and we derive the finite sample and asymptotic distribution of the likelihood ratio test and the restricted likelihood ratio test. The spectral representations of the likelihood ratio test and the restricted likelihood ratio test statistics are used as the basis of efficient simulation algorithms of their null distributions. The large sample {$\chi$}2 mixture approximations using the usual asymptotic theory for a null hypothesis on the boundary of the parameter space have been shown to be poor in simulation studies. Our asymptotic calculations explain these empirical results. The theory of Self and Liang applies only to linear mixed models for which the data vector can be partitioned into a large number of independent and identically distributed subvectors. One-way analysis of variance and penalized splines models illustrate the results.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MRZ6CIS5\\Crainiceanu and Ruppert - 2004 - Likelihood ratio tests in linear mixed models with.pdf;C\:\\Users\\devan\\Zotero\\storage\\5AY5XKQX\\j.1467-9868.2004.00438.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Degrees of freedom,Non-regular problems,Penalized splines},
  language = {en},
  number = {1}
}

@article{crawfordSexLiesSelfreported2015,
  title = {Sex, Lies and Self-Reported Counts: {{Bayesian}} Mixture Models for Heaping in Longitudinal Count Data via Birth\textendash Death Processes},
  shorttitle = {Sex, Lies and Self-Reported Counts},
  author = {Crawford, Forrest W. and Weiss, Robert E. and Suchard, Marc A.},
  year = {2015},
  month = jun,
  volume = {9},
  pages = {572--596},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS809},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7QT4HZ4N\\Crawford et al. - 2015 - Sex, lies and self-reported counts Bayesian mixtu.pdf;C\:\\Users\\devan\\Zotero\\storage\\JUQXDZJI\\Crawford et al. - 2015 - Sex, lies and self-reported counts Bayesian mixtu.pdf},
  journal = {The Annals of Applied Statistics},
  language = {en},
  number = {2}
}

@article{crowleyCovarianceAnalysisHeart1977,
  title = {Covariance {{Analysis}} of {{Heart Transplant Survival Data}}},
  author = {Crowley, John and Hu, Marie},
  year = {1977},
  volume = {72},
  pages = {27--36},
  issn = {0162-1459},
  doi = {10.2307/2286902},
  abstract = {This paper presents a number of analyses to assess the effects of various covariates on the survival of patients in the Stanford Heart Transplantation Program. The data have been updated from previously published versions and include some additional covariates, such as measures of tissue typing. The methods used allow for simultaneous investigation of several covariates and provide estimates of the relative risk of transplantation as well as significance tests.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2A5QXP6T\\Crowley and Hu - 1977 - Covariance Analysis of Heart Transplant Survival D.pdf},
  journal = {Journal of the American Statistical Association},
  number = {357}
}

@article{cummingParametricModelFiresize2001,
  title = {A Parametric Model of the Fire-Size Distribution},
  author = {Cumming, S G},
  year = {2001},
  month = aug,
  volume = {31},
  pages = {1297--1303},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/x01-032},
  abstract = {This paper developes statistical models of the size distribution of lightning-caused wildfires in the boreal mixedwood forests of Alberta, Canada, for the intervals 1980\textendash 1998 and 1961\textendash 1998. Above any minimum threshold size {$\geq$}3 ha, the logarithm of fire size is approximately exponentially distributed. However, computer simulations using the best-fit distribution would over predict the frequency of large fires, and thus the mean rate of disturbance. A truncated exponential distribution, which places an upper bound on fire size, is more suitable and, according to probability plots, provides an excellent fit to the data. I estimate the maximum fire size in the study area to be {$\approx$} 650 000 ha. This estimate is insensitive to the choice of lower bound for fire sizes (between 3 and 1000 ha) and to the choice of sampling interval. Parametric modelling of fire sizes using covariates derived from forest inventory data shows that the expected size of a fire is positively related to the abundance of pine forest in the vicinity of the point of detection and negatively related to the abundance of recently logged or burnt areas. This implies that variation in forest structure and disturbance history impose marked spatial variability on the fire size distribution. Other covariates, such as periodic indices of fire weather, could readily be evaluated in this framework.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DUVLT8R3\\Cumming - 2001 - A parametric model of the fire-size distribution.pdf;C\:\\Users\\devan\\Zotero\\storage\\HLSFXQTY\\Cumming - 2001 - A parametric model of the fire-size distribution.pdf},
  journal = {Canadian Journal of Forest Research},
  keywords = {No Covariates,Size},
  language = {en},
  number = {8}
}

@article{cummingsModelingHeapedCount2015,
  title = {Modeling {{Heaped Count Data}}},
  author = {Cummings, Tammy H. and Hardin, James W. and McLain, Alexander C. and Hussey, James R. and Bennett, Kevin J. and Wingood, Gina M.},
  year = {2015},
  month = jun,
  volume = {15},
  pages = {457--479},
  issn = {1536-867X, 1536-8734},
  doi = {10.1177/1536867X1501500207},
  abstract = {We present motivation and new commands for modeling heaped count data. These data may appear when subjects report counts that are rounded or favor multiples (digit preference) of a certain outcome, such as the number of cigarettes reported. The new commands for fitting count regression models (Poisson, generalized Poisson, negative binomial) are also accompanied by real-world examples comparing the heaped regression model with the usual regression model as well as the heaped zero-inflated model with the usual zero-inflated model.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WISTYWD2\\Cummings et al. - 2015 - Modeling Heaped Count Data.pdf},
  journal = {The Stata Journal: Promoting communications on statistics and Stata},
  language = {en},
  number = {2}
}

@article{cunninghamStochasticModelOccurence,
  title = {A {{Stochastic Model}} for the {{Occurence}} of {{Man}}-Caused {{Forest Fires}}.Pdf},
  author = {Cunningham, A.A. and Martell, David L.},
  year = {1973},
  volume = {3},
  pages = {282--287},
  abstract = {This paper discusses the occurence of man-caused forest fires during the summer fire season in a section of northwestern Ontario. Fire occurence is viewed as being a chance process and a stochastic model is developed to describe it. The results of this study indicate that a Poisson model with the average number of fires per day depending on the Fine Fuel Moisture Code is appropriate.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4VT28QIF\\Cunningham, Martell - 1973 - A Stochastic Model for the Occurence of Man-caused Forest Fires.pdf},
  journal = {Canadian Journal of Forest Research},
  keywords = {Count,Covariates}
}

@article{cunninghamStochasticModelOccurrence1973,
  title = {A {{Stochastic Model}} for the {{Occurrence}} of {{Man}}-Caused {{Forest Fires}}},
  author = {Cunningham, A. A. and Martell, D. L.},
  year = {1973},
  month = jun,
  volume = {3},
  pages = {282--287},
  issn = {0045-5067},
  doi = {10.1139/x73-038},
  abstract = {This paper discusses the occurrence of man-caused forest fires during the summer fire season in a section of northwestern Ontario. Fire occurrence is viewed as being a chance process and a stochast..., Cet article discute de l'av\`enement d'incendies forestiers caus\'es par l'homme au cours de la saison d'\'et\'e dans une section du nord-ouest de l'Ontario. L'av\`enement d'incendies est consid\'er\'e comme pro...},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BCG4HERZ\\Cunningham and Martell - 1973 - A Stochastic Model for the Occurrence of Man-cause.pdf;C\:\\Users\\devan\\Zotero\\storage\\HUYYFLCP\\x73-038.html},
  journal = {Canadian Journal of Forest Research},
  number = {2}
}

@article{cuzickSpatialClusteringInhomogeneous1990,
  title = {Spatial {{Clustering}} for {{Inhomogeneous Populations}}},
  author = {Cuzick, Jack and Edwards, Robert},
  year = {1990},
  volume = {52},
  pages = {73--104},
  issn = {0035-9246},
  abstract = {A new method for detecting spatial clustering of events in populations with non-uniform density is proposed. The method is based on selecting controls from the population at risk and computing interpoint distances for the combined sample. Nonparametric tests are developed which are based on the number of cases among the k nearest neighbours of each case and the number of cases nearer than the k nearest control. The performance of these tests is evaluated analytically and by simulation and the method is applied to a data set on the locations of cases of childhood leukaemia and lymphoma in a defined geographical area. In particular the impact on power of the choice of k and of the ratio of cases to controls is examined. Modifications of the procedure to study distances from predefined objects, to match for known risk factors which would produce unwanted clustering and issues related to estimation are also discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZCJ98BYG\\Cuzick and Edwards - 1990 - Spatial Clustering for Inhomogeneous Populations.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {1}
}

@article{dabyeCramerMisesTest2013,
  title = {On the {{Cram\'er}}\textendash von {{Mises}} Test with Parametric Hypothesis for Poisson Processes},
  author = {Dabye, A. S.},
  year = {2013},
  month = apr,
  volume = {16},
  pages = {1--13},
  issn = {1387-0874, 1572-9311},
  doi = {10.1007/s11203-013-9077-y},
  abstract = {The problem of the goodness of-fit testing for inhomogeneous Poisson process with parametric basic hypothesis is considered. A test statistic of the Cram\'er\textendash von Mises type with parameter replaced by the maximum likelihood estimator is proposed and its asymptotic behavior is studied. It is shown that in the case of shift parameter, the limit distribution of the test statistics (under hypothesis) does not depend on the true value of this parameter.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H6NDITYY\\Dabye - 2013 - On the Cramér–von Mises test with parametric hypot.pdf},
  journal = {Statistical Inference for Stochastic Processes},
  language = {en},
  number = {1}
}

@article{dachianGoodnessofFitTestsContinuous2009,
  title = {On the {{Goodness}}-of-{{Fit Tests}} for {{Some Continuous Time Processes}}},
  author = {Dachian, Serguei and Kutoyants, Yury A.},
  year = {2009},
  month = mar,
  abstract = {We present a review of several results concerning the construction of the Cram\textasciiacute er-von Mises and Kolmogorov-Smirnov type goodnessof-fit tests for continuous time processes. As the models we take a stochastic differential equation with small noise, ergodic diffusion process, Poisson process and self-exciting point processes. For every model we propose the tests which provide the asymptotic size {$\alpha$} and discuss the behaviour of the power function under local alternatives. The results of numerical simulations of the tests are presented.},
  archivePrefix = {arXiv},
  eprint = {0903.4642},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BDR8YYWJ\\Dachian and Kutoyants - 2009 - On the Goodness-of-Fit Tests for Some Continuous T.pdf},
  journal = {arXiv:0903.4642 [math, stat]},
  keywords = {Mathematics - Statistics Theory},
  language = {en},
  primaryClass = {math, stat}
}

@book{daleyIntroductionTheoryPoint2003,
  title = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}: {{Volume I}}: {{Elementary Theory}} and {{Methods}}},
  shorttitle = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}},
  author = {Daley, D. J. and {Vere-Jones}, D.},
  year = {2003},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b97277},
  abstract = {Point processes and random measures find wide applicability in telecommunications, earthquakes, image analysis, spatial point patterns, and stereology, to name but a few areas. The authors have made a major reshaping of their work in their first edition of 1988 and now present their Introduction to the Theory of Point Processes in two volumes with sub-titles "Elementary Theory and Models" and "General Theory and Structure". Volume One contains the introductory chapters from the first edition, together with an informal treatment of some of the later material intended to make it more accessible to readers primarily interested in models and applications. The main new material in this volume relates to marked point processes and to processes evolving in time, where the conditional intensity methodology provides a basis for model building, inference, and prediction. There are abundant examples whose purpose is both didactic and to illustrate further applications of the ideas and models that are the main substance of the text. Volume Two returns to the general theory, with additional material on marked and spatial processes. The necessary mathematical background is reviewed in appendices located in Volume One. Daryl Daley is a Senior Fellow in the Centre for Mathematics and Applications at the Australian National University, with research publications in a diverse range of applied probability models and their analysis; he is co-author with Joe Gani of an introductory text in epidemic modelling. David Vere-Jones is an Emeritus Professor at Victoria University of Wellington, widely known for his contributions to Markov chains, point processes, applications in seismology.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IZX8PISL\\9780387955414.html},
  isbn = {978-0-387-95541-4},
  language = {en},
  series = {Probability and {{Its Applications}}, {{An Introduction}} to the {{Theory}} of {{Point Processes}}}
}

@book{daleyIntroductionTheoryPoint2008,
  title = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}: {{Volume II}}: {{General Theory}} and {{Structure}}},
  shorttitle = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}},
  author = {Daley, D. J. and {Vere-Jones}, David},
  year = {2008},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-49835-5},
  abstract = {Point processes and random measures find wide applicability in telecommunications, earthquakes, image analysis, spatial point patterns and stereology, to name but a few areas. The authors have made a major reshaping of their work in their first edition of 1988 and now present An Introduction to the Theory of Point Processes in two volumes with subtitles Volume I: Elementary Theory and Methods and Volume II: General Theory and Structure. Volume I contains the introductory chapters from the first edition together with an account of basic models, second order theory, and an informal account of prediction, with the aim of making the material accessible to readers primarily interested in models and applications. It also has three appendices that review the mathematical background needed mainly in Volume II. Volume II sets out the basic theory of random measures and point processes in a unified setting and continues with the more theoretical topics of the first edition: limit theorems, ergodic theory, Palm theory, and evolutionary behaviour via martingales and conditional intensity. The very substantial new material in this second volume includes expanded discussions of marked point processes, convergence to equilibrium, and the structure of spatial point processes. D.J. Daley is recently retired from the Centre for Mathematics and Applications at the Australian National University, with research publications in a diverse range of applied probability models and their analysis; he is coauthor with Joe Gani of an introductory text on epidemic modelling. The Statistical Society of Australia awarded him their Pitman Medal for 2006. D. Vere-Jones is an Emeritus Professor at Victoria University of Wellington, widely known for his contributions to Markov chains, point processes, applications in seismology, and statistical education. He is a fellow and Gold Medallist of the Royal Society of New Zealand, and a director of the consulting group Statistical Research Associates.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZXDPGDWI\\9780387213378.html},
  isbn = {978-0-387-21337-8},
  language = {en},
  series = {Probability and {{Its Applications}}, {{An Introduction}} to the {{Theory}} of {{Point Processes}}}
}

@article{darlingKolmogorovSmirnovCramervonMises1957,
  title = {The {{Kolmogorov}}-{{Smirnov}}, {{Cramer}}-von {{Mises Tests}}},
  author = {Darling, D. A.},
  year = {1957},
  volume = {28},
  pages = {823--838},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WKSZ5NLB\\Darling - 1957 - The Kolmogorov-Smirnov, Cramer-von Mises Tests.pdf},
  journal = {The Annals of Mathematical Statistics},
  language = {en},
  number = {4}
}

@article{deanMixedPoissonInverseGaussianRegression1989,
  title = {A {{Mixed Poisson}}-{{Inverse}}-{{Gaussian Regression Model}}},
  author = {Dean, C. and Lawless, J. F. and Willmot, G. E.},
  year = {1989},
  volume = {17},
  pages = {171--181},
  issn = {0319-5724},
  doi = {10.2307/3314846},
  abstract = {The mixed Poisson-inverse-Gaussian distribution has been used by Holla, Sankaran, Sichel, and others in univariate problems involving counts. We propose a Poisson-inverse-Gaussian regression model which can be used for regression analysis of counts. The model provides an attractive framework for incorporating random effects in Poisson regression models and in handling extra-Poisson variation. Maximum-likelihood and quasilikelihood-moment estimation is investigated and illustrated with an example involving motor-insurance claims. /// Un m\'elange pond\'er\'e de lois de Poisson, avec des poids suivant une loi gaussienne inverse, a \'et\'e utilis\'e par Holla, Sankaran, Sichel et d'autres comme mod\`ele unidimensionnel dans des probl\`emes de d\'enombrement. Nous proposons un mod\`ele de r\'egression bas\'e sur un tel m\'elange. Celui-ci peut \^etre utilis\'e dans des analyses de r\'egression faites \`a partir de d\'enombrements. Il permet d'incorporer des effets al\'eatoires dans un mod\`ele de r\'egression bas\'e sur la loi de Poisson, ainsi que le traitement de la variation non repr\'esent\'ee par la loi de Poisson. L'estimation par la m\'ethode du maximum de vraisemblance et par la quasi-vraisemblance/moments est \'etudi\'ee et illustr\'ee \`a l'aide de donn\'ees au sujet de r\'eclamations relatives \`a l'assurance automobile.},
  journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
  number = {2}
}

@article{deanPenalizedQuasilikelihoodSpatially2004,
  title = {Penalized Quasi-Likelihood with Spatially Correlated Data},
  author = {Dean, C.B. and Ugarte, M.D. and Militino, A.F.},
  year = {2004},
  month = mar,
  volume = {45},
  pages = {235--248},
  issn = {01679473},
  doi = {10.1016/S0167-9473(02)00324-9},
  abstract = {This article discusses and evaluates penalized quasi-likelihood (PQL) estimation techniques for the situation where random e ects are correlated, as is typical in mapping studies. This is an approximate \"ytting technique which uses a Laplace approximation to the integrated mixed model likelihood. It is much easier to implement than usual maximum likelihood estimation. Our results show that the PQL estimates are reasonably unbiased for analysis of mixed Poisson models when there is correlation in the random e ects, except when the means are su ciently small to yield sparse data. However, although the normal approximation to the distribution of the parameter estimates works fairly well for the parameters in the mean it does not perform as well for the variance components. In addition, when the mean mortality counts are small, the estimated standard errors of the variance components tend to become more biased than those for the mean. We illustrate our approaches by applying PQL for mapping mortality in British Columbia, Canada, over the \"yve-year period 1985 \textendash 1989.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Y55GRRQ6\\Dean et al. - 2004 - Penalized quasi-likelihood with spatially correlat.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {2}
}

@article{delignette-mullerFitdistrplusPackageFitting2015,
  title = {Fitdistrplus: {{An R Package}} for {{Fitting Distributions}}},
  shorttitle = {Fitdistrplus},
  author = {{Delignette-Muller}, Marie Laure and Dutang, Christophe},
  year = {2015},
  month = mar,
  volume = {64},
  pages = {1--34},
  issn = {1548-7660},
  doi = {10.18637/jss.v064.i04},
  copyright = {Copyright (c) 2013 Marie Laure Delignette-Muller, Christophe Dutang},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C4TGN3QH\\Delignette-Muller and Dutang - 2015 - fitdistrplus An R Package for Fitting Distributio.pdf;C\:\\Users\\devan\\Zotero\\storage\\MT3R2EPH\\v064i04.html},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {1}
}

@article{dempsterMaximumLikelihoodIncomplete1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  volume = {39},
  pages = {1--38},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RGYC5AEZ\\Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf;C\:\\Users\\devan\\Zotero\\storage\\WUWPEUUY\\Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {1}
}

@article{dengGenomicSurveillanceReveals2020,
  title = {Genomic Surveillance Reveals Multiple Introductions of {{SARS}}-{{CoV}}-2 into {{Northern California}}},
  author = {Deng, Xianding and Gu, Wei and Federman, Scot and du Plessis, Louis and Pybus, Oliver G. and Faria, Nuno R. and Wang, Candace and Yu, Guixia and Bushnell, Brian and Pan, Chao-Yang and Guevara, Hugo and {Sotomayor-Gonzalez}, Alicia and Zorn, Kelsey and Gopez, Allan and Servellita, Venice and Hsu, Elaine and Miller, Steve and Bedford, Trevor and Greninger, Alexander L. and Roychoudhury, Pavitra and Starita, Lea M. and Famulare, Michael and Chu, Helen Y. and Shendure, Jay and Jerome, Keith R. and Anderson, Catie and Gangavarapu, Karthik and Zeller, Mark and Spencer, Emily and Andersen, Kristian G. and MacCannell, Duncan and Paden, Clinton R. and Li, Yan and Zhang, Jing and Tong, Suxiang and Armstrong, Gregory and Morrow, Scott and Willis, Matthew and Matyas, Bela T. and Mase, Sundari and Kasirye, Olivia and Park, Maggie and Masinde, Godfred and Chan, Curtis and Yu, Alexander T. and Chai, Shua J. and Villarino, Elsa and Bonin, Brandon and Wadford, Debra A. and Chiu, Charles Y.},
  year = {2020},
  month = jul,
  volume = {369},
  pages = {582--587},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abb9263},
  abstract = {Epidemic in Northern California Genome sequencing of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) outbreaks is valuable for tracing the sources and perhaps for drawing lessons about preventing future outbreaks. Genomic analysis by Deng et al. revealed that Northern California experienced a complex series of introductions of the virus, deriving not only from state-to-state transmission but also from international travel by air and ship. The study highlights the importance of being able to rapidly test and trace contacts of positive cases to enable swift control. Science, this issue p. 582 The coronavirus disease 2019 (COVID-19) pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has spread globally, with {$>$}365,000 cases in California as of 17 July 2020. We investigated the genomic epidemiology of SARS-CoV-2 in Northern California from late January to mid-March 2020, using samples from 36 patients spanning nine counties and the Grand Princess cruise ship. Phylogenetic analyses revealed the cryptic introduction of at least seven different SARS-CoV-2 lineages into California, including epidemic WA1 strains associated with Washington state, with lack of a predominant lineage and limited transmission among communities. Lineages associated with outbreak clusters in two counties were defined by a single base substitution in the viral genome. These findings support contact tracing, social distancing, and travel restrictions to contain the spread of SARS-CoV-2 in California and other states. The unfolding COVID-19 epidemic in Northern California appears to have resulted from multiple introductions of different lineages of the virus. The unfolding COVID-19 epidemic in Northern California appears to have resulted from multiple introductions of different lineages of the virus.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\I6XGA4HM\\Deng et al. - 2020 - Genomic surveillance reveals multiple introduction.pdf;C\:\\Users\\devan\\Zotero\\storage\\XXGK5BLS\\582.html},
  journal = {Science},
  language = {en},
  number = {6503},
  pmid = {32513865}
}

@article{deshpandeEstimatingNBAPlayer2016,
  title = {Estimating an {{NBA}} Player's Impact on His Team's Chances of Winning},
  author = {Deshpande, Sameer K. and Jensen, Shane T.},
  year = {2016},
  month = jan,
  volume = {12},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2015-0027},
  abstract = {Traditional NBA player evaluation metrics are based on scoring differential or some pace-adjusted linear combination of box score statistics like points, rebounds, assists, etc. These measures treat performances with the outcome of the game still in question (e.g. tie score with five minutes left) in exactly the same way as they treat performances with the outcome virtually decided (e.g. when one team leads by 30 points with one minute left). Because they ignore the context in which players perform, these measures can result in misleading estimates of how players help their teams win. We instead use a win probability framework for evaluating the impact NBA players have on their teams' chances of winning. We propose a Bayesian linear regression model to estimate an individual player's impact, after controlling for the other players on the court. We introduce several posterior summaries to derive rank-orderings of players within their team and across the league. This allows us to identify highly paid players with low impact relative to their teammates, as well as players whose high impact is not captured by existing metrics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9JZ9ILLP\\Deshpande and Jensen - 2016 - Estimating an NBA player’s impact on his team’s ch.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {2}
}

@article{deshpandeHierarchicalBayesianModel2017,
  title = {A Hierarchical {{Bayesian}} Model of Pitch Framing},
  author = {Deshpande, Sameer K. and Wyner, Abraham},
  year = {2017},
  month = sep,
  volume = {13},
  pages = {95--112},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2017-0027},
  abstract = {Since the advent of high-resolution pitch tracking data (PITCHf/x), many in the sabermetrics community have attempted to quantify a Major League Baseball catcher's ability to ``frame'' a pitch (i.e. increase the chance that a pitch is a called as a strike). Especially in the last 3 years, there has been an explosion of interest in the ``art of pitch framing'' in the popular press as well as signs that teams are considering framing when making roster decisions. We introduce a Bayesian hierarchical model to estimate each umpire's probability of calling a strike, adjusting for the pitch participants, pitch location, and contextual information like the count. Using our model, we can estimate each catcher's effect on an umpire's chance of calling a strike. We are then able translate these estimated effects into average runs saved across a season. We also introduce a new metric, analogous to Jensen, Shirley, and Wyner's Spatially Aggregate Fielding Evaluation metric, which provides a more honest assessment of the impact of framing.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\82LXQENE\\Deshpande and Wyner - 2017 - A hierarchical Bayesian model of pitch framing.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {3}
}

@misc{desjardinsHockeyProspectusNet2017,
  title = {Hockey {{Prospectus}} | {{Behind The Net}}: {{The Value}} of a {{Wide}}-{{Open Shot}}},
  shorttitle = {Hockey {{Prospectus}} | {{Behind The Net}}},
  author = {Desjardins, Gabriel},
  year = {2017},
  month = aug,
  file = {C\:\\Users\\devan\\Zotero\\storage\\V9HEFJB7\\1cvHw.html},
  howpublished = {http://archive.is/1cvHw},
  journal = {Hockey Prospectus}
}

@article{devlinNetworkDiffusionRanking2018,
  title = {A Network Diffusion Ranking Family That Includes the Methods of {{Markov}}, {{Massey}}, and {{Colley}}},
  author = {Devlin, Stephen and Treloar, Thomas},
  year = {2018},
  month = sep,
  volume = {14},
  pages = {91--101},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2017-0098},
  abstract = {We present a one parameter family of ratings and rankings that includes the Markov method, as well as the methods of Colley and Massey as particular cases. The rankings are based on a natural network diffusion process that unites the methodologies above in a common framework and brings strong intuition to how and why they differ. We also explore the behavior of the ranking family using both real and simulated data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\28LBGX55\\Devlin and Treloar - 2018 - A network diffusion ranking family that includes t.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {3}
}

@article{diaz-avalosSignificanceTestsCovariatedependent2014,
  title = {Significance Tests for Covariate-Dependent Trends in Inhomogeneous Spatio-Temporal Point Processes},
  author = {{D{\'i}az-Avalos}, Carlos and Juan, P. and Mateu, J.},
  year = {2014},
  month = mar,
  volume = {28},
  pages = {593--609},
  issn = {1436-3240, 1436-3259},
  doi = {10.1007/s00477-013-0775-1},
  abstract = {Modeling and inference for spatial and spatiotemporal point processes is an issue that has been broadly investigated in the last years. Application fields such as forestry, epidemiology and ecology have been the main engine driving such raised interest. The inclusion of spatially varying covariates in the models for the intensity function is becoming of particular interest, but little attention has been paid to testing the significance of such covariates. Testing the significance of covariates is important if one seeks to explain which covariates have an effect in the spatial or spatio-temporal distribution of the point pattern observed. We thus provide practical procedures to build statistical tests of significance for covariates that have an effect on the intensity function of a point pattern. Our approximation focuses on the conditional intensity function, by considering nonparametric kernelbased estimators. We calculate thinning probabilities under the conditions of absence and presence of a covariate and compare them through divergence measures. Based on Monte Carlo experiments, we approximate the statistical properties of our tests under a variety of practical scenarios. An application on testing the significance of a covariate in a spatio-temporal data set on wildfires is also developed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BTRITD7R\\Díaz-Avalos et al. - 2014 - Significance tests for covariate-dependent trends .pdf},
  journal = {Stochastic Environmental Research and Risk Assessment},
  language = {en},
  number = {3}
}

@article{dickey1970weighted,
  title = {The Weighted Likelihood Ratio, Sharp Hypotheses about Chances, the Order of a {{Markov}} Chain},
  author = {Dickey, James M and Lientz, BP},
  year = {1970},
  pages = {214--226},
  publisher = {{JSTOR}},
  journal = {The Annals of Mathematical Statistics}
}

@article{dickeyWeightedLikelihoodRatio1970,
  title = {The {{Weighted Likelihood Ratio}}, {{Sharp Hypotheses}} about {{Chances}}, the {{Order}} of a {{Markov Chain}}},
  author = {Dickey, James M. and Lientz, B. P.},
  year = {1970},
  volume = {41},
  pages = {214--226},
  issn = {0003-4851},
  abstract = {The Bayesian theory for testing a sharp hypothesis, defined by fixed values of parameters, is here presented in general terms. Arbitrary positive prior probability is attached to the hypothesis. The ratio of posterior to prior odds for the hypothesis is given by the weighted likelihood ratio, shown here to equal Leonard J. Savage's (1963) ratio of a posterior to a prior density (2.21). This Bayesian approach to hypothesis testing was suggested by Jeffreys (1948), Savage (1959), (1961), Lindley (1961), and Good (1950), (1965), but obscured some what by approximations and unique choices of prior distributions. This Bayesian theory is distinct from that of Lindley (1965) and that of Dickey (1967a). Applications are given to hypotheses about multinomial means, for example, equality of two binomial probabilities. A new test is presented for the order of a finite-state Markov chain.},
  journal = {The Annals of Mathematical Statistics},
  number = {1}
}

@article{diggleEquivalenceSmoothingParameter1988,
  title = {Equivalence of {{Smoothing Parameter Selectors}} in {{Density}} and {{Intensity Estimation}}},
  author = {Diggle, Peter and Marron, J. S.},
  year = {1988},
  month = sep,
  volume = {83},
  pages = {793--800},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1988.10478665},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TE7ETZLU\\Diggle and Marron - 1988 - Equivalence of Smoothing Parameter Selectors in De.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {403}
}

@article{diggleJointModellingRepeated2008,
  title = {Joint Modelling of Repeated Measurements and Time-to-event Outcomes: {{The}} Fourth {{Armitage}} Lecture},
  shorttitle = {Joint Modelling of Repeated Measurements and Time-to-event Outcomes},
  author = {Diggle, Peter J. and Sousa, In{\^e}s and Chetwynd, Amanda G.},
  year = {2008},
  month = jul,
  volume = {27},
  pages = {2981--2998},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3131},
  abstract = {In many longitudinal studies, the outcomes recorded on each subject include both a sequence of repeated measurements at pre-specified times and the time at which an event of particular interest occurs: for example, death, recurrence of symptoms or drop out from the study. The event time for each subject may be recorded exactly, interval censored or right censored. The term joint modelling refers to the statistical analysis of the resulting data while taking account of any association between the repeated measurement and time-to-event outcomes. In this paper, we first discuss different approaches to joint modelling and argue that the analysis strategy should depend on the scientific focus of the study. We then describe in detail a particularly simple, fully parametric approach. Finally, we use this approach to re-analyse data from a clinical trial of drug therapies for schizophrenic patients, in which the event time is an intervalcensored or right-censored time to withdrawal from the study due to adverse side effects. Copyright q 2007 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4GM64VQP\\Diggle et al. - 2008 - Joint modelling of repeated measurements and time‐.pdf;C\:\\Users\\devan\\Zotero\\storage\\9MPBU5XU\\Diggle et al. - 2008 - Joint modelling of repeated measurements and time‐.pdf;C\:\\Users\\devan\\Zotero\\storage\\LAUMSNMF\\Diggle et al. - 2008 - Joint modelling of repeated measurements and time‐.pdf;C\:\\Users\\devan\\Zotero\\storage\\UE2QBTKG\\Diggle et al. - 2008 - Joint modelling of repeated measurements and time‐.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {16}
}

@article{diggleKernelMethodSmoothing1985,
  title = {A {{Kernel Method}} for {{Smoothing Point Process Data}}},
  author = {Diggle, Peter J.},
  year = {1985},
  volume = {34},
  pages = {138},
  issn = {00359254},
  doi = {10.2307/2347366},
  abstract = {A method for estimating the local intensity of a one-dimensional point process is described. The estimator uses an adaptation of Rosenblatt's kernel method of non-parametric probability density estimation, with a correction for end-effects. An expression for the mean squared error is derived on the assumption that the underlying process is a stationary Cox process, and this result is used to suggest a practical method for choosing the value of the smoothing constant. The performance of the estimator is illustrated using simulated data. An application to data on the locations of joints along a coal seam is described. The extension to two-dimensional point processes is noted.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\855ZPY24\\Diggle - 1985 - A Kernel Method for Smoothing Point Process Data.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {2}
}

@article{diggleModelbasedGeostatistics,
  title = {Model-Based Geostatistics},
  author = {Diggle, P J and Tawn, J A and Moyeed, R A},
  pages = {52},
  abstract = {Conventional geostatistical methodology solves the problem of predicting the realized value of a linear functional of a Gaussian spatial stochastic process Sx) based on observations Yi  Sxi   Zi at sampling locations xi , where the Zi are mutually independent, zero-mean Gaussian random variables. We describe two spatial applications for which Gaussian distributional assumptions are clearly inappropriate. The \textregistered rst concerns the assessment of residual contamination from nuclear weapons testing on a South Paci\textregistered c island, in which the sampling method generates spatially indexed Poisson counts conditional on an unobserved spatially varying intensity of radioactivity; we conclude that a conventional geostatistical analysis oversmooths the data and underestimates the spatial extremes of the intensity. The second application provides a description of spatial variation in the risk of campylobacter infections relative to other enteric infections in part of north Lancashire and south Cumbria. For this application, we treat the data as binomial counts at unit postcode locations, conditionally on an unobserved relative risk surface which we estimate. The theoretical framework for our extension of geostatistical methods is that, conditionally on the unobserved process Sx, observations at sample locations xi form a generalized linear model with the corresponding values of Sxi  appearing as an offset term in the linear predictor. We use a Bayesian inferential framework, implemented via the Markov chain Monte Carlo method, to solve the prediction problem for non-linear functionals of Sx, making a proper allowance for the uncertainty in the estimation of any model parameters.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Y76Y4WVD\\Diggle et al. - Model-based geostatistics.pdf},
  language = {en}
}

@article{digglePointProcessMethodology2005,
  title = {Point Process Methodology for On-Line Spatio-Temporal Disease Surveillance},
  author = {Diggle, Peter and Rowlingson, Barry and Su, Ting-li},
  year = {2005},
  month = aug,
  volume = {16},
  pages = {423--434},
  issn = {1180-4009, 1099-095X},
  doi = {10.1002/env.712},
  abstract = {We formulate the problem of on-line spatio-temporal disease surveillance in terms of predicting spatially and temporally localised excursions over a pre-specified threshold value for the spatially and temporally varying intensity of a point process in which each point represents an individual case of the disease in question. Our point process model is a non-stationary log-Gaussian Cox process in which the spatio-temporal intensity,  \dh x; t\TH, has a multiplicative decomposition into two deterministic components, one describing purely spatial and the other purely temporal variation in the normal disease incidence pattern, and an unobserved stochastic component representing spatially and temporally localised departures from the normal pattern. We give methods for estimating the parameters of the model, and for making probabilistic predictions of the current intensity. We describe an application to on-line spatio-temporal surveillance of non-specific gastroenteric disease in the county of Hampshire, UK. The results are presented as maps of exceedance probabilities, P\{R(x; t) {$>$} cjdata\}, where R\dh x; t\TH{} is the current realisation of the unobserved stochastic component of  \dh x; t\TH{} and c is a pre-specified threshold. These maps are updated automatically in response to each day's incident data using a web-based reporting system. Copyright \# 2005 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IRTF54ST\\Diggle et al. - 2005 - Point process methodology for on-line spatio-tempo.pdf},
  journal = {Environmetrics},
  language = {en},
  number = {5}
}

@article{diggleSecondOrderAnalysisInhomogeneous2007,
  title = {Second-{{Order Analysis}} of {{Inhomogeneous Spatial Point Processes Using Case}}-{{Control Data}}},
  author = {Diggle, P. J. and {G{\'o}mez-Rubio}, V. and Brown, P. E. and Chetwynd, A. G. and Gooding, S.},
  year = {2007},
  volume = {63},
  pages = {550--557},
  abstract = {Methods for the statistical analysis of stationary spatial point process data are now well established, methods for nonstationary processes less so. One of many sources of nonstationary point process data is a case-control study in environmental epidemiology. In that context, the data consist of a realization of each of two spatial point processes representing the locations, within a specified geographical region, of individual cases of a disease and of controls drawn at random from the population at risk. In this article, we extend work by Baddeley, Moller, and Waagepetersen (2000, Statistica Neerlandica 54, 329-350) concerning estimation of the second-order properties of a nonstationary spatial point process. First, we show how casecontrol data can be used to overcome the problems encountered when using the same data to estimate both a spatially varying intensity and second-order properties. Second, we propose a semiparametric method for adjusting the estimate of intensity so as to take account of explanatory variables attached to the cases and controls. Our primary focus is estimation, but we also propose a new test for spatial clustering that we show to be competitive with existing tests. We describe an application to an ecological study in which juvenile and surviving adult trees assume the roles of controls and cases.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NZQJF9WG\\Diggle et al. - 2007 - Second-Order Analysis of Inhomogeneous Spatial Poi.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{diggleSecondorderAnalysisSpacetime1995,
  title = {Second-Order Analysis of Space-Time Clustering},
  author = {Diggle, Pj and Chetwynd, Ag and H{\"a}ggkvist, R. and Morris, Se},
  year = {1995},
  month = jun,
  volume = {4},
  pages = {124--136},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/096228029500400203},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YG8ZD7MJ\\Diggle et al. - 1995 - Second-order analysis of space-time clustering.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {2}
}

@article{diggleSecondOrderAnalysisSpatial1991,
  title = {Second-{{Order Analysis}} of {{Spatial Clustering}} for {{Inhomogeneous Populations}}},
  author = {Diggle, P. J. and Chetwynd, A. G.},
  year = {1991},
  month = sep,
  volume = {47},
  pages = {1155},
  issn = {0006341X},
  doi = {10.2307/2532668},
  abstract = {Motivated by recent interest in the possible spatial clustering of rare diseases, the paper develops an approach to the assessment of spatial clustering based on the second-moment properties of a labelled point process. The concept of no spatial clustering is identified with the hypothesis that in a realisation of a stationary spatial point process consisting of events of two qualitatively different types, the type 1 events are a random sample from the superposition of type 1 and type 2 events. A diagnostic plot for estimating the nature and physical scale of clustering effects is proposed. The availability of Monte Carlo tests of significance is noted. An application to published data on the spatial distribution of childhood leukaemia and lymphoma in North Humberside is described.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FDIAFKD7\\Diggle and Chetwynd - 1991 - Second-Order Analysis of Spatial Clustering for In.pdf;C\:\\Users\\devan\\Zotero\\storage\\Q7YA6P63\\Diggle and Chetwynd - 1991 - Second-Order Analysis of Spatial Clustering for In.pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{diggleSpatialSpatioTemporalLogGaussian2013,
  title = {Spatial and {{Spatio}}-{{Temporal Log}}-{{Gaussian Cox Processes}}: {{Extending}} the {{Geostatistical Paradigm}}},
  shorttitle = {Spatial and {{Spatio}}-{{Temporal Log}}-{{Gaussian Cox Processes}}},
  author = {Diggle, Peter J. and Moraga, Paula and Rowlingson, Barry and Taylor, Benjamin M.},
  year = {2013},
  month = nov,
  volume = {28},
  pages = {542--563},
  issn = {0883-4237},
  doi = {10.1214/13-STS441},
  abstract = {In this paper we first describe the class of log-Gaussian Cox processes (LGCPs) as models for spatial and spatio-temporal point process data. We discuss inference, with a particular focus on the computational challenges of likelihood-based inference. We then demonstrate the usefulness of the LGCP by describing four applications: estimating the intensity surface of a spatial point process; investigating spatial segregation in a multi-type process; constructing spatially continuous maps of disease risk from spatially discrete data; and real-time health surveillance. We argue that problems of this kind fit naturally into the realm of geostatistics, which traditionally is defined as the study of spatially continuous processes using spatially discrete observations at a finite number of locations. We suggest that a more useful definition of geostatistics is by the class of scientific problems that it addresses, rather than by particular models or data formats.},
  archivePrefix = {arXiv},
  eprint = {1312.6536},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GNVFMBLB\\Diggle et al. - 2013 - Spatial and Spatio-Temporal Log-Gaussian Cox Proce.pdf},
  journal = {Statistical Science},
  keywords = {Statistics - Methodology},
  language = {en},
  number = {4}
}

@book{diggleSpatiotemporalPointProcesses2013,
  title = {Spatio-Temporal {{Point Processes}}: {{Methods}} and {{Applications}}},
  author = {Diggle, Peter J},
  year = {2013},
  publisher = {{CRC Press}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\53BJ27N2\\Diggle - Spatio-temporal Point Processes Methods and Appli.pdf},
  isbn = {978-1-4665-6023-9},
  language = {en},
  series = {Chapman \& {{Hall}}/{{CRC Monographs}} on {{Statistics}} and {{Applied Probability}}}
}

@book{diggleStatisticalAnalysisSpatial2013,
  title = {Statistical {{Analysis}} of {{Spatial}} and {{Spatio}}-{{Temporal Point Patterns}}},
  author = {Diggle, Peter J.},
  year = {2013},
  edition = {Third},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Written by a prominent statistician and author, the first edition of this bestseller broke new ground in the then emerging subject of spatial statistics with its coverage of spatial point patterns. Retaining all the material from the second edition and adding substantial new material, Statistical An},
  file = {C\:\\Users\\devan\\Zotero\\storage\\V9Q4N27N\\9781466560239.html},
  isbn = {978-1-4665-6023-9},
  language = {en},
  series = {Chapman \& {{Hall}}/{{CRC Monographs}} on {{Statistics}} and {{Applied Probability}}}
}

@article{dobsonDiagnosticsJointLongitudinal2003,
  title = {Diagnostics for {{Joint Longitudinal}} and {{Dropout Time Modeling}}},
  author = {Dobson, Angela and Henderson, Robin},
  year = {2003},
  month = dec,
  volume = {59},
  pages = {741--751},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/j.0006-341X.2003.00087.x},
  abstract = {We present a variety of informal graphical procedures for diagnostic assessment of joint models for longitudinal and dropout time data. A random effects approach for Gaussian responses and proportional hazards dropout time is assumed. We consider preliminary assessment of dropout classification categories based on residuals following a standard longitudinal data analysis with no allowance for informative dropout. Residual properties conditional upon dropout information are discussed and case influence is considered. The proposed methods do not require computationally intensive methods over and above those used to fit the proposed model. A longitudinal trial into the treatment of schizophrenia is used to illustrate the suggestions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\S5UETTKP\\Dobson and Henderson - 2003 - Diagnostics for Joint Longitudinal and Dropout Tim.pdf;C\:\\Users\\devan\\Zotero\\storage\\TPLITUN4\\Dobson and Henderson - 2003 - Diagnostics for Joint Longitudinal and Dropout Tim.pdf},
  journal = {Biometrics},
  language = {en},
  number = {4}
}

@article{doWhatExpectationMaximization2008,
  title = {What Is the Expectation Maximization Algorithm?},
  author = {Do, Chuong B and Batzoglou, Serafim},
  year = {2008},
  month = aug,
  volume = {26},
  pages = {897--899},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt1406},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QWJ2JIQN\\Do and Batzoglou - 2008 - What is the expectation maximization algorithm.pdf;C\:\\Users\\devan\\Zotero\\storage\\Y728E6XB\\Do and Batzoglou - 2008 - What is the expectation maximization algorithm.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {8}
}

@article{drazekIntensityEstimationPoisson,
  title = {Intensity Estimation for {{Poisson}} Processes},
  author = {Drazek, Ludwik Czeslaw},
  pages = {83},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ALW3M7S6\\Drazek - Intensity estimation for Poisson processes.pdf},
  language = {en}
}

@misc{DrewHynesNhlapi2019,
  title = {Drew {{Hynes}} / Nhlapi},
  year = {2019},
  abstract = {Documenting the publicly accessible portions of the NHL API},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3AI4BQHI\\nhlapi.html},
  howpublished = {https://gitlab.com/dword4/nhlapi},
  journal = {GitLab},
  language = {en}
}

@book{drummondBayesianEvolutionaryAnalysis2015,
  title = {Bayesian {{Evolutionary Analysis}} with {{BEAST}}},
  author = {Drummond, Alexei J. and Bouckaert, Remco R.},
  year = {2015},
  month = aug,
  publisher = {{Cambridge University Press}},
  abstract = {What are the models used in phylogenetic analysis and what exactly is involved in Bayesian evolutionary analysis using Markov chain Monte Carlo (MCMC) methods? How can you choose and apply these models, which parameterisations and priors make sense, and how can you diagnose Bayesian MCMC when things go wrong? These are just a few of the questions answered in this comprehensive overview of Bayesian approaches to phylogenetics. This practical guide: \textbullet{} Addresses the theoretical aspects of the field \textbullet{} Advises on how to prepare and perform phylogenetic analysis \textbullet{} Helps with interpreting analyses and visualisation of phylogenies \textbullet{} Describes the software architecture \textbullet{} Helps developing BEAST 2.2 extensions to allow these models to be extended further. With an accompanying website providing example files and tutorials (http://beast2.org/), this one-stop reference to applying the latest phylogenetic models in BEAST 2 will provide essential guidance for all users - from those using phylogenetic tools, to computational biologists and Bayesian statisticians.},
  googlebooks = {deHUrQEACAAJ},
  isbn = {978-1-107-01965-2},
  keywords = {Science / Life Sciences / Biology,Science / Life Sciences / Genetics \& Genomics},
  language = {en}
}

@article{dunsonBayesianApproachJoint2003,
  title = {A {{Bayesian Approach}} for {{Joint Modeling}} of {{Cluster Size}} and {{Subunit}}-{{Specific Outcomes}}},
  author = {Dunson, David B. and Chen, Zhen and Harry, Jean},
  year = {2003},
  month = sep,
  volume = {59},
  pages = {521--530},
  issn = {0006341X, 15410420},
  doi = {10.1111/1541-0420.00062},
  abstract = {In applications that involve clustered data, such as longitudinal studies and developmental toxicity experiments, the number of subunits within a cluster is often correlated with outcomes measured on the individual subunits. Analyses that ignore this dependency can produce biased inferences. This article proposes a Bayesian framework for jointly modeling cluster size and multiple categorical and continuous outcomes measured on each subunit. We use a continuation ratio probit model for the cluster size and underlying normal regression models for each of the subunit-specific outcomes. Dependency between cluster size and the different outcomes is accommodated through a latent variable structure. The form of the model facilitates posterior computation via a simple and computationally efficient Gibbs sampler. The approach is illustrated with an application to developmental toxicity data, and other applications, to joint modeling of longitudinal and event time data, are discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KUUH2FAN\\Dunson et al. - 2003 - A Bayesian Approach for Joint Modeling of Cluster .pdf;C\:\\Users\\devan\\Zotero\\storage\\RHHIQJBN\\Dunson et al. - 2003 - A Bayesian Approach for Joint Modeling of Cluster .pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{dunsonBayesianInferenceOrderConstrained2003,
  title = {Bayesian {{Inference}} on {{Order}}-{{Constrained Parameters}} in {{Generalized Linear Models}}},
  author = {Dunson, David B. and Neelon, Brian},
  year = {2003},
  month = jun,
  volume = {59},
  pages = {286--295},
  issn = {0006341X},
  doi = {10.1111/1541-0420.00035},
  abstract = {In biomedical studies, there is often interest in assessing the association between one or more ordered categorical predictors and an outcome variable, adjusting for covariates. For a k-level predictor, one typically uses either a k - 1 degree of freedom (df) test or a single df trend test, which requires scores for the different levels of the predictor. In the absence of knowledge of a parametric form for the response function, one can incorporate monotonicity constraints to improve the efficiency of tests of association. This article proposes a general Bayesian approach for inference on order-constrained parameters in generalized linear models. Instead of choosing a prior distribution with support on the constrained space, which can result in major computational difficulties, we propose to map draws from an unconstrained posterior density using an isotonic regression transformation. This approach allows flat regions over which increases in the level of a predictor have no effect. Bayes factors for assessing ordered trends can be computed based on the output from a Gibbs sampling algorithm. Results from a simulation study are presented and the approach is applied to data from a time-to-pregnancy study.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CEFJPMU2\\Dunson and Neelon - 2003 - Bayesian Inference on Order-Constrained Parameters.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{dunsonBayesianLatentVariable2000,
  title = {Bayesian {{Latent Variable Models}} for {{Clustered Mixed Outcomes}}},
  author = {Dunson, David B.},
  year = {2000},
  volume = {62},
  pages = {355--366},
  abstract = {A generalframeworkis proposedformodellingclusteredmixedoutcomes.A mixtureof generalizedlinearmodels is used to describethe jointdistributionof a set of underlyingvariables, and an arbitraryfunctionrelates the underlyingvariablesto the observed outcomes. The model accommodatesmultileveldata structures,generalcovariateeffects and distinctlinkfunctionsand errordistributionfsoreach underlyingvariable.Withinthe frameworkproposed,novel models are developed for clusteredmultiplebinary,unorderedcategoricaland jointdiscrete and continuous outcomes.A MarkovchainMonteCarlosamplingalgorithmis describedforestimatingthe posterior distributionsof the parametersand latentvariables.Because of the flexibilityof the modelling frameworkandestimationproceduree, xtensionstoorderedcategoricaloutcomesandmorecomplex data structuresare straightforwardT.he methodsare illustratedby usingdata froma reproductive toxicitystudy.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7KFX55PI\\Dunson - 2000 - Bayesian Latent Variable Models for Clustered Mixe.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  language = {en},
  number = {2}
}

@article{dunsonBayesianLatentVariable2000a,
  title = {Bayesian Latent Variable Models for Clustered Mixed Outcomes},
  author = {Dunson, D. B.},
  year = {2000},
  volume = {62},
  pages = {355--366},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00236},
  abstract = {A general framework is proposed for modelling clustered mixed outcomes. A mixture of generalized linear models is used to describe the joint distribution of a set of underlying variables, and an arbitrary function relates the underlying variables to be observed outcomes. The model accommodates multilevel data structures, general covariate effects and distinct link functions and error distributions for each underlying variable. Within the framework proposed, novel models are developed for clustered multiple binary, unordered categorical and joint discrete and continuous outcomes. A Markov chain Monte Carlo sampling algorithm is described for estimating the posterior distributions of the parameters and latent variables. Because of the flexibility of the modelling framework and estimation procedure, extensions to ordered categorical outcomes and more complex data structures are straightforward. The methods are illustrated by using data from a reproductive toxicity study.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3FKB7XCN\\Dunson - 2000 - Bayesian latent variable models for clustered mixe.pdf;C\:\\Users\\devan\\Zotero\\storage\\QLJKDCSG\\1467-9868.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Gibbs sampler,Mixture model,Multihit model,Multiple outcomes,Reproductive applications},
  language = {en},
  number = {2}
}

@book{durrettProbabilityModelsDNA2010,
  title = {Probability {{Models}} for {{DNA Sequence Evolution}}},
  author = {Durrett, Richard},
  year = {2010},
  edition = {2. ed},
  publisher = {{Springer}},
  address = {{New York, NY}},
  annotation = {OCLC: 844038535},
  file = {C\:\\Users\\devan\\Zotero\\storage\\S5J8FPAF\\Durrett - 2010 - Probability Models for DNA Sequence Evolution.pdf},
  isbn = {978-0-387-78169-3 978-1-4419-2677-7},
  language = {en},
  series = {Probability and Its {{Applications}}}
}

@article{duttaIdentifyingNCAATournament2017,
  title = {Identifying {{NCAA}} Tournament Upsets Using {{Balance Optimization Subset Selection}}},
  author = {Dutta, Shouvik and Jacobson, Sheldon H. and Sauppe, Jason J.},
  year = {2017},
  month = jan,
  volume = {13},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2016-0062},
  abstract = {The NCAA basketball tournament attracts over 60 million people who fill out a bracket to try to predict the outcome of every tournament game correctly. Predictions are often made on the basis of instinct, statistics, or a combination of the two. This paper proposes a technique to select round-of-64 upsets in the tournament using a Balance Optimization Subset Selection model. The model determines which games feature match-ups that are statistically most similar to the match-ups in historical upsets. The technique is then applied to the tournament in each of the 13 years from 2003 to 2015 in order to select two games as potential upsets each year. Of the 26 selected games, 10 (38.4\%) were actual upsets, which is more than twice as many as the expected number of correct selections when using a weighted random selection method.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Z2YBH86K\\Dutta et al. - 2017 - Identifying NCAA tournament upsets using Balance O.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {2}
}

@article{eagerMixedEffectsModels,
  title = {Mixed {{Effects Models}} Are {{Sometimes Terrible}}},
  author = {Eager, Christopher and Roy, Joseph},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4ZPHPLTR\\1701.04858.pdf}
}

@article{earnestSmallAreaEstimation2010,
  title = {Small Area Estimation of Sparse Disease Counts Using Shared Component Models-Application to Birth Defect Registry Data in {{New South Wales}}, {{Australia}}},
  author = {Earnest, Arul and Beard, John R. and Morgan, Geoff and Lincoln, Douglas and Summerhayes, Richard and Donoghue, Deborah and Dunn, Therese and Muscatello, David and Mengersen, Kerrie},
  year = {2010},
  month = jul,
  volume = {16},
  pages = {684--693},
  issn = {13538292},
  doi = {10.1016/j.healthplace.2010.02.006},
  abstract = {In the field of disease mapping, little has been done to address the issue of analysing sparse health datasets. We hypothesised that by modelling two outcomes simultaneously, one would be able to better estimate the outcome with a sparse count. We tested this hypothesis utilising Bayesian models, studying both birth defects and caesarean sections using data from two large, linked birth registries in New South Wales from 1990 to 2004. We compared four spatial models across seven birth defects: spina bifida, ventricular septal defect, OS atrial septal defect, patent ductus arteriosus, cleft lip and or palate, trisomy 21 and hypospadias. For three of the birth defects, the shared component model with a zero-inflated Poisson (ZIP) extension performed better than other simpler models, having a lower deviance information criteria (DIC). With spina bifida, the ratio of relative risk associated with the shared component was 2.82 (95\% CI: 1.46\textendash 5.67). We found that shared component models are potentially beneficial, but only if there is a reasonably strong spatial correlation in effect for the study and referent outcomes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RWZNTDCI\\Earnest et al. - 2010 - Small area estimation of sparse disease counts usi.pdf},
  journal = {Health \& Place},
  language = {en},
  number = {4}
}

@techreport{ecologicalstratificationworkinggroupcanadaNationalEcologicalFramework1996,
  title = {A National Ecological Framework for {{Canada}}},
  author = {{Ecological Stratification Working Group (Canada)} and {Center for Land, Biological Resources Research (Canada)}},
  year = {1996},
  address = {{Hull, Quebec}},
  institution = {{Centre for Land and Biological Resources Research}},
  type = {State of the {{Environment Directorate}}}
}

@article{edwards1963bayesian,
  title = {Bayesian Statistical Inference for Psychological Research.},
  author = {Edwards, Ward and Lindman, Harold and Savage, Leonard J},
  year = {1963},
  volume = {70},
  pages = {193},
  publisher = {{American Psychological Association}},
  journal = {Psychological review},
  number = {3}
}

@book{el-shaarawiEncyclopediaEnvironmetrics2002,
  title = {Encyclopedia of Environmetrics},
  editor = {{El-Shaarawi}, A. H. and Piegorsch, Walter W.},
  year = {2002},
  publisher = {{Wiley}},
  address = {{Chichester ; New York}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NQPIJVRW\\El-Shaarawi and Piegorsch - 2002 - Encyclopedia of environmetrics.pdf},
  isbn = {978-0-471-89997-6},
  keywords = {Encyclopedias,Environmental sciences,Methodology,Statistical methods},
  language = {en},
  lccn = {GE45.S73 E53 2002}
}

@book{el-shaarawiEncyclopediaEnvironmetrics2002a,
  title = {Encyclopedia of Environmetrics},
  editor = {{El-Shaarawi}, A. H. and Piegorsch, Walter W.},
  year = {2002},
  publisher = {{Wiley}},
  address = {{Chichester ; New York}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8WDAL73R\\El-Shaarawi and Piegorsch - 2002 - Encyclopedia of environmetrics.pdf;C\:\\Users\\devan\\Zotero\\storage\\96SNKC22\\El-Shaarawi and Piegorsch - 2002 - Encyclopedia of environmetrics.pdf;C\:\\Users\\devan\\Zotero\\storage\\I67ZSC4Y\\El-Shaarawi and Piegorsch - 2002 - Encyclopedia of environmetrics.pdf;C\:\\Users\\devan\\Zotero\\storage\\SVUAIJG5\\El-Shaarawi and Piegorsch - 2002 - Encyclopedia of environmetrics.pdf},
  isbn = {978-0-471-89997-6},
  keywords = {Encyclopedias,Environmental sciences,Methodology,Statistical methods},
  language = {en},
  lccn = {GE45.S73 E53 2002}
}

@article{eldersNonparametricBayesianInference,
  title = {Nonparametric {{Bayesian Inference}} of {{Inhomogeneous Poisson Process Intensities}}},
  author = {Elders, Amon},
  pages = {26},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3FGWVJQG\\Elders - Nonparametric Bayesian Inference of Inhomogeneous .pdf;C\:\\Users\\devan\\Zotero\\storage\\HZWY9JTX\\Elders - Nonparametric Bayesian Inference of Inhomogeneous .pdf},
  language = {en}
}

@misc{ellisNHLGameData2018,
  title = {{{NHL Game Data}}},
  author = {Ellis, Martin},
  year = {2018},
  abstract = {Game, team, player and plays information including x,y coordinates},
  file = {C\:\\Users\\devan\\Zotero\\storage\\A3EQTQ36\\nhl-game-data.html},
  howpublished = {https://kaggle.com/martinellis/nhl-game-data},
  journal = {Kaggle},
  language = {en}
}

@article{entekhabiProbabilisticRepresentationTemporal1989,
  title = {Probabilistic Representation of the Temporal Rainfall Process by a Modified {{Neyman}}-{{Scott Rectangular Pulses Model}}: {{Parameter}} Estimation and Validation},
  shorttitle = {Probabilistic Representation of the Temporal Rainfall Process by a Modified {{Neyman}}-{{Scott Rectangular Pulses Model}}},
  author = {Entekhabi, Dara and {Rodriguez-Iturbe}, Ignacio and Eagleson, Peter S.},
  year = {1989},
  month = feb,
  volume = {25},
  pages = {295--302},
  issn = {00431397},
  doi = {10.1029/WR025i002p00295},
  file = {C\:\\Users\\devan\\Zotero\\storage\\L46VKB28\\Entekhabi et al. - 1989 - Probabilistic representation of the temporal rainf.pdf;C\:\\Users\\devan\\Zotero\\storage\\M37Y62X5\\Entekhabi et al. - 1989 - Probabilistic representation of the temporal rainf.pdf},
  journal = {Water Resources Research},
  language = {en},
  number = {2}
}

@misc{EucalyptsFire,
  title = {Eucalypts and {{Fire}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DU6EJXMY\\Heitjan et al. - 2016 - Detecting outliers in complex profiles using a $chi2$ control chart method(62).pdf},
  keywords = {Fire Good}
}

@article{faganAdvantageLeftiesOneonone2019,
  title = {The Advantage of Lefties in One-on-One Sports},
  author = {Fagan, Francois and Haugh, Martin and Cooper, Hal},
  year = {2019},
  month = feb,
  volume = {15},
  pages = {1--25},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2017-0076},
  abstract = {Left-handers comprise approximately 15\% of professional tennis players, but only 11\% of the general population. In boxing, baseball, fencing, table-tennis and specialist batting positions in cricket the contrast is even starker, with 30\% or more of top players often being lefthanded. In this paper we propose a model for identifying the advantage of being left-handed in one-on-one interactive sports (as well as the inherent skill of each player). We construct a Bayesian latent ability model in the spirit of the classic Glicko model but with the additional complication of having a latent factor, i.e. the advantage of lefthandedness, that we need to estimate. Inference is further complicated by the truncated nature of data-sets that arise from only having data of the top players. We show how to infer the advantage of left-handedness when only the proportion of top left-handed players is available. We use this result to develop a simple dynamic model for inferring how the advantage of left-handedness varies through time. We also extend the model to cases where we have ranking or match-play data. We test these models on 2014 matchplay data from top male professional tennis players, and the dynamic model on data from 1985 to 2016.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9V8NZT8H\\Fagan et al. - 2019 - The advantage of lefties in one-on-one sports.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {1}
}

@article{fanLocalEMEMSAlgorithm2011,
  title = {Local-{{EM}} and the {{EMS Algorithm}}},
  author = {Fan, Chun-Po Steve and Stafford, Jamie and Brown, Patrick E.},
  year = {2011},
  month = jan,
  volume = {20},
  pages = {750--766},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2011.10106},
  file = {C\:\\Users\\devan\\Zotero\\storage\\A9EQIIXX\\Fan et al. - 2011 - Local-EM and the EMS Algorithm.pdf;C\:\\Users\\devan\\Zotero\\storage\\GQDGZXAK\\Fan et al. - 2011 - Local-EM and the EMS Algorithm.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {3}
}

@misc{Fellows2016,
  title = {{{OpenStreetMap}}: {{Access}} to {{Open Street Map Raster Images}}},
  author = {Fellows, Ian},
  year = {2016}
}

@article{fengJointAnalysisMultivariate2012,
  title = {Joint Analysis of Multivariate Spatial Count and Zero-Heavy Count Outcomes Using Common Spatial Factor Models: {{JOINT ANALYSIS OF SPATIAL COUNT OUTCOMES}}},
  shorttitle = {Joint Analysis of Multivariate Spatial Count and Zero-Heavy Count Outcomes Using Common Spatial Factor Models},
  author = {Feng, C.X. and Dean, C.B.},
  year = {2012},
  month = sep,
  volume = {23},
  pages = {493--508},
  issn = {11804009},
  doi = {10.1002/env.2158},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Z6GKSUP7\\Feng and Dean - 2012 - Joint analysis of multivariate spatial count and z.pdf},
  journal = {Environmetrics},
  language = {en},
  number = {6}
}

@article{fengMODELSMETHODSSPATIAL,
  title = {{{MODELS AND METHODS FOR SPATIAL DATA}}: {{APPLICATIONS IN EPIDEMIOLOGICAL}}, {{ENVIRONMENTAL AND ECOLOGICAL STUDIES}}},
  author = {Feng, Cindy Xin},
  pages = {160},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6ILQT8PE\\Feng - MODELS AND METHODS FOR SPATIAL DATA APPLICATIONS .pdf},
  language = {en}
}

@article{fishmanStatisticalAnalysisSpacetime1976,
  title = {The Statistical Analysis of Space-Time Point Processes},
  author = {Fishman, P. and Snyder, D.},
  year = {1976},
  month = may,
  volume = {22},
  pages = {257--274},
  issn = {0018-9448},
  doi = {10.1109/TIT.1976.1055558},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UTAXSZFA\\Fishman and Snyder - 1976 - The statistical analysis of space-time point proce.pdf},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {3}
}

@book{forestrycanadaDevelopmentStructureCanadian1992,
  title = {Development and Structure of the {{Canadian Forest Fire Behavior Prediction System}}},
  author = {{Forestry Canada}},
  year = {1992},
  volume = {3},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\V3QJ5X6E\\Forestry Canada - 1992 - Development and structure of the Canadian Forest F.pdf;C\:\\Users\\devan\\Zotero\\storage\\VC29H2SS\\publications.html},
  isbn = {978-0-662-19812-3},
  language = {English}
}

@article{fortuneUseBayesianInference2014,
  title = {The Use of {{Bayesian}} Inference to Inform the Surveillance of Temperature-Related Occupational Morbidity in {{Ontario}}, {{Canada}}, 2004\textendash 2010},
  author = {Fortune, Melanie and Mustard, Cameron and Brown, Patrick},
  year = {2014},
  month = jul,
  volume = {132},
  pages = {449--456},
  issn = {00139351},
  doi = {10.1016/j.envres.2014.04.022},
  abstract = {Purpose: To assess the associations of occupational heat and cold-related illnesses presenting in emergency departments in south western Ontario, Canada, with daily meteorological conditions using Bayesian inference. Methodology: Meteorological and air pollution data for the south western economic region of Ontario were gathered from Environment Canada and the Ministry of Environment. Daily heat and cold-related emergency department visits clinically attributed to work from 2004 to 2010 were tabulated. A novel application of Bayesian inference on a flexible Poisson time series model was undertaken to examine linear and non-linear associations between average, regional meteorological conditions and daily morbidity rates, to adjust for relevant confounders and temporal trends, and to consider potential interactions. Results: Bilinear associations were observed between regional temperatures and morbidities resulting from extreme temperature exposures. The median increase in the daily rate of emergency department visits for heat illness was 75\% for each degree above 22 1C (posterior 95\% credible interval (CI) relative rate{$\frac{1}{4}$}1.56\textendash 1.99) in the daily maximum temperature. Below 0 1C, rates of occupational cold illness increased by a median of 15\% for each degree decrease in the minimum temperature (posterior 95\% CI 0.80\textendash 0.91); wind speed also had a significant effect. Conclusions: The observed associations can inform occupational surveillance and injury prevention programming, as well as public health efforts targeting vulnerable populations. Methodologically, the use of Bayesian inference in time series analyses of meteorological exposures is feasible and conducive to providing accurate advice for policy and practice.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3KQFGM8P\\Fortune et al. - 2014 - The use of Bayesian inference to inform the survei.pdf;C\:\\Users\\devan\\Zotero\\storage\\7HZLD5CP\\Fortune et al. - 2014 - The use of Bayesian inference to inform the survei.pdf},
  journal = {Environmental Research},
  language = {en}
}

@article{franksCharacterizingSpatialStructure2015,
  title = {Characterizing the Spatial Structure of Defensive Skill in Professional Basketball},
  author = {Franks, Alexander and Miller, Andrew and Bornn, Luke and Goldsberry, Kirk},
  year = {2015},
  month = mar,
  volume = {9},
  pages = {94--121},
  issn = {1932-6157},
  doi = {10.1214/14-AOAS799},
  abstract = {Although basketball is a dualistic sport, with all players competing on both offense and defense, almost all of the sport's conventional metrics are designed to summarize offensive play. As a result, player valuations are largely based on offensive performances and to a much lesser degree on defensive ones. Steals, blocks and defensive rebounds provide only a limited summary of defensive effectiveness, yet they persist because they summarize salient events that are easy to observe. Due to the inefficacy of traditional defensive statistics, the state of the art in defensive analytics remains qualitative, based on expert intuition and analysis that can be prone to human biases and imprecision. Fortunately, emerging optical player tracking systems have the potential to enable a richer quantitative characterization of basketball performance, particularly defensive performance. Unfortunately, due to computational and methodological complexities, that potential remains unmet. This paper attempts to fill this void, combining spatial and spatio-temporal processes, matrix factorization techniques and hierarchical regression models with player tracking data to advance the state of defensive analytics in the NBA. Our approach detects, characterizes and quantifies multiple aspects of defensive play in basketball, supporting some common understandings of defensive effectiveness, challenging others and opening up many new insights into the defensive elements of basketball.},
  archivePrefix = {arXiv},
  eprint = {1405.0231},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RNVUK6AC\\Franks et al. - 2015 - Characterizing the spatial structure of defensive .pdf;C\:\\Users\\devan\\Zotero\\storage\\XW8Q3LCF\\Franks et al. - 2015 - Characterizing the spatial structure of defensive .pdf},
  journal = {The Annals of Applied Statistics},
  keywords = {Statistics - Applications},
  language = {en},
  number = {1}
}

@article{franksMetaanalyticsToolsUnderstanding2016,
  title = {Meta-Analytics: Tools for Understanding the Statistical Properties of Sports Metrics},
  shorttitle = {Meta-Analytics},
  author = {Franks, Alexander M. and D'Amour, Alexander and Cervone, Daniel and Bornn, Luke},
  year = {2016},
  month = jan,
  volume = {12},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2016-0098},
  abstract = {In sports, there is a constant effort to improve metrics that assess player ability, but there has been almost no effort to quantify and compare existing metrics. Any individual making a management, coaching, or gambling decision is quickly overwhelmed with hundreds of statistics. We address this problem by proposing a set of ``meta-metrics'', which can be used to identify the metrics that provide the most unique and reliable information for decision-makers. Specifically, we develop methods to evaluate metrics based on three criteria: (1) stability: does the metric measure the same thing over time (2) discrimination: does the metric differentiate between players and (3) independence: does the metric provide new information? Our methods are easy to implement and widely applicable so they should be of interest to the broader sports community. We demonstrate our methods in analyses of both NBA and NHL metrics. Our results indicate the most reliable metrics and highlight how they should be used by sports analysts. The meta-metrics also provide useful insights about how to best construct new metrics that provide independent and reliable information about athletes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DGM8QKXC\\Franks et al. - 2016 - Meta-analytics tools for understanding the statis.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {4}
}

@article{frenchMarginalMarkRegression2009,
  title = {Marginal {{Mark Regression Analysis}} of {{Recurrent Marked Point Process Data}}},
  author = {French, Benjamin and Heagerty, Patrick J.},
  year = {2009},
  volume = {65},
  pages = {415--422},
  abstract = {Longitudinal studies typically collect information on the timing of key clinical events and on specific characte that describe those events. Random variables that measure qualitative or quantitative aspects associated with the occur of an event are known as marks. Recurrent marked point process data consist of possibly recurrent events, with the mar possibly exposure) measured if and only if an event occurs. Analysis choices depend on which aspect of the data is of pr scientific interest. First, factors that influence the occurrence or timing of the event may be characterized using recur event analysis methods. Second, if there is more than one event per subject, then the association between exposure an mark may be quantified using repeated measures regression methods. We detail assumptions required of any time-dep exposure process and the event time process to ensure that linear or generalized linear mixed models and generalized estim equations provide valid estimates. We provide theoretical and empirical evidence that if these conditions are not satisf then an independence estimating equation should be used for consistent estimation of association. We conclude with t recommendation that analysts carefully explore both the exposure and event time processes prior to implementing a rep measures analysis of recurrent marked point process data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4HMUVYS9\\French and Heagerty - 2009 - Marginal Mark Regression Analysis of Recurrent Mar.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZSXYNKQ9\\French and Heagerty - 2009 - Marginal Mark Regression Analysis of Recurrent Mar.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{frigyesiNonNegativeMatrixFactorization2008,
  title = {Non-{{Negative Matrix Factorization}} for the {{Analysis}} of {{Complex Gene Expression Data}}: {{Identification}} of {{Clinically Relevant Tumor Subtypes}}},
  shorttitle = {Non-{{Negative Matrix Factorization}} for the {{Analysis}} of {{Complex Gene Expression Data}}},
  author = {Frigyesi, Attila and H{\"o}glund, Mattias},
  year = {2008},
  month = jan,
  volume = {6},
  pages = {CIN.S606},
  issn = {1176-9351, 1176-9351},
  doi = {10.4137/CIN.S606},
  abstract = {Non-negative matrix factorization (NMF) is a relatively new approach to analyze gene expression data that models data by additive combinations of non-negative basis vectors (metagenes). The non-negativity constraint makes sense biologically as genes may either be expressed or not, but never show negative expression. We applied NMF to five different microarray data sets. We estimated the appropriate number metagens by comparing the residual error of NMF reconstruction of data to that of NMF reconstruction of permutated data, thus finding when a given solution contained more information than noise. This analysis also revealed that NMF could not factorize one of the data sets in a meaningful way. We used GO categories and pre defined gene sets to evaluate the biological significance of the obtained metagenes. By analyses of metagenes specific for the same GO-categories we could show that individual metagenes activated different aspects of the same biological processes. Several of the obtained metagenes correlated with tumor subtypes and tumors with characteristic chromosomal translocations, indicating that metagenes may correspond to specific disease entities. Hence, NMF extracts biological relevant structures of microarray expression data and may thus contribute to a deeper understanding of tumor behavior.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2ZV4YVI5\\Frigyesi and Höglund - 2008 - Non-Negative Matrix Factorization for the Analysis.pdf},
  journal = {Cancer Informatics},
  language = {en}
}

@techreport{ftmac,
  title = {A Review of the 2016 Horse River Wildfire.},
  author = {Nash, Todd and St Arnaud, Lee and Tithecott, Al and Simpson, Brian and Stocks, Biran and others},
  year = {2017},
  institution = {{MNP Consulting}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UGRHL2DN\\Nash et al. - 2017 - A review of the 2016 horse river wildfire..pdf}
}

@article{fuentesClassNonseparableNonstationary2008,
  title = {A Class of Nonseparable and Nonstationary Spatial Temporal Covariance Functions},
  author = {Fuentes, Montserrat and Chen, Li and Davis, Jerry M.},
  year = {2008},
  volume = {19},
  pages = {487--507},
  issn = {1099-095X},
  doi = {10.1002/env.891},
  abstract = {Spectral methods are powerful tools to study and model the dependency structure of spatial temporal processes. However, standard spectral approaches as well as geostatistical methods assume separability and stationarity of the covariance function; these can be very unrealistic assumptions in many settings. In this work, we introduce a general and flexible parametric class of spatial temporal covariance models, that allows for lack of stationarity and separability by using a spectral representation of the process. This new class of covariance models has a unique parameter that indicates the strength of the interaction between the spatial and temporal components; it has the separable covariance model as a particular case. We introduce an application with ambient ozone air pollution data provided by the U.S. Environmental Protection Agency (U.S. EPA). Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8IZZ2AXN\\Fuentes et al. - 2008 - A class of nonseparable and nonstationary spatial .pdf;C\:\\Users\\devan\\Zotero\\storage\\Y4TRLJVF\\env.html},
  journal = {Environmetrics},
  keywords = {ambient ozone,non separable covariance,nonseparability,nonstationarity,ozone modeling,spatial covariance,spatial temporal models,spectral density,spectral domain},
  language = {en},
  number = {5}
}

@article{fuglstadConstructingPriorsThat2017,
  title = {Constructing {{Priors}} That {{Penalize}} the {{Complexity}} of {{Gaussian Random Fields}}},
  author = {Fuglstad, Geir-Arne and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2017},
  month = nov,
  abstract = {Priors are important for achieving proper posteriors with physically meaningful covariance structures for Gaussian random fields (GRFs) since the likelihood typically only provides limited information about the covariance structure under in-fill asymptotics. We extend the recent Penalised Complexity prior framework and develop a principled joint prior for the range and the marginal variance of one-dimensional, two-dimensional and three-dimensional Mat\'ern GRFs with fixed smoothness. The prior is weakly informative and penalises complexity by shrinking the range towards infinity and the marginal variance towards zero. We propose guidelines for selecting the hyperparameters, and a simulation study shows that the new prior provides a principled alternative to reference priors that can leverage prior knowledge to achieve shorter credible intervals while maintaining good coverage.},
  archivePrefix = {arXiv},
  eprint = {1503.00256},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8JJCKZ3V\\Fuglstad et al. - 2017 - Constructing Priors that Penalize the Complexity o.pdf},
  journal = {arXiv:1503.00256 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{fuglstadConstructingPriorsThat2019,
  title = {Constructing {{Priors}} That {{Penalize}} the {{Complexity}} of {{Gaussian Random Fields}}},
  author = {Fuglstad, Geir-Arne and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2019},
  month = jan,
  volume = {114},
  pages = {445--452},
  issn = {0162-1459},
  doi = {10.1080/01621459.2017.1415907},
  abstract = {Priors are important for achieving proper posteriors with physically meaningful covariance structures for Gaussian random fields (GRFs) since the likelihood typically only provides limited information about the covariance structure under in-fill asymptotics. We extend the recent penalized complexity prior framework and develop a principled joint prior for the range and the marginal variance of one-dimensional, two-dimensional, and three-dimensional Mat\'ern GRFs with fixed smoothness. The prior is weakly informative and penalizes complexity by shrinking the range toward infinity and the marginal variance toward zero. We propose guidelines for selecting the hyperparameters, and a simulation study shows that the new prior provides a principled alternative to reference priors that can leverage prior knowledge to achieve shorter credible intervals while maintaining good coverage.We extend the prior to a nonstationary GRF parameterized through local ranges and marginal standard deviations, and introduce a scheme for selecting the hyperparameters based on the coverage of the parameters when fitting simulated stationary data. The approach is applied to a dataset of annual precipitation in southern Norway and the scheme for selecting the hyperparameters leads to conservative estimates of nonstationarity and improved predictive performance over the stationary model. Supplementary materials for this article are available online.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GYMJ3SAW\\Fuglstad et al. - 2019 - Constructing Priors that Penalize the Complexity o.pdf;C\:\\Users\\devan\\Zotero\\storage\\K98NEPW3\\01621459.2017.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Bayesian,Nonstationary,Penalized complexity,Priors,Range,Spatial models},
  number = {525}
}

@article{gabrielEstimatingSecondOrderCharacteristics2014,
  title = {Estimating {{Second}}-{{Order Characteristics}} of {{Inhomogeneous Spatio}}-{{Temporal Point Processes}}: {{Influence}} of {{Edge Correction Methods}} and {{Intensity Estimates}}},
  shorttitle = {Estimating {{Second}}-{{Order Characteristics}} of {{Inhomogeneous Spatio}}-{{Temporal Point Processes}}},
  author = {Gabriel, Edith},
  year = {2014},
  month = jun,
  volume = {16},
  pages = {411--431},
  issn = {1387-5841, 1573-7713},
  doi = {10.1007/s11009-013-9358-3},
  abstract = {Non-parametric estimates of the K-function and the pair correlation function play a fundamental role for exploratory and explanatory analysis of spatial and spatio-temporal point patterns. These estimates usually require information from outside of the study region, resulting to the so-called edge effects which have to be corrected. They also depend on first-order characteristics, which have to be estimated in practice. In this paper, we extend classical edge correction methods to the spatio-temporal setting and compare the performance of the related estimators for stationary/non-stationary and/or isotropic/anisotropic point patterns. Further, we explore the influence of the estimated intensity function on these estimators.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LKDYH38K\\Gabriel - 2014 - Estimating Second-Order Characteristics of Inhomog.pdf},
  journal = {Methodology and Computing in Applied Probability},
  language = {en},
  number = {2}
}

@article{gabrielStppPackagePlotting2013,
  title = {Stpp: {{An R Package}} for {{Plotting}}, {{Simulating}} and {{Analyzing Spatio}}-{{Temporal Point Patterns}}},
  shorttitle = {{\textbf{Stpp}}},
  author = {Gabriel, Edith and Rowlingson, Barry and Diggle, Peter},
  year = {2013},
  volume = {53},
  issn = {1548-7660},
  doi = {10.18637/jss.v053.i02},
  file = {C\:\\Users\\devan\\Zotero\\storage\\E3UFG3F4\\Gabriel et al. - 2013 - bstppb  An iRi Package for Plotting, Si.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {2}
}

@article{gaoSharedRandomEffect2004,
  title = {A Shared Random Effect Parameter Approach for Longitudinal Dementia Data with Non-Ignorable Missing Data},
  author = {Gao, Sujuan},
  year = {2004},
  month = jan,
  volume = {23},
  pages = {211--219},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.1710},
  abstract = {A signi\"ycant source of missing data in longitudinal epidemiologic studies on elderly individuals is death. It is generally believed that these missing data by death are non-ignorable to likelihood based inference. Inference based on data only from surviving participants in the study may lead to biased results. In this paper we model both the probability of disease and the probability of death using shared random e ect parameters. We also propose to use the Laplace approximation for obtaining an approximate likelihood function so that high dimensional integration over the distributions of the random e ect parameters is not necessary. Parameter estimates can be obtained by maximizing the approximate log-likelihood function. Data from a longitudinal dementia study will be used to illustrate the approach. A small simulation is conducted to compare parameter estimates from the proposed method to the `naive' method where missing data is considered at random. Copyright ? 2004 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3IE8ITS7\\Gao - 2004 - A shared random effect parameter approach for long.pdf;C\:\\Users\\devan\\Zotero\\storage\\V3AVT7W9\\Gao - 2004 - A shared random effect parameter approach for long.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {2}
}

@article{gatrellSpatialPointPattern1996,
  title = {Spatial {{Point Pattern Analysis}} and {{Its Application}} in {{Geographical Epidemiology}}},
  author = {Gatrell, Anthony C. and Bailey, Trevor C. and Diggle, Peter J. and Rowlingson, Barry S.},
  year = {1996},
  volume = {21},
  pages = {256},
  issn = {00202754},
  doi = {10.2307/622936},
  file = {C\:\\Users\\devan\\Zotero\\storage\\APZWC3EM\\Gatrell et al. - 1996 - Spatial Point Pattern Analysis and Its Application.pdf;C\:\\Users\\devan\\Zotero\\storage\\MYZ2H8K6\\Gatrell et al. - 1996 - Spatial Point Pattern Analysis and Its Application.pdf},
  journal = {Transactions of the Institute of British Geographers},
  language = {en},
  number = {1}
}

@article{gaujouxFlexibleSoftwarePackage2010,
  title = {A Flexible Software Package for Nonnegative Matrix Factorization},
  author = {Gaujoux, Renaud and Seoighe, Cathal},
  year = {2010},
  pages = {9},
  abstract = {Background: Nonnegative Matrix Factorization (NMF) is an unsupervised learning technique that has been applied successfully in several fields, including signal processing, face recognition and text mining. Recent applications of NMF in bioinformatics have demonstrated its ability to extract meaningful information from high-dimensional data such as gene expression microarrays. Developments in NMF theory and applications have resulted in a variety of algorithms and methods. However, most NMF implementations have been on commercial platforms, while those that are freely available typically require programming skills. This limits their use by the wider research community. Results: Our objective is to provide the bioinformatics community with an open-source, easy-to-use and unified interface to standard NMF algorithms, as well as with a simple framework to help implement and test new NMF methods. For that purpose, we have developed a package for the R/BioConductor platform. The package ports public code to R, and is structured to enable users to easily modify and/or add algorithms. It includes a number of published NMF algorithms and initialization methods and facilitates the combination of these to produce new NMF strategies. Commonly used benchmark data and visualization methods are provided to help in the comparison and interpretation of the results. Conclusions: The NMF package helps realize the potential of Nonnegative Matrix Factorization, especially in bioinformatics, providing easy access to methods that have already yielded new insights in many applications. Documentation, source code and sample data are available from CRAN.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IJNH24GF\\Gaujoux and Seoighe - 2010 - SAoftfwlaerexible R package for nonnegative matrix.pdf},
  language = {en}
}

@article{gavrikovUseMarkedPoint1995,
  title = {The Use of Marked Point Processes in Ecological and Environmental Forest Studies},
  author = {Gavrikov, Vladimir and Stoyan, Dietrich},
  year = {1995},
  month = dec,
  volume = {2},
  pages = {331--344},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/BF00569362},
  abstract = {The paper presents new applications of marked point processes in forestry. The tools are pair correlation functions and mark connection functions. Studies of the structures of even-aged pine forests in Siberia show a tendency of development from clustering in young stands towards random distribution in elder stands for the trunk positions. The tree tops as the movable parts of trees are still more regular in old forests. Furthermore, in Siberian fir forests interesting tendencies of attraction between members of several tree classes are detected. Finally, studies of the distribution of damaged trees in some central European forests show that there the damaged trees appear as singletons; dominating trees are preferentially damaged.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SYREQNGK\\Gavrikov and Stoyan - 1995 - The use of marked point processes in ecological an.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {4}
}

@book{gelfandHandbookEnvironmentalEcological2019,
  title = {Handbook of {{Environmental}} and {{Ecological Statistics}}},
  author = {Gelfand, Alan E. and Fuentes, Montserrat and Hoeting, Jennifer A. and Smith, Richard Lyttleton},
  year = {2019},
  month = jan,
  publisher = {{CRC Press}},
  abstract = {This handbook focuses on the enormous literature applying statistical methodology and modelling to environmental and ecological processes. The 21st century statistics community has become increasingly interdisciplinary, bringing a large collection of modern tools to all areas of application in environmental processes. In addition, the environmental community has substantially increased its scope of data collection including observational data, satellite-derived data, and computer model output. The resultant impact in this latter community has been substantial; no longer are simple regression and analysis of variance methods adequate. The contribution of this handbook is to assemble a state-of-the-art view of this interface.   Features:   An internationally regarded editorial team.   A distinguished collection of contributors.   A thoroughly contemporary treatment of a substantial interdisciplinary interface.   Written to engage both statisticians as well as quantitative environmental researchers.   34 chapters covering methodology, ecological processes, environmental exposure, and statistical methods in climate science.},
  googlebooks = {JzqDDwAAQBAJ},
  isbn = {978-1-4987-5212-1},
  keywords = {Mathematics / Probability \& Statistics / General,Nature / Ecology,Science / Environmental Science},
  language = {en}
}

@article{gelfandProperMultivariateConditional2003,
  title = {Proper Multivariate Conditional Autoregressive Models for Spatial Data Analysis},
  author = {Gelfand, A. E.},
  year = {2003},
  month = jan,
  volume = {4},
  pages = {11--15},
  issn = {14654644, 14684357},
  doi = {10.1093/biostatistics/4.1.11},
  abstract = {In the past decade conditional autoregressive modelling specifications have found considerable application for the analysis of spatial data. Nearly all of this work is done in the univariate case and employs an improper specification. Our contribution here is to move to multivariate conditional autoregressive models and to provide rich, flexible classes which yield proper distributions. Our approach is to introduce spatial autoregression parameters. We first clarify what classes can be developed from the family of Mardia (1988) and contrast with recent work of Kim et al. (2000). We then present a novel parametric linear transformation which provides an extension with attractive interpretation. We propose to employ these models as specifications for second-stage spatial effects in hierarchical models. Two applications are discussed; one for the two-dimensional case modelling spatial patterns of child growth, the other for a four-dimensional situation modelling spatial variation in HLA-B allele frequencies. In each case, full Bayesian inference is carried out using Markov chain Monte Carlo simulation.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SGKUJZ57\\Gelfand - 2003 - Proper multivariate conditional autoregressive mod.pdf},
  journal = {Biostatistics},
  language = {en},
  number = {1}
}

@book{gelmanBayesianDataAnalysis2013,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2013},
  edition = {Third},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian Analysis Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data A},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MSUYCR3A\\9781439840955.html},
  language = {en},
  series = {Chapman \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}}}
}

@book{gelmanDataAnalysisUsing2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  publisher = {{Cambridge University Press}},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/\textasciitilde gelman/arm/},
  googlebooks = {lV3DIdV0F9AC},
  isbn = {978-0-521-68689-1},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Regression Analysis,Political Science / General,Psychology / Assessment; Testing \& Measurement,Social Science / Research},
  language = {en}
}

@article{gelmanMultilevelHierarchicalModeling2006,
  title = {Multilevel ({{Hierarchical}}) {{Modeling}}: {{What It Can}} and {{Cannot Do}}},
  shorttitle = {Multilevel ({{Hierarchical}}) {{Modeling}}},
  author = {Gelman, Andrew},
  year = {2006},
  month = aug,
  volume = {48},
  pages = {432--435},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017005000000661},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BVQEPEXC\\Gelman - 2006 - Multilevel (Hierarchical) Modeling What It Can an.pdf;C\:\\Users\\devan\\Zotero\\storage\\GC5PCGFV\\Gelman - 2006 - Multilevel (Hierarchical) Modeling What It Can an.pdf},
  journal = {Technometrics},
  language = {en},
  number = {3}
}

@article{gelmanPriorDistributionsVariance,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models},
  author = {Gelman, Andrew},
  pages = {19},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of ``noninformative'' prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\99YDF7DE\\Gelman - Prior distributions for variance parameters in hie.pdf},
  language = {en}
}

@article{gelmanPriorDistributionsVariance2006,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by {{Browne}} and {{Draper}})},
  author = {Gelman, Andrew},
  year = {2006},
  month = sep,
  volume = {1},
  pages = {515--534},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA117A},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-ttt family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-ttt family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-ttt family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\299VYIPN\\Gelman - 2006 - Prior distributions for variance parameters in hie.pdf;C\:\\Users\\devan\\Zotero\\storage\\VDEHIV2A\\1340371048.html},
  journal = {Bayesian Analysis},
  keywords = {Bayesian inference,conditional conjugacy,folded-noncentral-$t$ distribution,half-$t$ distribution,hierarchical model,multilevel model,noninformative prior distribution,weakly informative prior distribution},
  language = {EN},
  mrnumber = {MR2221284},
  number = {3},
  zmnumber = {1331.62139}
}

@article{gelmanUnderstandingPredictiveInformation2014,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  year = {2014},
  month = nov,
  volume = {24},
  pages = {997--1016},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7VFAPDEN\\Gelman et al. - 2014 - Understanding predictive information criteria for .pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {6}
}

@article{gelmanUnderstandingPredictiveInformation2014a,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  year = {2014},
  month = nov,
  volume = {24},
  pages = {997--1016},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5WAQU66V\\Gelman et al. - 2014 - Understanding predictive information criteria for .pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {6}
}

@article{geyerSimulationProceduresLikelihood1994,
  title = {Simulation {{Procedures}} and {{Likelihood Inference}} for {{Spatial Point Processes}}},
  author = {Geyer, Charles J. and M{\o}ller, Jesper},
  year = {1994},
  volume = {21},
  pages = {359--373},
  abstract = {An alternative algorithm to the usual birth-and-death procedure for simulating spatial point processes is introduced. The algorithm is used in a discussion of unconditional versus conditional likelihood inference for parametric models of spatial point processes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\352W9MKZ\\Geyer and Møller - 1994 - Simulation Procedures and Likelihood Inference for.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {4}
}

@article{geysPseudolikelihoodModelingMultivariate1999,
  title = {Pseudolikelihood {{Modeling}} of {{Multivariate Outcomes}} in {{Developmental Toxicology}}},
  author = {Geys, Helena and Molenberghs, Geert and Ryan, Louise M.},
  year = {1999},
  month = sep,
  volume = {94},
  pages = {734--745},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1999.10474176},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GMXWM7CG\\Geys et al. - 1999 - Pseudolikelihood Modeling of Multivariate Outcomes.pdf;C\:\\Users\\devan\\Zotero\\storage\\MEBYGZSP\\Geys et al. - 1999 - Pseudolikelihood Modeling of Multivariate Outcomes.pdf;C\:\\Users\\devan\\Zotero\\storage\\N2YZAZRF\\Geys et al. - 1999 - Pseudolikelihood Modeling of Multivariate Outcomes.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {447}
}

@article{girvanCommunityStructureSocial2002,
  title = {Community Structure in Social and Biological Networks},
  author = {Girvan, M. and Newman, M. E. J.},
  year = {2002},
  month = jun,
  volume = {99},
  pages = {7821--7826},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.122653799},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CHW3CIPA\\Girvan and Newman - 2002 - Community structure in social and biological netwo.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {12}
}

@book{gomez-rubioAdvancedSpatialModeling2019,
  title = {Advanced {{Spatial Modeling}} with {{Stochastic Partial Differential Equations Using R}} and {{INLA}}},
  author = {{G{\'o}mez-Rubio}, Virgilio and Bakka, Haakon and Lenzi, Amanda and {Castro-Camilo}, Daniela and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2019},
  publisher = {{CRC Press}},
  abstract = {Modeling spatial and spatio-temporal continuous processes is an important and challenging problem in spatial statistics. Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA describes in detail the stochastic partial differential equations (SPDE) approach for mod},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TLYX8V2X\\9781138369856.html},
  language = {en}
}

@article{gomezTutorialMethodsIntervalcensored2009,
  title = {Tutorial on Methods for Interval-Censored Data and Their Implementation in {{R}}},
  author = {G{\'o}mez, Guadalupe and Calle, M Luz and Oller, Ramon and Langohr, Klaus},
  editor = {Melis, Guadalupe G{\'o}mez},
  year = {2009},
  month = dec,
  volume = {9},
  pages = {259--297},
  issn = {1471-082X, 1477-0342},
  doi = {10.1177/1471082X0900900402},
  abstract = {Interval censoring is encountered in many practical situations when the event of interest cannot be observed and it is only known to have occurred within a time window. The theory for the analysis of interval-censored data has been developed over the past three decades and several reviews have been written. However, it is still a common practice in medical and reliability studies to simplify the interval censoring structure of the data into a more standard right censoring situation by, for instance, imputing the midpoint of the censoring interval. The availability of software for right censoring might well be the main reason for this simplifying practice. In contrast, several methods have been developed to deal with interval-censored data and the corresponding algorithms to make the procedures feasible are scattered across the statistical software or remain behind the personal computers of many researchers. The purpose of this tutorial is to present, in a pedagogical and unified manner, the methodology and the available software for analyzing interval-censored data. The paper covers frequentist non-parametric, parametric and semiparametric estimating approaches, non-parametric tests for comparing survival curves and a section on simulation of interval-censored data. The methods and the software are described using the data from a dental study.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\P5SJ9RIA\\Gómez et al. - 2009 - Tutorial on methods for interval-censored data and.pdf},
  journal = {Statistical Modelling: An International Journal},
  language = {en},
  number = {4}
}

@article{gonzalezSpatiotemporalPointProcess2016,
  title = {Spatio-Temporal Point Process Statistics: {{A}} Review},
  shorttitle = {Spatio-Temporal Point Process Statistics},
  author = {Gonz{\'a}lez, Jonatan A. and {Rodr{\'i}guez-Cort{\'e}s}, Francisco J. and Cronie, Ottmar and Mateu, Jorge},
  year = {2016},
  month = nov,
  volume = {18},
  pages = {505--544},
  issn = {22116753},
  doi = {10.1016/j.spasta.2016.10.002},
  abstract = {Spatio-temporal point process data have been analysed quite a bit in specialised fields, with the aim of better understanding the inherent mechanisms that govern the temporal evolution of events placed in a planar region. In particular, in the last decade there has been an acceleration of methodological developments, accompanied by a broad collection of applications as spatiotemporally indexed data have become more widely available in many scientific fields.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JLHFQSYZ\\González et al. - 2016 - Spatio-temporal point process statistics A review.pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@article{Goode2003,
  title = {Are Pilots at Risk of Accidents Due to Fatigue?},
  author = {Goode, Jeffrey H.},
  year = {2003},
  volume = {34},
  pages = {309--313},
  issn = {00224375},
  doi = {10.1016/S0022-4375(03)00033-1},
  abstract = {Problem: There is concern in the aviation community that pilot schedules can lead to fatigue and increased chance of an aviation accident. Yet despite this concern, there is little empirical analysis showing the relationship between pilot schedules and commercial aviation accidents. This study attempts to demonstrate an empirical relationship between pilot schedules and aviation accidents. Method: Data for human factors-related accidents and pilot work patterns were identified. The distribution of pilot work schedule parameters for the accidents was compared to that for all pilots using a chi-square test to determine if the proportions of accidents and length of duty exposure were the same. If the distributions are the same, then one could infer that pilot human factor accidents are not affected by work schedule parameters. Results: The proportion of accidents associated with pilots having longer duty periods is higher than the proportion of longer duty periods for all pilots. Discussion: There is a discernible pattern of increased probability of an accident as duty time increases for commercial aircraft pilots in the United States. Impact on Industry: The analysis suggests that establishing limits on duty time for commercial pilots would reduce risk. Such a rule is likely to be expensive and could substantially impact the commercial airlines. In return, there is likely to be a reduction in the risk of commercial aviation accidents due to pilot fatigue. \textcopyright{} 2003 National Safety Council and Elsevier Science Ltd. All rights reserved.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3JS4I67L\\Goode - 2003 - Are pilots at risk of accidents due to fatigue.pdf},
  isbn = {0022-4375},
  journal = {Journal of Safety Research},
  keywords = {Aviation accidents,FAA rulemaking,Fatigue,Pilot flight and duty,Risk analysis},
  number = {3},
  pmid = {12963077}
}

@article{goodeArePilotsRisk2003,
  title = {Are Pilots at Risk of Accidents Due to Fatigue?},
  author = {Goode, Jeffrey H.},
  year = {2003},
  month = aug,
  volume = {34},
  pages = {309--313},
  issn = {00224375},
  doi = {10.1016/S0022-4375(03)00033-1},
  abstract = {Problem: There is concern in the aviation community that pilot schedules can lead to fatigue and increased chance of an aviation accident. Yet despite this concern, there is little empirical analysis showing the relationship between pilot schedules and commercial aviation accidents. This study attempts to demonstrate an empirical relationship between pilot schedules and aviation accidents. Method: Data for human factorsrelated accidents and pilot work patterns were identified. The distribution of pilot work schedule parameters for the accidents was compared to that for all pilots using a chi-square test to determine if the proportions of accidents and length of duty exposure were the same. If the distributions are the same, then one could infer that pilot human factor accidents are not affected by work schedule parameters. Results: The proportion of accidents associated with pilots having longer duty periods is higher than the proportion of longer duty periods for all pilots. Discussion: There is a discernible pattern of increased probability of an accident as duty time increases for commercial aircraft pilots in the United States. Impact on Industry: The analysis suggests that establishing limits on duty time for commercial pilots would reduce risk. Such a rule is likely to be expensive and could substantially impact the commercial airlines. In return, there is likely to be a reduction in the risk of commercial aviation accidents due to pilot fatigue.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4AMRYX5Y\\Goode - 2003 - Are pilots at risk of accidents due to fatigue.pdf;C\:\\Users\\devan\\Zotero\\storage\\EQH265XI\\Goode - 2003 - Are pilots at risk of accidents due to fatigue.pdf},
  journal = {Journal of Safety Research},
  language = {en},
  number = {3}
}

@misc{goreaudAvoidingMisinterpretationBiotic,
  title = {Avoiding Misinterpretation of Biotic Interactions with the Intertype {{K}} 12 - Function: Population Independence vs. Random Labelling Hypotheses},
  author = {Goreaud, F. and P{\'e}lissier, R.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XZ4FJCBC\\1100-9233(2003)014[0681_AMOBIW]2.0.pdf}
}

@article{gregoryPilotFatigueSurvey2010,
  title = {Pilot {{Fatigue Survey}}: {{Exploring Fatigue Factors}} in {{Air Medical Operations}}},
  shorttitle = {Pilot {{Fatigue Survey}}},
  author = {Gregory, Kevin B. and Winn, William and Johnson, Kent and Rosekind, Mark R.},
  year = {2010},
  month = nov,
  volume = {29},
  pages = {309--319},
  issn = {1067991X},
  doi = {10.1016/j.amj.2010.07.002},
  abstract = {Introduction: Humans confront significant physiological challenges with sleep and alertness when working in 24/7 operations. Methods: A web-based national survey of air medical pilots examined issues relevant to fatigue and sleep management. Results: Six hundred ninety-seven responses were received, with a majority of rotor wing pilots working 3/3/7 and 7/7 duty schedules. Over 84\% of the pilots reported that fatigue had affected their flight performance; less than 28\% reported ``nodding off'' during flight. More than 90\% reported a separate work site ``rest'' room with a bed available. Over 90\% reported no company policies restricting on-duty sleep. Approximately half of the pilots reported getting 4 hours or more sleep during a typical night shift. Approximately half reported that sleep inertia had never compromised flight safety. Over 90\% reported that it was better to sleep during the night and overcome sleep inertia if necessary.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6TRG97EC\\Gregory et al. - 2010 - Pilot Fatigue Survey Exploring Fatigue Factors in.pdf;C\:\\Users\\devan\\Zotero\\storage\\MFJ3TBSV\\Gregory et al. - 2010 - Pilot Fatigue Survey Exploring Fatigue Factors in.pdf},
  journal = {Air Medical Journal},
  language = {en},
  number = {6}
}

@article{grevenRestrictedLikelihoodRatio2008,
  title = {Restricted {{Likelihood Ratio Testing}} for {{Zero Variance Components}} in {{Linear Mixed Models}}},
  author = {Greven, Sonja and Crainiceanu, Ciprian M. and K{\"u}chenhoff, Helmut and Peters, Annette},
  year = {2008},
  volume = {17},
  pages = {870--891},
  issn = {1061-8600},
  abstract = {The goal of our article is to provide a transparent, robust, and computationally feasible statistical platform for restricted likelihood ratio testing (RLRT) for zero variance components in linear mixed models. This problem is nonstandard because under the null hypothesis the parameter is on the boundary of the parameter space. Our proposed approach is different from the asymptotic results of Stram and Lee who assumed that the outcome vector can be partitioned into many independent subvectors. Thus, our methodology applies to a wider class of mixed models, which includes models with a moderate number of clusters or nonparametric smoothing components. We propose two approximations to the finite sample null distribution of the RLRT statistic. Both approximations converge weakly to the asymptotic distribution obtained by Stram and Lee when their assumptions hold. When their assumptions do not hold, we show in extensive simulation studies that both approximations outperform the Stram and Lee approximation and the parametric bootstrap. We also identify and address numerical problems associated with standard mixed model software. Our methods are motivated by and applied to a large longitudinal study on air pollution health effects in a highly susceptible cohort. Relevant software is posted as an online supplement.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UTBFIADH\\Greven et al. - 2008 - Restricted Likelihood Ratio Testing for Zero Varia.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  number = {4}
}

@article{gronauTutorialBridgeSampling2017,
  title = {A {{Tutorial}} on {{Bridge Sampling}}},
  author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
  year = {2017},
  month = mar,
  abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng \& Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model\textemdash a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
  archivePrefix = {arXiv},
  eprint = {1703.05984},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IIXYJAIF\\Gronau et al. - 2017 - A Tutorial on Bridge Sampling.pdf},
  journal = {arXiv:1703.05984 [stat]},
  keywords = {Statistics - Computation},
  language = {en},
  primaryClass = {stat}
}

@techreport{grossKernelDensityEstimation2015,
  title = {Kernel Density Estimation for Heaped Data},
  author = {Gro{\ss}, Marcus and Rendtel, Ulrich},
  year = {2015},
  institution = {{Free University Berlin, School of Business \& Economics}},
  abstract = {In self-reported data usually a phenomenon called 'heaping' occurs, i.e. survey participants round the values of their income, weight or height to some degree. Additionally, respondents may be more prone to round off or up due to social desirability. By ignoring the heaping process a severe bias in terms of spikes and bumps is introduced when applying kernel density methods naively to the rounded data. A generalized Stochastic Expectation Maximization (SEM) approach accounting for heaping with potentially asymmetric rounding behaviour in univariate kernel density estimation is presented in this work. The introduced methods are applied to survey data of the German Socio-Economic Panel and exhibit very good performance simulations.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SFXBZN8B\\Groß and Rendtel - 2015 - Kernel density estimation for heaped data.pdf;C\:\\Users\\devan\\Zotero\\storage\\NM9EHA5N\\201527.html},
  keywords = {Heaping,Kernel density estimation,Measurement error,Rounded data,Self-reported data,Survey Data},
  language = {en},
  number = {2015/27}
}

@article{grossKernelDensityEstimation2016,
  title = {Kernel {{Density Estimation}} for {{Heaped Data}}},
  author = {Gro{\ss}, Marcus and Rendtel, Ulrich},
  year = {2016},
  month = sep,
  volume = {4},
  pages = {339--361},
  issn = {2325-0984, 2325-0992},
  doi = {10.1093/jssam/smw011},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SAZ3EUFY\\Groß and Rendtel - 2016 - Kernel Density Estimation for Heaped Data.pdf;C\:\\Users\\devan\\Zotero\\storage\\WVP3UXLY\\Groß and Rendtel - 2016 - Kernel Density Estimation for Heaped Data.pdf},
  journal = {Journal of Survey Statistics and Methodology},
  language = {en},
  number = {3}
}

@article{guanTestIndependenceMarks2007,
  title = {Test for Independence between Marks and Points of Marked Point Processes: A Subsampling Approach},
  shorttitle = {Test for Independence between Marks and Points of Marked Point Processes},
  author = {Guan, Yongtao and Afshartous, David R.},
  year = {2007},
  month = may,
  volume = {14},
  pages = {101--111},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-007-0010-7},
  abstract = {Ecological data often involve measurements taken at irregularly spaced locations (e.g., the heights of trees in a forest). A useful approach for modeling such data is via a marked point process, where the marks (i.e., measurements) and points (i.e., locations) are often assumed to be independent. Although this is a convenient assumption, it may not hold in practice. Schlather et al. (Journal of the Royal Statistical Society Services B, 66, 79\textendash 93, 2004) proposed a simulation-based approach to test this assumption. This paper presents a new method for testing the assumption of independence between the marks and the points. Instead of considering a simulation approach, we derive analytical results that allow the test to be implemented via a conventional {$\chi$}2 statistic. We illustrate the use of our approach by applying it to an example involving desert plant data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CAE5X38P\\Guan and Afshartous - 2007 - Test for independence between marks and points of .pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {2}
}

@article{guanTestsIndependenceMarks2006,
  title = {Tests for {{Independence}} between {{Marks}} and {{Points}} of a {{Marked Point Process}}},
  author = {Guan, Yongtao},
  year = {2006},
  volume = {62},
  pages = {126--134},
  abstract = {A convenient assumption while modeling a marked point process is that the observations (i.e., marks) and the locations (i.e., points) are independent. We propose new graphical and formal testing approaches to test for this assumption. The proposed graphical procedures are easy to obtain and can be used to diagnose the nature and range of dependence between marks and points. The formal testing procedures require only minimal conditions on marks and thus can be applied to a variety of settings. We illustrate these procedures through a simulation study and an application to some real data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZJIPR6Z5\\Guan - 2006 - Tests for Independence between Marks and Points of.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{guanTestsIndependenceMarks2006a,
  title = {Tests for {{Independence}} between {{Marks}} and {{Points}} of a {{Marked Point Process}}},
  author = {Guan, Yongtao},
  year = {2006},
  month = mar,
  volume = {62},
  pages = {126--134},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2005.00395.x},
  abstract = {A convenient assumption while modeling a marked point process is that the observations (i.e., marks) and the locations (i.e., points) are independent. We propose new graphical and formal testing approaches to test for this assumption. The proposed graphical procedures are easy to obtain and can be used to diagnose the nature and range of dependence between marks and points. The formal testing procedures require only minimal conditions on marks and thus can be applied to a variety of settings. We illustrate these procedures through a simulation study and an application to some real data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D362GQBI\\Guan - 2006 - Tests for Independence between Marks and Points of.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{gueorguievaCorrelatedProbitModel2001,
  title = {A {{Correlated Probit Model}} for {{Joint Modeling}} of {{Clustered Binary}} and {{Continuous Responses}}},
  author = {Gueorguieva, Ralitza V and Agresti, Alan},
  year = {2001},
  month = sep,
  volume = {96},
  pages = {1102--1112},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214501753208762},
  file = {C\:\\Users\\devan\\Zotero\\storage\\M4C3CMHV\\Gueorguieva and Agresti - 2001 - A Correlated Probit Model for Joint Modeling of Cl.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {455}
}

@article{gugushviliNonparametricBayesianInference2015,
  title = {Nonparametric {{Bayesian}} Inference for Multidimensional Compound {{Poisson}} Processes},
  author = {Gugushvili, Shota and {van der Meulen}, Frank and Spreij, Peter},
  year = {2015},
  month = mar,
  volume = {2},
  pages = {1--15},
  issn = {2351-6054, 2351-6046},
  doi = {10.15559/15-VMSTA20},
  abstract = {Given a sample from a discretely observed multidimensional compound Poisson process, we study the problem of nonparametric estimation of its jump size density r0 and intensity {$\lambda$}0. We take a nonparametric Bayesian approach to the problem and determine posterior contraction rates in this context, which, under some assumptions, we argue to be optimal posterior contraction rates. In particular, our results imply the existence of Bayesian point estimates that converge to the true parameter pair (r0, {$\lambda$}0) at these rates. To the best of our knowledge, construction of nonparametric density estimators for inference in the class of discretely observed multidimensional L\'evy processes, and the study of their rates of convergence is a new contribution to the literature.},
  archivePrefix = {arXiv},
  eprint = {1412.7739},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F4HAAZLR\\Gugushvili et al. - 2015 - Nonparametric Bayesian inference for multidimensio.pdf},
  journal = {Modern Stochastics: Theory and Applications},
  keywords = {Mathematics - Statistics Theory},
  language = {en},
  number = {1}
}

@article{guttorpNonparametricEstimationIntensities1990,
  title = {Nonparametric {{Estimation}} of {{Intensities}} for {{Sampled Counting Processes}}},
  author = {Guttorp, Peter and Thompson, Mary Lou},
  year = {1990},
  volume = {52},
  pages = {157--173},
  abstract = {Sampled counting processes are often studied using methods for the analysis of stationary time series. We express some time series parameters as smoothed versions of corresponding point process parameters and use these relations to suggest estimates of the point process parameters. In addition, we propose a reconstructive approach to the estimation problem. We derive some properties of the estimators and apply them to data from meteorology, entomology and particle physics. A simulation study indicates which estimators perform better in different situations.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9XBQYBG9\\Guttorp and Thompson - 1990 - Nonparametric Estimation of Intensities for Sample.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {1}
}

@article{handPatternDiscoveryDetection2004,
  title = {Pattern {{Discovery}} and {{Detection}}: {{A Unified Statistical Methodology}}},
  shorttitle = {Pattern {{Discovery}} and {{Detection}}},
  author = {Hand, David J. and Bolton, Richard J.},
  year = {2004},
  month = oct,
  volume = {31},
  pages = {885--924},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/0266476042000270518},
  file = {C\:\\Users\\devan\\Zotero\\storage\\M6NQIL3Z\\Hand and Bolton - 2004 - Pattern Discovery and Detection A Unified Statist.pdf;C\:\\Users\\devan\\Zotero\\storage\\MDAU9K8A\\Hand and Bolton - 2004 - Pattern Discovery and Detection A Unified Statist.pdf},
  journal = {Journal of Applied Statistics},
  language = {en},
  number = {8}
}

@article{hanesFireregimeChangesCanada2019,
  title = {Fire-Regime Changes in {{Canada}} over the Last Half Century},
  author = {Hanes, C. C. and Wang, X. and Jain, P. and Parisien, M.-A. and Little, J. M. and Flannigan, M. D.},
  year = {2019},
  volume = {49},
  pages = {256--269},
  doi = {10.1139/cjfr-2018-0293},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\I7PB9CMP\\publications.html},
  language = {English}
}

@phdthesis{harrisHeapedDataCount,
  title = {Heaped {{Data In Count Models}}},
  author = {Harris, Tammy},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WY9IRR95\\Harris - Heaped Data In Count Models.pdf},
  language = {en}
}

@article{hartePtProcessPackageModelling2010,
  title = {{{PtProcess}}: {{An R Package}} for {{Modelling Marked Point Processes Indexed}} by {{Time}}},
  shorttitle = {{{{\textbf{PtProcess}}}}},
  author = {Harte, David},
  year = {2010},
  volume = {35},
  issn = {1548-7660},
  doi = {10.18637/jss.v035.i08},
  abstract = {This paper describes the package PtProcess which uses the R statistical language. The package provides a unified approach to fitting and simulating a wide variety of temporal point process or temporal marked point process models. The models are specified by an intensity function which is conditional on the history of the process. The user needs to provide routines for calculating the conditional intensity function. Then the package enables one to carry out maximum likelihood fitting, goodness of fit testing, simulation and comparison of models. The package includes the routines for the conditional intensity functions for a variety of standard point process models. The package is intended to simplify the fitting of point process models indexed by time in much the same way as generalized linear model programs have simplified the fitting of various linear models. The primary examples used in this paper are earthquake sequences but the package is intended to have a much wider applicability.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X4LJPAPC\\Harte - 2010 - bPtProcessb  An iRi Package for Modelli.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {8}
}

@book{hastieElementsStatisticalLearning2009,
  title = {{The elements of statistical learning: data mining, inference and prediction}},
  shorttitle = {{The elements of statistical learning}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  publisher = {{Springer}},
  abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates."--Publisher's website.},
  annotation = {OCLC: 1058138445},
  isbn = {9780387848570 9780387848587 9781282827264 9781282126749 9786612126741},
  language = {English.},
  series = {{Springer Series in Statistics}}
}

@misc{hawerchuckBehindthenetBlog2007082007,
  title = {Behindthenet {{Blog}}: 2007-08 5v5 {{Goaltender Performance}}},
  shorttitle = {Behindthenet {{Blog}}},
  author = {{Hawerchuck}},
  year = {2007},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PPPRV76U\\2007-08-5v5-goaltender-performance.html}
}

@misc{hawerchuckShotLocationShot2010,
  title = {Shot {{Location}} by {{Shot Type}}},
  author = {Hawerchuck},
  year = {2010},
  month = apr,
  abstract = {Shot Location by Shot Type},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ARBVB8WR\\shot-location-by-shot-type.html},
  howpublished = {https://www.arcticicehockey.com/2010/4/26/1441542/shot-location-by-shot-type},
  journal = {Arctic Ice Hockey}
}

@article{healyNoteMultivariateCUSUM1987,
  title = {A {{Note}} on {{Multivariate CUSUM Procedures}}},
  author = {Healy, John D.},
  year = {1987},
  month = nov,
  volume = {29},
  pages = {409--412},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1987.10488268},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C9TVD7ZK\\Healy - 1987 - A Note on Multivariate CUSUM Procedures.pdf},
  journal = {Technometrics},
  language = {en},
  number = {4}
}

@article{hefleyDynamicSpatiotemporalModels2017,
  title = {Dynamic Spatio-Temporal Models for Spatial Data},
  author = {Hefley, Trevor J. and Hooten, Mevin B. and Hanks, Ephraim M. and Russell, Robin E. and Walsh, Daniel P.},
  year = {2017},
  month = may,
  volume = {20},
  pages = {206--220},
  issn = {22116753},
  doi = {10.1016/j.spasta.2017.02.005},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IFA3HQ3F\\Hefley et al. - 2017 - Dynamic spatio-temporal models for spatial data.pdf;C\:\\Users\\devan\\Zotero\\storage\\X4USZ38C\\Hefley et al. - 2017 - Dynamic spatio-temporal models for spatial data.pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@article{heitjanIgnorabilityCoarseData1991,
  title = {Ignorability and {{Coarse Data}}},
  author = {Heitjan, Daniel F. and Rubin, Donald B.},
  year = {1991},
  month = dec,
  volume = {19},
  pages = {2244--2253},
  issn = {0090-5364},
  doi = {10.1214/aos/1176348396},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6H4SJY2X\\Heitjan and Rubin - 1991 - Ignorability and Coarse Data.pdf;C\:\\Users\\devan\\Zotero\\storage\\VA3D459I\\Heitjan and Rubin - 1991 - Ignorability and Coarse Data.pdf},
  journal = {The Annals of Statistics},
  language = {en},
  number = {4}
}

@article{heitjanInferenceCoarseData,
  title = {Inference from {{Coarse Data Via Multiple Imputation}} with {{Application}} to {{Age Heaping}}},
  author = {Heitjan, Daniel F and Rubin, Donald B},
  pages = {12},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ITJBQTBQ\\Heitjan and Rubin - Inference from Coarse Data Via Multiple Imputation.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZPSN9TXF\\Heitjan and Rubin - Inference from Coarse Data Via Multiple Imputation.pdf},
  language = {en}
}

@article{heitjanInferenceGroupedContinuous1989,
  title = {Inference from {{Grouped Continuous Data}}: {{A Review}}},
  shorttitle = {Inference from {{Grouped Continuous Data}}},
  author = {Heitjan, Daniel F.},
  year = {1989},
  month = may,
  volume = {4},
  pages = {164--179},
  issn = {0883-4237},
  doi = {10.1214/ss/1177012601},
  abstract = {Grouped" data are defined to be the result of observing continuous variables only up to the nearest interval, rectangle or triangle. This paper traces the development of statistical methods for grouped data, focusing on the major results and their interpretations. It emphasizes the impact of likelihood and Bayesian ideas on the analysis of grouped data, particularly as they influence current work.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2DAD3QAL\\Heitjan - 1989 - Inference from Grouped Continuous Data A Review.pdf;C\:\\Users\\devan\\Zotero\\storage\\IEUGCTXV\\Heitjan - 1989 - Inference from Grouped Continuous Data A Review.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {2}
}

@article{heitjanInferenceGroupedContinuous1989a,
  title = {Inference from {{Grouped Continuous Data}}: {{A Review}}},
  shorttitle = {Inference from {{Grouped Continuous Data}}},
  author = {Heitjan, Daniel F.},
  year = {1989},
  month = may,
  volume = {4},
  pages = {164--179},
  issn = {0883-4237},
  doi = {10.1214/ss/1177012601},
  abstract = {Grouped" data are defined to be the result of observing continuous variables only up to the nearest interval, rectangle or triangle. This paper traces the development of statistical methods for grouped data, focusing on the major results and their interpretations. It emphasizes the impact of likelihood and Bayesian ideas on the analysis of grouped data, particularly as they influence current work.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XRHDM6ID\\Heitjan - 1989 - Inference from Grouped Continuous Data A Review.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {2}
}

@article{helselNondetectsDataAnalysis2005,
  title = {Nondetects and Data Analysis. {{Statistics}} for Censored Environmental Data.},
  author = {Helsel, D. R.},
  year = {2005},
  abstract = {This book provides solutions for the interpretation and analysis of data that fall below the laboratory detection limit. Survival analysis methods are adapted for studies of trace chemicals in air, water, soils, and biota. The 12 chapters are organised by objective, such as computing intervals, comparing groups, and correlations.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JFJMT9BH\\www.cabdirect.org.html},
  journal = {Nondetects and data analysis. Statistics for censored environmental data.},
  language = {English}
}

@article{hendersonJointModellingLongitudinal2000,
  title = {Joint Modelling of Longitudinal Measurements and Event Time Data},
  author = {Henderson, R.},
  year = {2000},
  month = dec,
  volume = {1},
  pages = {465--480},
  issn = {14654644, 14684357},
  doi = {10.1093/biostatistics/1.4.465},
  abstract = {This paper formulates a class of models for the joint behaviour of a sequence of longitudinal measurements and an associated sequence of event times, including single-event survival data. This class includes and extends a number of specific models which have been proposed recently, and, in the absence of association, reduces to separate models for the measurements and events based, respectively, on a normal linear model with correlated errors and a semi-parametric proportional hazards or intensity model with frailty.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HP2QLCTZ\\Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf;C\:\\Users\\devan\\Zotero\\storage\\L7FHZKIP\\Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf;C\:\\Users\\devan\\Zotero\\storage\\M7597H67\\Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf;C\:\\Users\\devan\\Zotero\\storage\\RXKZLNQB\\Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf;C\:\\Users\\devan\\Zotero\\storage\\WLL7CU2A\\Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf},
  journal = {Biostatistics},
  language = {en},
  number = {4}
}

@article{hendersonJointModellingLongitudinal2000a,
  title = {Joint Modelling of Longitudinal Measurements and Event Time Data},
  author = {Henderson, R. and Diggle, P. and Dobson, A.},
  year = {2000},
  month = dec,
  volume = {1},
  pages = {465--480},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/1.4.465},
  abstract = {This paper formulates a class of models for the joint behaviour of a sequence of longitudinal measurements and an associated sequence of event times, including single-event survival data. This class includes and extends a number of specific models which have been proposed recently, and, in the absence of association, reduces to separate models for the measurements and events based, respectively, on a normal linear model with correlated errors and a semi-parametric proportional hazards or intensity model with frailty. Special cases of the model class are discussed in detail and an estimation procedure which allows the two components to be linked through a latent stochastic process is described. Methods are illustrated using results from a clinical trial into the treatment of schizophrenia.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QXUUET2B\\Henderson et al. - 2000 - Joint modelling of longitudinal measurements and e.pdf},
  journal = {Biostatistics (Oxford, England)},
  language = {eng},
  number = {4},
  pmid = {12933568}
}

@article{henrysInferenceClusteredInhomogeneous2009,
  title = {Inference for {{Clustered Inhomogeneous Spatial Point Processes}}},
  author = {Henrys, P. A. and Brown, P. E.},
  year = {2009},
  volume = {65},
  pages = {423--430},
  abstract = {We propose a method to test for significant differences in the levels of clustering between two spatial point processes (cases and controls) while taking into account differences in their first-order intensities. The key advance on earlier methods is that the controls are not assumed to be a Poisson process. Inference and diagnostics are based around the inhomogeneous if-function with confidence envelopes obtained from either resampling events in a nonparametric bootstrap approach, or simulating new events as in a parametric bootstrap. Methods developed are demonstrated using the locations of adult and juvenile trees in a tropical forest. A simulation study briefly examines the accuracy and power of the inferential procedures.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\69UIMS6Q\\Henrys and Brown - 2009 - Inference for Clustered Inhomogeneous Spatial Poin.pdf;C\:\\Users\\devan\\Zotero\\storage\\Y9ATZVGW\\Henrys and Brown - 2009 - Inference for Clustered Inhomogeneous Spatial Poin.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{herringBayesianModelingMultiple2007,
  title = {Bayesian {{Modeling}} of {{Multiple Episode Occurrence}} and {{Severity}} with a {{Terminating Event}}},
  author = {Herring, Amy H. and Yang, Juan},
  year = {2007},
  volume = {63},
  pages = {381--388},
  abstract = {An individual's health condition can affect the frequency and intensity of episodes that can occur repeatedly and that may be related to an event time of interest. For example, bleeding episodes during pregnancy may indicate problems predictive of preterm delivery. Motivated by this application, we propose a joint model for a multiple episode process and an event time. The frequency of occurrence and severity of the episodes are characterized by a latent variable model, which allows an individual's episode intensity to change dynamically over time. This latent episode intensity is then incorporated as a predictor in a discrete time model for the terminating event. Time-varying coefficients are used to distinguish among effects earlier versus later in gestation. Formulating the model within a Bayesian framework, prior distributions are chosen so that conditional posterior distributions are conjugate after data augmentation. Posterior computation proceeds via an efficient Gibbs sampling algorithm. The methods are illustrated using bleeding episode and gestational length data from a pregnancy study.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PGXZJJTY\\Herring and Yang - 2007 - Bayesian Modeling of Multiple Episode Occurrence a.pdf;C\:\\Users\\devan\\Zotero\\storage\\U2SDCSUP\\Herring and Yang - 2007 - Bayesian Modeling of Multiple Episode Occurrence a.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@phdthesis{heStatisticalModelingCO2,
  title = {Statistical {{Modeling}} of {{CO2 Flux Data}}},
  author = {He, Fang},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MMN643BH\\He - Statistical Modeling of CO2 Flux Data.pdf},
  language = {en}
}

@article{hickeyJointModellingTimetoevent2016,
  title = {Joint Modelling of Time-to-Event and Multivariate Longitudinal Outcomes: Recent Developments and Issues},
  shorttitle = {Joint Modelling of Time-to-Event and Multivariate Longitudinal Outcomes},
  author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and {Kolamunnage-Dona}, Ruwanthi},
  year = {2016},
  month = dec,
  volume = {16},
  pages = {117},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0212-5},
  abstract = {Background: Available methods for the joint modelling of longitudinal and time-to-event outcomes have typically only allowed for a single longitudinal outcome and a solitary event time. In practice, clinical studies are likely to record multiple longitudinal outcomes. Incorporating all sources of data will improve the predictive capability of any model and lead to more informative inferences for the purpose of medical decision-making. Methods: We reviewed current methodologies of joint modelling for time-to-event data and multivariate longitudinal data including the distributional and modelling assumptions, the association structures, estimation approaches, software tools for implementation and clinical applications of the methodologies. Results: We found that a large number of different models have recently been proposed. Most considered jointly modelling linear mixed models with proportional hazard models, with correlation between multiple longitudinal outcomes accounted for through multivariate normally distributed random effects. So-called current value and random effects parameterisations are commonly used to link the models. Despite developments, software is still lacking, which has translated into limited uptake by medical researchers. Conclusion: Although, in an era of personalized medicine, the value of multivariate joint modelling has been established, researchers are currently limited in their ability to fit these models routinely. We make a series of recommendations for future research needs.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MK27SWRQ\\Hickey et al. - 2016 - Joint modelling of time-to-event and multivariate .pdf},
  journal = {BMC Medical Research Methodology},
  language = {en},
  number = {1}
}

@misc{HighdimensionalPeaksoverthresholdInference,
  title = {High-Dimensional Peaks-over-Threshold Inference | {{Biometrika}} | {{Oxford Academic}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TMBKXGMX\\High-dimensional peaks-over-threshold inference  .pdf;C\:\\Users\\devan\\Zotero\\storage\\7PVVIHKJ\\5042780.html},
  howpublished = {https://academic.oup.com/biomet/article/105/3/575/5042780}
}

@book{hijmansRasterGeographicData2019,
  title = {Raster: Geographic Data Analysis and Modeling},
  author = {Hijmans, Robert J.},
  year = {2019}
}

@book{hilbeBayesianModelsAstrophysical2017,
  title = {Bayesian {{Models}} for {{Astrophysical Data}}: {{Using R}}, {{JAGS}}, {{Python}}, and {{Stan}}},
  shorttitle = {Bayesian {{Models}} for {{Astrophysical Data}}},
  author = {Hilbe, Joseph M. and {de Souza}, Rafael S. and Ishida, Emille E. O.},
  year = {2017},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781316459515},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F33QUD4S\\Hilbe et al. - 2017 - Bayesian Models for Astrophysical Data Using R, J.pdf},
  isbn = {978-1-316-45951-5},
  language = {en}
}

@article{hildemanLevelSetCox2017,
  title = {Level Set {{Cox}} Processes},
  author = {Hildeman, Anders and Bolin, David and Wallin, Jonas and Illian, Janine B.},
  year = {2017},
  month = aug,
  abstract = {The log-Gaussian Cox process (LGCP) is a popular point process for modeling noninteracting spatial point patterns. This paper extends the LGCP model to handle data exhibiting fundamentally different behaviors in different subregions of the spatial domain. The aim of the analyst might be either to identify and classify these regions, to perform kriging, or to derive some properties of the parameters driving the random field in one or several of the subregions. The extension is based on replacing the latent Gaussian random field in the LGCP by a latent spatial mixture model. The mixture model is specified using a latent, categorically valued, random field induced by level set operations on a Gaussian random field. Conditional on the classification, the intensity surface for each class is modeled by a set of independent Gaussian random fields. This allows for standard stationary covariance structures, such as the Mat\'ern family, to be used to model Gaussian random fields with some degree of general smoothness but also occasional and structured sharp discontinuities. A computationally efficient MCMC method is proposed for Bayesian inference and we show consistency of finite dimensional approximations of the model. Finally, the model is fitted to point pattern data derived from a tropical rainforest on Barro Colorado island, Panama. We show that the proposed model is able to capture behavior for which inference based on the standard LGCP is biased.},
  archivePrefix = {arXiv},
  eprint = {1708.06982},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NGMIWUIC\\Hildeman et al. - 2017 - Level set Cox processes.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZGJNCZGS\\Hildeman et al. - 2017 - Level set Cox processes.pdf},
  journal = {arXiv:1708.06982 [stat]},
  keywords = {62M30,Statistics - Applications,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{hoefMultivariableSpatialPrediction,
  title = {Multivariable {{Spatial Prediction}}},
  author = {Hoef, Jay M Ver and Cressie, Noel},
  pages = {22},
  abstract = {For spatial prediction, it has been usual to predict one variable at a time, with the predictor using data from the same type of variable (kriging) or using additional data from auxiliary variables (cokriging). Optimal predictors can be expressed in terms of covariance functions or variograms. In earth science applications, it is often desirable to predict the joint spatial abundance of variables. A review of cokriging shows that a new cross-variogram allows optimal prediction without any symmetry condition on the covariance function. A bivariate model shows that cokriging with previously used cross-variograms can result in inferior prediction. The simultaneous spatial predietion of several variables, based on the new cross-variogram, is then developed. Multivariable spatial prediction yields the mean-squared prediction error matrix, and so allows the construction of multivariate prediction regions. Relationships between cross-variograms, between single-variable and multivariable spatial prediction, and between generalized least squares estimation and spatial prediction are also given.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CCT7Z74I\\Hoef and Cressie - Multivariable Spatial Prediction.pdf},
  language = {en}
}

@article{hoganBayesianFactorAnalysis2004,
  title = {Bayesian {{Factor Analysis}} for {{Spatially Correlated Data}}, {{With Application}} to {{Summarizing Area}}-{{Level Material Deprivation From Census Data}}},
  author = {Hogan, Joseph W and Tchernis, Rusty},
  year = {2004},
  month = jun,
  volume = {99},
  pages = {314--324},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214504000000296},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7K8FEA44\\Hogan and Tchernis - 2004 - Bayesian Factor Analysis for Spatially Correlated .pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {466}
}

@misc{hohlNumbersIssuesBinning2017,
  title = {Behind the {{Numbers}}: {{The}} Issues with Binning, {{QoC}}, and Scoring Chances},
  shorttitle = {Behind the {{Numbers}}},
  author = {Hohl, Garret},
  year = {2017},
  month = feb,
  abstract = {Every once-in-a-while I will rant on the concepts and ideas behind what numbers suggest in a series called Behind the Numbers, as a tip of the hat to the website that brought me into hockey analyti\ldots},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HXNEDIT2\\behind-the-numbers-the-issues-with-binning-qoc-and-scoring-chances.html},
  journal = {Hockey Graphs},
  language = {en}
}

@misc{hohlNumbersWhatMakes2017,
  title = {Behind the {{Numbers}}: {{What Makes}} a {{Stat Good}}},
  shorttitle = {Behind the {{Numbers}}},
  author = {Hohl, Garret},
  year = {2017},
  month = dec,
  abstract = {By MithrandirMage [CC BY-SA 3.0], via Wikimedia Commons Every once-in-a-while I will rant on the concepts and ideas behind what numbers suggest in a series called Behind the Numbers, as a tip of th\ldots},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4LE2DIP2\\behind-the-numbers-what-makes-a-stat-good.html},
  journal = {Hockey Graphs},
  language = {en}
}

@article{hoModellingMarkedPoint2008,
  title = {Modelling Marked Point Patterns by Intensity-Marked {{Cox}} Processes},
  author = {Ho, Lai Ping and Stoyan, D.},
  year = {2008},
  month = aug,
  volume = {78},
  pages = {1194--1199},
  issn = {01677152},
  doi = {10.1016/j.spl.2007.11.013},
  abstract = {This paper introduces two models of marked Cox point processes where the marks are constructed by means of the intensity function in order to obtain correlations between local point density and marks. Explicit expressions for various functional secondorder characteristics are derived.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VJITL2VZ\\Ho and Stoyan - 2008 - Modelling marked point patterns by intensity-marke.pdf},
  journal = {Statistics \& Probability Letters},
  language = {en},
  number = {10}
}

@article{hoModellingMarkedPoint2008a,
  title = {Modelling Marked Point Patterns by Intensity-Marked {{Cox}} Processes},
  author = {Ho, Lai Ping and Stoyan, D.},
  year = {2008},
  month = aug,
  volume = {78},
  pages = {1194--1199},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2007.11.013},
  abstract = {This paper introduces two models of marked Cox point processes where the marks are constructed by means of the intensity function in order to obtain correlations between local point density and marks. Explicit expressions for various functional second-order characteristics are derived.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JDYHAG84\\Ho and Stoyan - 2008 - Modelling marked point patterns by intensity-marke.pdf;C\:\\Users\\devan\\Zotero\\storage\\2TUMGFS6\\S0167715207003926.html},
  journal = {Statistics \& Probability Letters},
  language = {en},
  number = {10}
}

@article{horneLikelihoodCrossValidationLeast2006,
  title = {Likelihood {{Cross}}-{{Validation Versus Least Squares Cross}}-{{Validation}} for {{Choosing}} the {{Smoothing Parameter}} in {{Kernel Home}}-{{Range Analysis}}},
  author = {Horne, Jon S. and Garton, Edward O.},
  year = {2006},
  month = jun,
  volume = {70},
  pages = {641--648},
  issn = {0022-541X, 1937-2817},
  doi = {10.2193/0022-541X(2006)70[641:LCVLSC]2.0.CO;2},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7T5GLLPU\\Horne and Garton - 2006 - Likelihood Cross-Validation Versus Least Squares C.pdf},
  journal = {Journal of Wildlife Management},
  language = {en},
  number = {3}
}

@article{hotelling1947multivariate,
  title = {Multivariate Quality Control},
  author = {Hotelling, Harold},
  year = {1947},
  publisher = {{McGraw-Hill}},
  journal = {Techniques of statistical analysis}
}

@article{hunterNewMetricsEvaluating2018,
  title = {New Metrics for Evaluating Home Plate Umpire Consistency and Accuracy},
  author = {Hunter, David J.},
  year = {2018},
  month = nov,
  volume = {14},
  pages = {159--172},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2018-0061},
  abstract = {The availability of pitch-tracking data has led to increased scrutiny of Major League Baseball umpires. While many studies have attempted to rate umpires based on their conformity to the rule book strike zone, players and managers tend to accept deviations from this zone, provided that umpires establish consistent zones within a game. Using tools from computational geometry, we propose new metrics for assessing the consistency and accuracy of an umpire's ball and strike calls over the course of a game. We apply these metrics to pitch-tracking data on all ball and strike calls made during the 2017 MLB regular season, giving some characterizations of the variation in performance of MLB umpires. This analysis demonstrates that measures of consistency can complement current accuracy-based evaluations of umpires.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EIDW2YWI\\Hunter - 2018 - New metrics for evaluating home plate umpire consi.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {4}
}

@article{hutchinsPositiondependentMotifCharacterization2008,
  title = {Position-Dependent Motif Characterization Using Non-Negative Matrix Factorization},
  author = {Hutchins, Lucie N. and Murphy, Sean M. and Singh, Priyam and Graber, Joel H.},
  year = {2008},
  month = dec,
  volume = {24},
  pages = {2684--2690},
  issn = {1460-2059, 1367-4803},
  doi = {10.1093/bioinformatics/btn526},
  abstract = {Motivation: Cis-acting regulatory elements are frequently constrained by both sequence content and positioning relative to a functional site, such as a splice or polyadenylation site. We describe an approach to regulatory motif analysis based on non-negative matrix factorization (NMF). Whereas existing pattern recognition algorithms commonly focus primarily on sequence content, our method simultaneously characterizes both positioning and sequence content of putative motifs.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5KNE4LJB\\Hutchins et al. - 2008 - Position-dependent motif characterization using no.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {23}
}

@article{Hyndman2010,
  title = {Rainbow Plots, Bagplots, and Boxplots for Functional Data},
  author = {Hyndman, Rob J. and Shang, Han Lin},
  year = {2010},
  volume = {19},
  pages = {29--45},
  issn = {10618600},
  doi = {10.1198/jcgs.2009.08158},
  abstract = {We propose new tools for visualizing large amounts of functional data in the form of smooth curves. The proposed tools include functional versions of the bagplot and boxplot, which make use of the first two robust principal component scores. Tukey's data depth and highest density regions. By-products of our graphical displays are outlier detection methods for functional data. We compare these new outlier detection methods with existing methods for detecting outliers in functional data, and show that our methods are better able to identify outliers. An R-package containing computer code and datasets is available in the online supplements},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WNP9MHHQ\\Hyndman and Shang - 2010 - Rainbow plots, bagplots, and boxplots for function.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {Highest density regions,Kernel density estimation,Outlier detection,Robust principal component analysis,Tukey's halfspace location depth},
  number = {1}
}

@article{hyndmanRainbowPlotsBagplots2010,
  title = {Rainbow {{Plots}}, {{Bagplots}}, and {{Boxplots}} for {{Functional Data}}},
  author = {Hyndman, Rob J. and Shang, Han Lin},
  year = {2010},
  month = jan,
  volume = {19},
  pages = {29--45},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2009.08158},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RWNEXBRY\\Hyndman and Shang - 2010 - Rainbow Plots, Bagplots, and Boxplots for Function.pdf;C\:\\Users\\devan\\Zotero\\storage\\TJ49TW3Y\\Hyndman and Shang - 2010 - Rainbow Plots, Bagplots, and Boxplots for Function.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {1}
}

@article{hyndmanRobustForecastingMortality2007,
  title = {Robust Forecasting of Mortality and Fertility Rates: {{A}} Functional Data Approach},
  shorttitle = {Robust Forecasting of Mortality and Fertility Rates},
  author = {Hyndman, Rob J. and Shahid Ullah, Md.},
  year = {2007},
  month = jun,
  volume = {51},
  pages = {4942--4956},
  issn = {01679473},
  doi = {10.1016/j.csda.2006.07.028},
  abstract = {A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age, is robust for outlying years due to wars and epidemics, and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis, nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error. The model is a generalization of the Lee\textendash Carter (LC) model commonly used in mortality and fertility forecasting. The methodology is applied to French mortality data and Australian fertility data, and the forecasts obtained are shown to be superior to those from the LC method and several of its variants.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SDKT336H\\Hyndman and Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rate.pdf;C\:\\Users\\devan\\Zotero\\storage\\SPIY7N74\\Hyndman and Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rate.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {10}
}

@article{iftimiSecondorderAnalysisMarked2016,
  title = {The Second-Order Analysis of Marked Spatio-Temporal Point Processes, with an Application to Earthquake Data},
  author = {Iftimi, Adina and Cronie, Ottmar and Montes, Francisco},
  year = {2016},
  month = nov,
  abstract = {To analyse interaction in marked spatio-temporal point processes (MSTPPs), we introduce marked (cross) second-order reduced moment measures and K-functions for general inhomogeneous second-order intensity reweighted stationary MSTPPs. These summary statistics, which allow us to quantify dependence between different markcategories of the points, are depending on the specific mark space and mark reference measure chosen. We also look closer at how the summary statistics reduce under assumptions such as the MSTPP being multivariate and/or stationary. A new test for independent marking is devised and unbiased minus-sampling estimators are derived for all statistics considered. In addition, we treat Voronoi intensity estimators for MSTPPs and indicate their unbiasedness. These new statistics are finally employed to analyse the well-known Andaman sea earthquake dataset. We find that clustering takes place between main and fore-/aftershocks at virtually all space and time scales. In addition, we find evidence that, conditionally on the space-time locations of the earthquakes, the magnitudes do not behave like an iid sequence.},
  archivePrefix = {arXiv},
  eprint = {1611.04808},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\A75PDB4Q\\Iftimi et al. - 2016 - The second-order analysis of marked spatio-tempora.pdf},
  journal = {arXiv:1611.04808 [stat]},
  keywords = {60G55; 60D05; 62M30,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{illianCONTRIBUTIONSSPATIALPOINT,
  title = {{{CONTRIBUTIONS OF SPATIAL POINT PROCESS MODELLING TO BIODIVERSITY THEORY}}},
  author = {Illian, Janine and Burslem, David},
  pages = {21},
  abstract = {Recent decades have seen an unprecedented decline in biodiversity that has led to a growing concern about the consequences of biodiversity loss for the functioning of ecosystems. Key research in plant community ecology seeks to reveal the mechanisms that allow a large number of species to coexist and sustain biodiversity. Processes in plant communities are predominantly local and interactions take place in a spatial context. They thus need to be modelled from the individual plants' perspective.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D296BBVI\\Illian and Burslem - CONTRIBUTIONS OF SPATIAL POINT PROCESS MODELLING T.pdf;C\:\\Users\\devan\\Zotero\\storage\\I9JQXMHW\\Illian and Burslem - CONTRIBUTIONS OF SPATIAL POINT PROCESS MODELLING T.pdf},
  language = {en}
}

@article{illianContributionsSpatialPoint2007,
  title = {Contributions of Spatial Point Process Modelling to Biodiversity Theory},
  author = {Illian, J. B. and Burslem, D. F. R. P.},
  year = {2007},
  volume = {148},
  pages = {9--29},
  issn = {2102-6238},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TAZDTIV2\\199497.html},
  journal = {Journal de la Societe Francaise de Statistique},
  language = {en-GB}
}

@article{illianFittingComplexEcological2013,
  title = {Fitting Complex Ecological Point Process Models with Integrated Nested {{Laplace}} Approximation},
  author = {Illian, Janine B. and Martino, Sara and S{\o}rbye, Sigrunn H. and Gallego-Fern{\'a}ndez, Juan B. and Zunzunegui, Mar{\'i}a and Esquivias, M. Paz and Travis, Justin M. J.},
  year = {2013},
  month = apr,
  volume = {4},
  pages = {305--315},
  issn = {2041-210X},
  doi = {10.1111/2041-210x.12017@10.1111/(ISSN)2041-210X.STATSTOO},
  abstract = {We highlight an emerging statistical method, integrated nested Laplace approximation (INLA), which is ideally suited for fitting complex models to many of the rich spatial data sets that ecologist...},
  file = {C\:\\Users\\devan\\Zotero\\storage\\T4Y4IUB2\\(ISSN)2041-210X.html},
  journal = {Methods in Ecology and Evolution},
  language = {en},
  number = {4}
}

@article{illianGibbsPointProcess2009,
  title = {Gibbs Point Process Models with Mixed Effects},
  author = {Illian, Janine B. and Hendrichsen, Ditte K.},
  year = {2009},
  month = aug,
  volume = {21},
  pages = {341--353},
  issn = {11804009, 1099095X},
  doi = {10.1002/env.1008},
  abstract = {We consider spatial point patterns that have been observed repeatedly in the same area at several points in time. We take a maximum pseudolikelihood approach (Besag, 1976) to parameter estimation in the context of Gibbs processes (Stoyan et al., 1995; Illian et al., 2008). More specifically, we discuss pair-wise interaction processes where the conditional intensity has a log-linear form and extend existing models by expressing the intensity and the interaction terms in the pseudolikelihood as a sum of fixed and random effects, where the latter accounts for variation over time. We initially derive a Strauss process model with mixed effects. As this model is too simplistic in the given context, we further consider a more general model that allows for inter-group differences in intensity and interaction strength and has a more flexible interaction function. We apply the approximate Berman\textendash Turner device (Baddeley and Turner, 2000) to a generalised linear mixed model with log link and Poisson outcome rather than a simple generalised linear model. Estimates are obtained using existing software for generalised linear mixed models based on penalised quasi-likelihood methods (Bresow and Clayton, 1993).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TTBRWIUE\\Illian and Hendrichsen - 2009 - Gibbs point process models with mixed effects.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZMW43XRV\\Illian and Hendrichsen - 2009 - Gibbs point process models with mixed effects.pdf},
  journal = {Environmetrics},
  language = {en},
  number = {3-4}
}

@article{illianHierarchicalSpatialPoint2009,
  title = {Hierarchical Spatial Point Process Analysis for a Plant Community with High Biodiversity},
  author = {Illian, Janine B. and M{\o}ller, Jesper and Waagepetersen, Rasmus P.},
  year = {2009},
  month = sep,
  volume = {16},
  pages = {389--405},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-007-0070-8},
  abstract = {A complex multivariate spatial point pattern of a plant community with high biodiversity is modelled using a hierarchical multivariate point process model. In the model, interactions between plants with different post-fire regeneration strategies are of key interest. We consider initially a maximum likelihood approach to inference where problems arise due to unknown interaction radii for the plants. We next demonstrate that a Bayesian approach provides a flexible framework for incorporating prior information concerning the interaction radii. From an ecological perspective, we are able both to confirm existing knowledge on species' interactions and to generate new biological questions and hypotheses on species' interactions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\U3JITA68\\Illian et al. - 2009 - Hierarchical spatial point process analysis for a .pdf;C\:\\Users\\devan\\Zotero\\storage\\ZUM5I4A8\\Illian et al. - 2009 - Hierarchical spatial point process analysis for a .pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {3}
}

@article{illianImprovingUsabilitySpatial2017,
  title = {Improving the Usability of Spatial Point Process Methodology: An Interdisciplinary Dialogue between Statistics and Ecology},
  shorttitle = {Improving the Usability of Spatial Point Process Methodology},
  author = {Illian, Janine B. and Burslem, David F. R. P.},
  year = {2017},
  month = oct,
  volume = {101},
  pages = {495--520},
  issn = {1863-8171, 1863-818X},
  doi = {10.1007/s10182-017-0301-8},
  abstract = {The last few decades have seen an increasing interest and strong development in spatial point process methodology, and associated software that facilitates model fitting has become available. A lot of this progress has made these approaches more accessible to users, through freely available software. However, in the ecological user community the methodology has only been slowly picked up despite its obvious relevance to the field. This paper reflects on this development, highlighting mutual benefits of interdisciplinary dialogue for both statistics and ecology. We detail the contribution point process methodology has made to research on biodiversity theory as a result of this dialogue and reflect on reasons for the slow take-up of the methodology. This primarily concerns the current lack of consideration of the usability of the approaches, which we discuss in detail, presenting current discussions as well as indicating future directions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\72WWKB59\\Illian and Burslem - 2017 - Improving the usability of spatial point process m.pdf},
  journal = {AStA Advances in Statistical Analysis},
  language = {en},
  number = {4}
}

@article{illianImprovingUsabilitySpatial2017a,
  title = {Improving the Usability of Spatial Point Process Methodology: An Interdisciplinary Dialogue between Statistics and Ecology},
  shorttitle = {Improving the Usability of Spatial Point Process Methodology},
  author = {Illian, Janine B. and Burslem, David F. R. P.},
  year = {2017},
  month = oct,
  volume = {101},
  pages = {495--520},
  issn = {1863-8171, 1863-818X},
  doi = {10.1007/s10182-017-0301-8},
  abstract = {The last few decades have seen an increasing interest and strong development in spatial point process methodology, and associated software that facilitates model fitting has become available. A lot of this progress has made these approaches more accessible to users, through freely available software. However, in the ecological user community the methodology has only been slowly picked up despite its obvious relevance to the field. This paper reflects on this development, highlighting mutual benefits of interdisciplinary dialogue for both statistics and ecology. We detail the contribution point process methodology has made to research on biodiversity theory as a result of this dialogue and reflect on reasons for the slow take-up of the methodology. This primarily concerns the current lack of consideration of the usability of the approaches, which we discuss in detail, presenting current discussions as well as indicating future directions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9P4BBK22\\Illian and Burslem - 2017 - Improving the usability of spatial point process m.pdf;C\:\\Users\\devan\\Zotero\\storage\\D62HYGSL\\Illian and Burslem - 2017 - Improving the usability of spatial point process m.pdf},
  journal = {AStA Advances in Statistical Analysis},
  language = {en},
  number = {4}
}

@article{illianPrincipalComponentAnalysis,
  title = {Principal {{Component Analysis}} for {{Spatial Point Process Data}}},
  author = {Illian, Janine and Benson, Erica and Staines, Harry and Crawford, John},
  pages = {4},
  abstract = {Spatial point processes are complex stochastic models. Their complexity increases with the number of types in a multi-type point pattern due to the increasing number of potential inter-and intra-type interactions and thus the increasing number of parameters complicating simulation and estimation. This calls for a statistical method which reduces the dimensionality of the dataset, i.e. groups the point processes. But even though multivariate statistical methods such as principal component analysis (PCA) and factor analysis have been around for a long time and are a popular tool for dimension reduction in many contexts, no similar method exists for spatial point pattern data. This talk will illustrate how functional data analysis tools can be applied to the second order statistics of multi-type point processes, in particular to (inhomogeneous) L-functions, to derive a PCA method for spatial point pattern data. We will briefly introduce functional data analysis, in particular functional PCA and discuss how these methods can be extended to work in the context of spatial point patterns. A detailed simulation study was used to validate the approach and showed promising results, giving rise to some general guidelines and results. We conclude with an application, investigating a multi-type spatial point pattern formed by a natural plant community in the heathlands of Western Australia, comprising 6378 plants from 68 species (see Illian (200x)).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YNEMJW9G\\Illian et al. - Principal Component Analysis for Spatial Point Pro.pdf},
  language = {en}
}

@article{illianSpatialModellingEcological,
  title = {Spatial Modelling for Ecological Surveys \textendash{} Contributions from and to Point Process Modelling},
  author = {Illian, Janine},
  pages = {94},
  file = {C\:\\Users\\devan\\Zotero\\storage\\25GLPRW7\\Illian - spatial modelling for ecological surveys – contrib.pdf},
  language = {en}
}

@article{illianSpatialPointProcess,
  title = {Spatial Point Process Modelling in the Context of Biodiversity Theory},
  author = {Illian, Janine B},
  pages = {3},
  file = {C\:\\Users\\devan\\Zotero\\storage\\M9PPPWBP\\Illian - Spatial point process modelling in the context of .pdf;C\:\\Users\\devan\\Zotero\\storage\\Z7X53Y3H\\Illian - Spatial point process modelling in the context of .pdf},
  language = {en}
}

@article{illianToolboxFittingComplex2012,
  title = {A Toolbox for Fitting Complex Spatial Point Process Models Using Integrated Nested {{Laplace}} Approximation ({{INLA}})},
  author = {Illian, Janine B. and S{\o}rbye, Sigrunn H. and Rue, H{\aa}vard},
  year = {2012},
  month = dec,
  volume = {6},
  pages = {1499--1530},
  issn = {1932-6157},
  doi = {10.1214/11-AOAS530},
  abstract = {This paper develops methodology that provides a toolbox for routinely fitting complex models to realistic spatial point pattern data. We consider models that are based on log-Gaussian Cox processes and include local interaction in these by considering constructed covariates. This enables us to use integrated nested Laplace approximation and to considerably speed up the inferential task. In addition, methods for model comparison and model assessment facilitate the modelling process. The performance of the approach is assessed in a simulation study. To demonstrate the versatility of the approach, models are fitted to two rather different examples, a large rainforest data set with covariates and a point pattern with multiple marks.},
  archivePrefix = {arXiv},
  eprint = {1301.1817},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PVDLF92G\\Illian et al. - 2012 - A toolbox for fitting complex spatial point proces.pdf},
  journal = {The Annals of Applied Statistics},
  keywords = {Statistics - Applications},
  language = {en},
  number = {4}
}

@article{illianUsingINLAFit2012,
  title = {Using {{INLA To Fit A Complex Point Process Model With Temporally Varying Effects}} \textendash{} {{A Case Study}}},
  author = {Illian, Janine B. and S{\o}rbye, Sigrunn H},
  year = {2012},
  volume = {3},
  pages = {29},
  abstract = {Integrated nested Laplace approximation (INLA) provides a fast and yet quite exact approach to fitting complex latent Gaussian models which comprise many statistical models in a Bayesian context, including log Gaussian Cox processes. This paper discusses how a joint log Gaussian Cox process model may be fitted to independent replicated point patterns. We illustrate the approach by fitting a model to data on the locations of muskoxen (Ovibos moschatus) herds in Zackenberg valley, Northeast Greenland and by detailing how this model is specified within the R-interface R-INLA. The paper strongly focuses on practical problems involved in the modelling process, including issues of spatial scale, edge effects and prior choices, and finishes with a discussion on models with varying boundary conditions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8729IZLY\\Illian and Sørbye - Using INLA To Fit A Complex Point Process Model Wi.pdf;C\:\\Users\\devan\\Zotero\\storage\\QG5V5J7I\\Illian and Sørbye - Using INLA To Fit A Complex Point Process Model Wi.pdf;C\:\\Users\\devan\\Zotero\\storage\\YFZLW2PG\\Illian and Sørbye - Using INLA To Fit A Complex Point Process Model Wi.pdf},
  journal = {Journal of Environmental Statistics},
  language = {en},
  number = {7}
}

@book{internationalconferenceontheapplicationsofgisandspatialanalysistoveterinaryscienceGISVET042004,
  title = {{{GISVET}}'04},
  editor = {{International Conference on the Applications of GIS {and} Spatial Analysis to Veterinary Science} and Durr, P. A. and Martin, S. Wayne and Veterinary Laboratories Agency and GISVET},
  year = {2004},
  publisher = {{Veterinary Laboratories Agency}},
  address = {{Weybridge}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WB6K3BR2\\International Conference on the Applications of GIS and Spatial Analysis to Veterinary Science et al. - 2004 - GISVET'04.pdf},
  isbn = {978-1-899513-23-9},
  language = {en}
}

@techreport{IntroductionPointProcesses,
  title = {Introduction to {{Point Processes}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Z9FQILET\\Microsoft Word - pp4.pdf}
}

@article{Jackson2006,
  title = {Prevalence of Fatigue among Commercial Pilots},
  author = {Jackson, Craig A. and Earl, Laurie},
  year = {2006},
  volume = {56},
  pages = {263--268},
  issn = {09627480},
  doi = {10.1093/occmed/kql021},
  abstract = {BACKGROUND: Short-haul pilots have largely been neglected in studies of fatigue, sleep loss and circadian disruption created by flight operations, but anecdotal evidence from pilots suggests that with the increasing amount of low-cost air travel, commercial pilots working short-haul operations may be becoming seriously fatigued. AIMS: This study attempted to ascertain how much subjective fatigue short-haul pilots reported, and makes comparisons between low-cost and scheduled airline pilots. METHODS: Pilots completed anonymous questionnaires (encompassing aviation factors, flight experience and a fatigue scale) posted on the Professional Pilots' Rumours Network website. RESULTS: Data were collected from 162 short-haul pilots and statistical adjustment for operational factors was made. Seventy-five percent reported severe fatigue and 81{{\%}} reported the fatigue to be worse than 2 years ago. Eighty percent considered their thought processes were reduced while flying. Severe fatigue was reported more frequently by low-cost airline pilots than scheduled airline pilots (P = 0.05) and fatigue ratings were higher in this group (P = 0.03). Pilots who reported regularly flying into their 'discretion' hours had lower physical and psychological health, and overall fatigue scores, and poorer self-rated general health. Flying into discretion time occurred no more frequently in low-cost airline pilots than scheduled airline pilots. CONCLUSIONS: Identifiable fatigue problems are reported by short-haul pilots, but this cannot be attributed solely to the work schedules of low-cost airlines as regular use of discretion time appears to be associated with fatigue regardless of airline.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WDEK373H\\Jackson and Earl - 2006 - Prevalence of fatigue among commercial pilots.pdf},
  isbn = {0962-7480; 1471-8405},
  journal = {Occupational Medicine},
  keywords = {Aviation,Fatigue,Pilots,Short-haul},
  number = {4},
  pmid = {16733255}
}

@article{jacksonPrevalenceFatigueCommercial2006,
  title = {Prevalence of Fatigue among Commercial Pilots},
  author = {Jackson, Craig A. and Earl, Laurie},
  year = {2006},
  month = jun,
  volume = {56},
  pages = {263--268},
  issn = {1471-8405, 0962-7480},
  doi = {10.1093/occmed/kql021},
  abstract = {Background Short-haul pilots have largely been neglected in studies of fatigue, sleep loss and circadian disruption created by flight operations, but anecdotal evidence from pilots suggests that with the increasing amount of low-cost air travel, commercial pilots working short-haul operations may be becoming seriously fatigued. ................................................................................................................................................................................... Aims This study attempted to ascertain how much subjective fatigue short-haul pilots reported, and makes comparisons between low-cost and scheduled airline pilots. ................................................................................................................................................................................... Methods Pilots completed anonymous questionnaires (encompassing aviation factors, flight experience and a fatigue scale) posted on the Professional Pilots' Rumours Network website. ................................................................................................................................................................................... Results Data were collected from 162 short-haul pilots and statistical adjustment for operational factors was made. Seventy-five percent reported severe fatigue and 81\% reported the fatigue to be worse than 2 years ago. Eighty percent considered their thought processes were reduced while flying. Severe fatigue was reported more frequently by low-cost airline pilots than scheduled airline pilots (P 5 0.05) and fatigue ratings were higher in this group (P 5 0.03). Pilots who reported regularly flying into their `discretion' hours had lower physical and psychological health, and overall fatigue scores, and poorer self-rated general health. Flying into discretion time occurred no more frequently in low-cost airline pilots than scheduled airline pilots. ................................................................................................................................................................................... Conclusions Identifiable fatigue problems are reported by short-haul pilots, but this cannot be attributed solely to the work schedules of low-cost airlines as regular use of discretion time appears to be associated with fatigue regardless of airline.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\G2WQVIKD\\Jackson and Earl - 2006 - Prevalence of fatigue among commercial pilots.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZS7PIB7M\\Jackson and Earl - 2006 - Prevalence of fatigue among commercial pilots.pdf},
  journal = {Occupational Medicine},
  language = {en},
  number = {4}
}

@article{jalilianDecompositionVarianceSpatial2013,
  title = {Decomposition of {{Variance}} for {{Spatial Cox Processes}}: {{{\emph{Variance}}}}{\emph{ Decomposition for }}{{{\emph{Cox}}}}{\emph{ Processes}}},
  shorttitle = {Decomposition of {{Variance}} for {{Spatial Cox Processes}}},
  author = {Jalilian, Abdollah and Guan, Yongtao and Waagepetersen, Rasmus},
  year = {2013},
  month = mar,
  volume = {40},
  pages = {119--137},
  issn = {03036898},
  doi = {10.1111/j.1467-9469.2012.00795.x},
  abstract = {Spatial Cox point processes is a natural framework for quantifying the various sources of variation governing the spatial distribution of rain forest trees. We introduce a general criterion for variance decomposition for spatial Cox processes and apply it to specific Cox process models with additive or log linear random intensity functions. We moreover consider a new and flexible class of pair correlation function models given in terms of normal variance mixture covariance functions. The proposed methodology is applied to point pattern data sets of locations of tropical rain forest trees.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EB6QA8JB\\Jalilian et al. - 2013 - Decomposition of Variance for Spatial Cox Processe.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {1}
}

@book{jeffreys1998,
  title = {The {{Theory}} of {{Probability}}},
  author = {Jeffreys, Harold},
  year = {1998},
  publisher = {{OUP Oxford}},
  isbn = {978-0-19-158967-6},
  series = {Oxford {{Classic Texts}} in the {{Physical Sciences}}}
}

@book{jeffreysTheoryProbability1998,
  title = {The {{Theory}} of {{Probability}}},
  author = {Jeffreys, Harold},
  year = {1998},
  month = aug,
  publisher = {{OUP Oxford}},
  abstract = {Another title in the reissued Oxford Classic Texts in the Physical Sciences series, Jeffrey's Theory of Probability, first published in 1939, was the first to develop a fundamental theory of scientific inference based on the ideas of Bayesian statistics. His ideas were way ahead of their time and it is only in the past ten years that the subject of Bayes' factors has been significantly developed and extended. Until recently the two schools of statistics (Bayesian and Frequentist) were distinctly different and set apart. Recent work (aided by increased computer power and availability) has changed all that and today's graduate students and researchers all require an understanding of Bayesian ideas. This book is their starting point.},
  googlebooks = {vh9Act9rtzQC},
  isbn = {978-0-19-158967-6},
  keywords = {Mathematics / Probability \& Statistics / General},
  language = {en}
}

@incollection{jensenCorrelationAutocorrelationProfiles2011,
  title = {Correlation and {{Autocorrelation}} in {{Profiles}}},
  booktitle = {Statistical {{Analysis}} of {{Profile Monitoring}}},
  author = {Jensen, Willis A. and Birch, Jeffrey B.},
  editor = {Noorossana, Rassoul and Saghaei, Abbas and Amiri, Amirhossein},
  year = {2011},
  pages = {253--268},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118071984.ch9},
  abstract = {This chapter contains sections titled: Introduction Methods for WPA for Linear Models Methods for BPC for Linear Models Methods for WPA and BPC for Other (Nonlinear) Models Phase I Analysis Phase II Analysis Related Issues: Rational Subgrouping and Random Effects Discussion and Open Questions Acknowledgment References},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BNIIRB4G\\9781118071984.html},
  isbn = {978-1-118-07198-4},
  keywords = {BPC and related issues,correlation and autocorrelation in profile monitoring - themselves not independent of each other,for between-profile correlation (BPC) linear models - measurements correlated across profiles,methods,rational subgrouping and random effects - type of grouping,with differences minimized},
  language = {en}
}

@article{jensenPseudolikelihoodExponentialFamily1991,
  title = {Pseudolikelihood for {{Exponential Family Models}} of {{Spatial Point Processes}}},
  author = {Jensen, Jens Ledet and M{\o}ller, Jesper},
  year = {1991},
  volume = {1},
  pages = {445--461},
  file = {C\:\\Users\\devan\\Zotero\\storage\\96TCPFRM\\Jensen and Møller - 1991 - Pseudolikelihood for Exponential Family Models of .pdf},
  journal = {The Annals of Applied Probability},
  language = {en},
  number = {3}
}

@article{jiangGeostatisticalSurvivalModels2014,
  title = {Geostatistical Survival Models for Environmental Risk Assessment with Large Retrospective Cohorts},
  author = {Jiang, Huan and Brown, Patrick E. and Rue, H{\aa}vard and Shimakura, Silvia},
  year = {2014},
  month = jun,
  volume = {177},
  pages = {679--695},
  issn = {09641998},
  doi = {10.1111/rssa.12041},
  abstract = {Motivated by the problem of cancer risk assessment near a nuclear power generating station, the paper describes a methodology for fitting a spatially correlated survival model to large retrospective cohort data sets. Retrospective cohorts, which can be assembled inexpensively from population-based health databases, can partially account for lags between exposures and outcome of chronic diseases such as cancer. These data sets overcome one of the principal limitations of cross-sectional spatial analyses, though performing statistical inference requires accommodating censored and truncated event times as well as spatial dependence. The use of spatial survival models for large retrospective cohorts is described, and Bayesian inference using Markov random-field approximations and integrated nested Laplace approximations is presented. The method is applied to data from individuals living near Pickering Nuclear Generating Station in Canada, showing that the effect of ambient radiation on cancer is not statistically significant.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YT34RSWZ\\Jiang et al. - 2014 - Geostatistical survival models for environmental r.pdf},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  language = {en},
  number = {3}
}

@article{johnsonEstimatingAnimalResource2013,
  title = {Estimating Animal Resource Selection from Telemetry Data Using Point Process Models},
  author = {Johnson, Devin S. and Hooten, Mevin B. and Kuhn, Carey E.},
  year = {2013},
  volume = {82},
  pages = {1155--1164},
  issn = {1365-2656},
  doi = {10.1111/1365-2656.12087},
  abstract = {Analyses of animal resource selection functions (RSF) using data collected from relocations of individuals via remote telemetry devices have become commonplace. Increasing technological advances, however, have produced statistical challenges in analysing such highly autocorrelated data. Weighted distribution methods have been proposed for analysing RSFs with telemetry data. However, they can be computationally challenging due to an intractable normalizing constant and cannot be aggregated (i.e. collapsed) over time to make space-only inference. In this study, we take a conceptually different approach to modelling animal telemetry data for making RSF inference. We consider the telemetry data to be a realization of a space\textendash time point process. Under the point process paradigm, the times of the relocations are also considered to be random rather than fixed. We show the point process models we propose are a generalization of the weighted distribution telemetry models. By generalizing the weighted model, we can access several numerical techniques for evaluating point process likelihoods that make use of common statistical software. Thus, the analysis methods can be readily implemented by animal ecologists. In addition to ease of computation, the point process models can be aggregated over time by marginalizing over the temporal component of the model. This allows a full range of models to be constructed for RSF analysis at the individual movement level up to the study area level. To demonstrate the analysis of telemetry data with the point process approach, we analysed a data set of telemetry locations from northern fur seals (Callorhinus ursinus) in the Pribilof Islands, Alaska. Both a space\textendash time and an aggregated space-only model were fitted. At the individual level, the space\textendash time analysis showed little selection relative to the habitat covariates. However, at the study area level, the space-only model showed strong selection relative to the covariates.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UXSVFUER\\Johnson et al. - 2013 - Estimating animal resource selection from telemetr.pdf;C\:\\Users\\devan\\Zotero\\storage\\9R9I7UF6\\1365-2656.html},
  journal = {Journal of Animal Ecology},
  keywords = {animal telemetry,point process,resource selection,space–time,weighted distribution},
  language = {en},
  number = {6}
}

@article{johnsonEstimatingTimingLifeHistory2004,
  title = {Estimating {{Timing}} of {{Life}}-{{History Events}} with {{Coarse Data}}},
  author = {Johnson, Devin S. and Barry, Ronald P. and Bowyer, R. Terry},
  year = {2004},
  month = oct,
  volume = {85},
  pages = {932--939},
  issn = {0022-2372},
  doi = {10.1644/BFW-009},
  abstract = {Abstract.  Populations often are sampled with a coarse scale of measurement. As scale becomes increasingly coarse, the variance estimate can become biased; Shep},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9CCHVH6X\\Johnson et al. - 2004 - Estimating Timing of Life-History Events with Coar.pdf;C\:\\Users\\devan\\Zotero\\storage\\RX7YYQX2\\860737.html},
  journal = {Journal of Mammalogy},
  language = {en},
  number = {5}
}

@article{jones-toddSpatiotemporalMultispeciesModel2018,
  title = {A Spatiotemporal Multispecies Model of a Semicontinuous Response},
  author = {{Jones-Todd}, Charlotte M. and Swallow, Ben and Illian, Janine B. and Toms, Mike},
  year = {2018},
  month = apr,
  volume = {67},
  pages = {705--722},
  issn = {00359254},
  doi = {10.1111/rssc.12250},
  abstract = {As accessible and potentially vulnerable species high up in the food chain, birds are often used as indicator species to highlight changes in ecosystems. This study focuses on multiple spatially dependent relationships between a raptor (sparrowhawk), a potential prey species (house sparrow) and a sympatric species (collared doves) in space and time. We construct a complex spatiotemporal latent Gaussian model to incorporate both predator\textendash prey and sympatric relationships, which is novel in two ways. First, different types of species interactions are represented by a shared spatiotemporal random effect, which extends existing approaches to multivariate spatial modelling through the use of a joint latent modelling approach. Second, we use a delta\textendash gamma model to capture the semicontinuous nature of the data to model the binary and continuous sections of the response jointly. The results indicate that sparrowhawks have a localized effect on the presence of house sparrows, which could indicate that house sparrows avoid sites where sparrowhawks are present.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2CNAY9BN\\Jones-Todd et al. - 2018 - A spatiotemporal multispecies model of a semiconti.pdf;C\:\\Users\\devan\\Zotero\\storage\\Y4NYV6EL\\Jones-Todd et al. - 2018 - A spatiotemporal multispecies model of a semiconti.pdf},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  language = {en},
  number = {3}
}

@article{josephSpatiotemporalPredictionWildfire2019,
  title = {Spatiotemporal Prediction of Wildfire Size Extremes with {{Bayesian}} Finite Sample Maxima},
  author = {Joseph, Maxwell B. and Rossi, Matthew W. and Mietkiewicz, Nathan P. and Mahood, Adam L. and Cattau, Megan E. and St. Denis, Lise Ann and Nagy, R. Chelsea and Iglesias, Virginia and Abatzoglou, John T. and Balch, Jennifer K.},
  year = {2019},
  month = apr,
  volume = {0},
  pages = {e01898},
  issn = {1051-0761},
  doi = {10.1002/eap.1898},
  abstract = {Abstract Wildfires are becoming more frequent in parts of the globe, but predicting where and when wildfires occur remains difficult. To predict wildfire extremes across the contiguous United States, we integrate a 30-yr wildfire record with meteorological and housing data in spatiotemporal Bayesian statistical models with spatially varying nonlinear effects. We compared different distributions for the number and sizes of large fires to generate a posterior predictive distribution based on finite sample maxima for extreme events (the largest fires over bounded spatiotemporal domains). A zero-inflated negative binomial model for fire counts and a lognormal model for burned areas provided the best performance. This model attains 99\% interval coverage for the number of fires and 93\% coverage for fire sizes over a six year withheld data set. Dryness and air temperature strongly predict extreme wildfire probabilities. Housing density has a hump-shaped relationship with fire occurrence, with more fires occurring at intermediate housing densities. Statistically, these drivers affect the chance of an extreme wildfire in two ways: by altering fire size distributions, and by altering fire frequency, which influences sampling from the tails of fire size distributions. We conclude that recent extremes should not be surprising, and that the contiguous United States may be on the verge of even larger wildfire extremes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3YFFQLCL\\Joseph et al. - 2019 - Spatiotemporal prediction of wildfire size extreme.pdf;C\:\\Users\\devan\\Zotero\\storage\\62WHWVK8\\eap.html},
  journal = {Ecological Applications},
  keywords = {Bayesian,climate,extremes,fire,spatiotemporal,wildfire},
  number = {0}
}

@misc{joshuac.c.chanFastComputationDeviance,
  title = {Fast {{Computation}} of the {{Deviance Information Criterion}} for {{Latent Variable Models}}},
  author = {{Joshua C.C. Chan} and Grant, Angelia L.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JB6Y25AR\\9_2014_chan_grant.pdf}
}

@article{juanCharacterizingSpatialtemporalForest,
  title = {Characterizing Spatial-Temporal Forest Fire Patterns},
  author = {Juan, P and Mateu, J and {D{\'i}az-Avalos}, C},
  pages = {4},
  abstract = {The spatial-temporal patterns of wildfire incidence and their relationship to various meteorological, geographical and geological variables is analyzed. Such relationships may be treated as components in separable point process models for wildfire activity. We show some of the techniques for the analysis of spatial point patterns that have become available due to recent developments in point process modelling software. These developments permit convenient exploratory data analysis, model fitting, and model assessment. The discussion of these techniques is conducted jointly with and in the context of some preliminary analyses of a collection of data sets which are of considerable interest in their own right. These data sets consist of the complete records of wildfires which occurred in Castilla La Mancha (a central Spanish region) during the years 1998-2007.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3EG2DJ5D\\Juan et al. - Characterizing spatial-temporal forest ﬁre pattern.pdf},
  language = {en}
}

@article{juanverdoyEnhancingSPDEModeling2019,
  title = {Enhancing the {{SPDE}} Modeling of Spatial Point Processes with {{INLA}}, Applied to Wildfires. {{Choosing}} the Best Mesh for Each Database},
  author = {Juan Verdoy, Pablo},
  year = {2019},
  month = may,
  pages = {1--34},
  issn = {0361-0918, 1532-4141},
  doi = {10.1080/03610918.2019.1618473},
  abstract = {Wildfires play an important role in shaping landscapes and as a source of CO2 and particulate matter, and are a typical spatial point process studied in many papers. Modeling the spatial variability of a wildfire could be performed in different ways and an important issue is the computational facilities that the new techniques afford us. The most common approaches have been through point pattern analysis or by Markov random fields. These methods have made it possible to build risk maps, but for many forest managers it is very useful to know the size of the fire as well as its location. In this work, we use Stochastic Partial Differential Equation (SPDE) with Integrated Nested Laplace Approximation (INLA) to model the size of the forest fires observed in the Valencian Community, Spain. But the most important element in this paper is the process that needs to be carried out prior to simulating and analyzing the different point patterns, namely, the choice of the most suitable mesh for the database. We describe and take advantage of the Bayesian methodology by including INLA and SPDE in the modeling process in all the scenarios.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7T3H3QT6\\Juan Verdoy - 2019 - Enhancing the SPDE modeling of spatial point proce.pdf},
  journal = {Communications in Statistics - Simulation and Computation},
  language = {en}
}

@article{juarez-colungaEfficientPanelDesigns2014,
  title = {Efficient Panel Designs for Longitudinal Recurrent Event Studies Recording Panel Counts},
  author = {{Juarez-Colunga}, E. and Dean, C. B. and Balshaw, R.},
  year = {2014},
  month = apr,
  volume = {15},
  pages = {234--250},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxt054},
  abstract = {Many clinical trials are designed to study outcome measures recorded as the number of events occurring during specific intervals, called panel data. In such data, the intervals are specified by a planned set of follow-up times. As the collection of panel data results in a partial loss of information relative to a record of the actual event times, it is important to gain a thorough understanding of the impact of panel study designs on the efficiency of the estimates of treatment effects and covariates. This understanding can then be used as a base from which to formulate appropriate designs by layering in other concerns, e.g. clinical constraints, or other practical considerations. We compare the efficiency of the analysis of panel data with respect to the analysis of data recorded precisely as times of recurrences, and articulate conditions for efficient panel designs where the focus is on estimation of a treatment effect when adjusting for other covariates. We build from the efficiency comparisons to optimize the design of panel follow-up times. We model the recurrent intensity through the common proportional intensity framework, with the treatment effect modeled flexibly as piecewise constant over panels, or groups of panels. We provide some important considerations for the design of efficient panel studies, and illustrate the methods through analysis of designs of studies of adenomas.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X9QZD6XD\\Juarez-Colunga et al. - 2014 - Efficient panel designs for longitudinal recurrent.pdf},
  journal = {Biostatistics},
  language = {en},
  number = {2}
}

@article{juarez-colungaJointModelingZero2017,
  title = {Joint Modeling of Zero-inflated Panel Count and Severity Outcomes},
  author = {Juarez-Colunga, E. and Silva, G. L. and Dean, C. B.},
  year = {2017},
  month = dec,
  volume = {73},
  pages = {1413--1423},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/biom.12691},
  abstract = {Panel counts are often encountered in longitudinal, such as diary, studies where individuals are followed over time and the number of events occurring in time intervals, or panels, is recorded. This article develops methods for situations where, in addition to the counts of events, a mark, denoting a measure of severity of the events, is recorded. In many situations there is an association between the panel counts and their marks. This is the case for our motivating application that studies the effect of two hormone therapy treatments in reducing counts and severities of vasomotor symptoms in women after hysterectomy/ovariectomy. We model the event counts and their severities jointly through the use of shared random effects. We also compare, through simulation, the power of testing for the treatment effect based on such joint modeling and an alternative scoring approach, which is commonly employed. The scoring approach analyzes the compound outcome of counts times weighted severity. We discuss this approach and quantify challenges which may arise in isolating the treatment effect when such a scoring approach is used. We also show that the power of detecting a treatment effect is higher when using the joint model than analysis using the scoring approach. Inference is performed via Markov chain Monte Carlo methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3F8K3EF6\\Juarez‐Colunga et al. - 2017 - Joint modeling of zero‐inflated panel count and se.pdf;C\:\\Users\\devan\\Zotero\\storage\\MW2EYKK2\\Juarez‐Colunga et al. - 2017 - Joint modeling of zero‐inflated panel count and se.pdf},
  journal = {Biometrics},
  language = {en},
  number = {4}
}

@article{juarez-colungaJointModelingZeroinflated2017,
  title = {Joint Modeling of Zero-Inflated Panel Count and Severity Outcomes},
  author = {Juarez-Colunga, E. and Silva, G. L. and Dean, C. B.},
  year = {2017},
  volume = {73},
  pages = {1413--1423},
  issn = {1541-0420},
  doi = {10.1111/biom.12691},
  abstract = {Panel counts are often encountered in longitudinal, such as diary, studies where individuals are followed over time and the number of events occurring in time intervals, or panels, is recorded. This article develops methods for situations where, in addition to the counts of events, a mark, denoting a measure of severity of the events, is recorded. In many situations there is an association between the panel counts and their marks. This is the case for our motivating application that studies the effect of two hormone therapy treatments in reducing counts and severities of vasomotor symptoms in women after hysterectomy/ovariectomy. We model the event counts and their severities jointly through the use of shared random effects. We also compare, through simulation, the power of testing for the treatment effect based on such joint modeling and an alternative scoring approach, which is commonly employed. The scoring approach analyzes the compound outcome of counts times weighted severity. We discuss this approach and quantify challenges which may arise in isolating the treatment effect when such a scoring approach is used. We also show that the power of detecting a treatment effect is higher when using the joint model than analysis using the scoring approach. Inference is performed via Markov chain Monte Carlo methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YWMJAEHS\\Juarez‐Colunga et al. - 2017 - Joint modeling of zero-inflated panel count and se.pdf;C\:\\Users\\devan\\Zotero\\storage\\DLIKE6NV\\biom.html},
  journal = {Biometrics},
  keywords = {Bayesian analysis,Hurdle model,Longitudinal data,Poisson regression,Shared frailty model,Two-part model},
  language = {en},
  number = {4}
}

@article{kaasCOMPOUNDPOISSONDISTRIBUTIONS,
  title = {{{COMPOUND POISSON DISTRIBUTIONS AND GLM}}'{{S}} \textemdash{} {{TWEEDIE}}'{{S DISTRIBUTION}}},
  author = {Kaas, Rob},
  pages = {10},
  abstract = {Generalized Linear Models are especially useful for actuarial applications, since they allow one to estimate multiplicative models, and also allow forms of heteroscedasticity such as they are found frequently in actuarial problems, of Poisson-type, of gamma-type with a fixed coefficient of variation, and in-between (Tweedie's class of Compound Poisson\textendash gamma distributions).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HSRB2XX8\\Kaas - COMPOUND POISSON DISTRIBUTIONS AND GLM’S — TWEEDIE.pdf;C\:\\Users\\devan\\Zotero\\storage\\JW2Q5M72\\Kaas - COMPOUND POISSON DISTRIBUTIONS AND GLM’S — TWEEDIE.PDF},
  language = {en}
}

@article{Kahle2013,
  title = {Ggmap: {{Spatial Visualization}} with Ggplot2},
  author = {Kahle, David and Wickham, Hadley},
  year = {2013},
  volume = {5},
  pages = {144--161},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SD49XHVL\\Kahle and Wickham - 2013 - ggmap Spatial Visualization with ggplot2.pdf},
  journal = {The R Journal},
  number = {1}
}

@article{kaiContinuationratioModelCategorical2008,
  title = {Continuation-Ratio {{Model}} for {{Categorical Data}}: {{A Gibbs Sampling Approach}}},
  author = {Kai, Wan},
  year = {2008},
  pages = {6},
  abstract = {In this paper we discuss the continuationratio model for ordinal data. This particular type of model is to model the probability of one particular category given the categories proceeding this one. It can be shown that estimation of the continuationratio model parameters can be done efficiently by using the techniques in fitting the binary data models. In this way, one does not have to estimate the cut-point parameters as in the cumulative probability models. A Bayesian approach with the use of Gibbs sampler is adopted in this paper. The adaptive rejection sampling method proposed by Gilks and Wild is used. The adaptive rejection sampling (ARS) algorithm is an efficient and direct method to sample from complicated log-concave densities often found in many Gibbs sampling scheme. We applied this model to analyse data obtained from experiments about quality of telephone connection conducted by British Telecom (BT) Laboratory. The final results are satisfactory.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CKY5N7MS\\Kai - 2008 - Continuation-ratio Model for Categorical Data A G.pdf;C\:\\Users\\devan\\Zotero\\storage\\IZVZKJNP\\Kai - 2008 - Continuation-ratio Model for Categorical Data A G.pdf;C\:\\Users\\devan\\Zotero\\storage\\P8I2B66W\\Kai - 2008 - Continuation-ratio Model for Categorical Data A G.pdf},
  journal = {Hong Kong},
  language = {en}
}

@article{kaplanDecomposingPythagoras2017,
  title = {Decomposing {{Pythagoras}}},
  author = {Kaplan, Edward H. and Rich, Candler},
  year = {2017},
  month = dec,
  volume = {13},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2017-0055},
  abstract = {The Pythagorean win expectancy model developed by Bill James remains one of the most celebrated results in sports analytics. Many have extended the application of this model from its original use in baseball to other sports. Others have shown technical scoring conditions that imply the equivalence of win probability and the Pythagorean model. However, no explanation has been offered for why different sports yield different results beyond ``that's what the data say.'' This article presents a theoretical analysis of the Pythagorean model by first deducing an exact within-team equation relating win percentage to seasonal scoring records, and then reconciling mathematically this result with the Pythagorean model which is cross-sectional across teams in a league. We derive a complete decomposition of the Pythagorean coefficient {$\gamma$} in terms of the exact model, and show that {$\gamma$} captures two key quantities \textendash{} average points per game, and the average margins of victory and defeat \textendash{} that together explain why different sports yield different results. We demonstrate this decomposition using the past decade of seasonal results from MLB baseball, NBA basketball, NFL football, and NHL hockey, and show that the data do reflect the properties deduced in our analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GUL5FF4S\\Kaplan and Rich - 2017 - Decomposing Pythagoras.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {4}
}

@misc{kasanOfficeOfficialsAre2008,
  title = {Off-Ice Officials Are a Fourth Team at Every Game},
  author = {Kasan, Sam},
  year = {2008},
  abstract = {The latest news, analysis and stories from NHL.com, the official site of the National Hockey League},
  file = {C\:\\Users\\devan\\Zotero\\storage\\76LV6NSD\\c-388400.html},
  howpublished = {https://www.nhl.com/news/off-ice-officials-are-a-fourth-team-at-every-game/c-388400},
  journal = {NHL.com},
  language = {en\_CA}
}

@inproceedings{kayAreLightningFires2007,
  title = {Are Lightning Fires Unnatural? {{A}} Comparison of Aboriginal and Lightning Ignition Rates in the {{United States}}},
  booktitle = {Fire in {{Grasslandand Shrubland Ecosystems}}},
  author = {Kay, Charles E},
  year = {2007},
  address = {{Tallahassee, FL}},
  abstract = {It is now widely acknowledged that frequent, low-intensity fires once structured many plant communities. Despite an abundance of ethnographic evidence, however, as well as a growing body of ecological data, many professionals still tend to minimize the importance of aboriginal burning compared to that of lightning-caused fires. Based on fire occurrence data (1970\textendash 2002) provided by the National Interagency Fire Center, I calculated the number of lightning fires/million acres (400,000 ha) per year for every national forest in the United States. Those values range from a low of Ͻ1 lightning-caused fire/400,000 ha per year for eastern deciduous forests, to a high of 158 lightning-caused fires/400,000 ha per year in western pine forests. Those data can then be compared with potential aboriginal ignition rates based on estimates of native populations and the number of fires set by each individual per year. Using the lowest published estimate of native people in the United States and Canada prior to European influences (2 million) and assuming that each individual started only 1 fire per year\textemdash potential aboriginal ignition rates were 2.7\textendash 350 times greater than current lightning ignition rates. Using more realistic estimates of native populations, as well as the number of fires each person started per year, potential aboriginal ignition rates were 270\textendash 35,000 times greater than known lightning ignition rates. Thus, lightning-caused fires may have been largely irrelevant for at least the last 10,000 y. Instead, the dominant ecological force likely has been aboriginal burning.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PBERUA2Y\\Kay - ARE LIGHTNING FIRES UNNATURAL A COMPARISON OF ABO.pdf},
  language = {en}
}

@techreport{keeleyDistributionLightningManCaused1982,
  title = {Distribution of {{Lightning}}- and {{Man}}-{{Caused Wildfires}} in {{California}}},
  author = {Keeley, Jon E},
  year = {1982},
  pages = {7},
  address = {{Berkeley, CA}},
  institution = {{U.S. Department of Agriculture}},
  abstract = {During the 1970 decade on lands under fire jurisdiction by the California Division of Forestry (CDF) and the United States Forest Service (USFS) there were over 100,000 wildfires, 16.2 percent of which were lightning-caused and these accounted for 13.1 percent of all area burned. On USFS land, August is the peak month for lightning fires whereas July is the peak for mancaused fires. On average, lightning fires occur at higher elevations than man-caused fires and this is reflected in differences in the types of vegetation providing fuel for ignition. The number of lightning fires is positively correlated with distance from the coast and latitude whereas the number of man-caused fires is negatively correlated with these two parameters. Correlations between other parameters are presented and the question of "natural" burning patterns is discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CVZBSGCC\\Keeley - Distribution of Lightning- and Man-Caused Wildfire.pdf},
  language = {en},
  type = {{{PSW}}-58}
}

@article{keeleyReexaminingFireSuppression1999,
  title = {Reexamining {{Fire Suppression Impacts}} on {{Brushland Fire Regimes}}},
  author = {Keeley, J. E.},
  year = {1999},
  month = jun,
  volume = {284},
  pages = {1829--1832},
  issn = {00368075, 10959203},
  doi = {10.1126/science.284.5421.1829},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UN5F7MCB\\Keeley - 1999 - Reexamining Fire Suppression Impacts on Brushland .pdf;C\:\\Users\\devan\\Zotero\\storage\\VQCDRRTP\\Keeley - 1999 - Reexamining Fire Suppression Impacts on Brushland .pdf},
  journal = {Science},
  language = {en},
  number = {5421}
}

@article{keenshawSituationalInfluencesAcceptable2004,
  title = {Situational {{Influences}} of {{Acceptable Wildland Fire Management Actions}}},
  author = {Keenshaw, Katie and Vaske, Jerry J. and Bright, Alan D. and Absher, James D.},
  year = {2004},
  month = jul,
  volume = {17},
  pages = {477--489},
  issn = {0894-1920},
  doi = {10.1080/08941920490452427},
  abstract = {This article examines the effect of fire-specific situational factors on forest users' normative beliefs about wildland fire management. The acceptability of three fire management actions for eight scenarios was examined. The scenarios varied five factors: (1) fire origin, (2) air quality impact, (3) risk of private property damage, (4) forest recovery, and (5) outdoor recreation impact. Data were obtained from a mail survey of visitors to three national forests: (1) Arapaho\textendash Roosevelt, Colorado (n = 469), (2) Mt. Baker\textendash Snoqualmie, Washington (n = 498), and (3) San Bernardino, California (n = 321). Conjoint analyses indicated varying levels of the five factors (e.g., fire started by humans or lightning) differentially affected acceptability ratings of management actions. Similar percentages of importance were attributed to four of the factors for decisions regarding ``put the fire out'' and ``contain the fire.'' There was more dispersion in the relative importance of factors for ``letting the fire burn.''},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BU5SHCYK\\KNEESHAW et al. - 2004 - Situational Influences of Acceptable Wildland Fire.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZS9STKTA\\08941920490452427.html},
  journal = {Society \& Natural Resources},
  keywords = {acceptability norms,fire management,national forests,situational factors},
  number = {6}
}

@article{keffordSalinityToleranceRiverine2007,
  title = {Salinity Tolerance of Riverine Microinvertebrates from the Southern {{Murray}}\textendash{{Darling Basin}}},
  author = {Kefford, Ben J. and Fields, Elizabeth J. and Clay, Colin and Nugegoda, Dayanthi},
  year = {2007},
  month = dec,
  volume = {58},
  pages = {1019--1031},
  issn = {1448-6059},
  doi = {10.1071/MF06046},
  abstract = {Concern about the effect of rising salinity on freshwater biodiversity has led to studies investigating the salt tolerance of macroinvertebrates and fish, with less attention given to microinvertebrates. We investigated the acute lethal effects of salinity on 12 microinvertebrate species from rivers in the southern Murray\textendash Darling Basin in central Victoria, Australia. For a subset of these species, sub-lethal salinity effects and the effect of water temperature on salinity tolerance were also investigated. The most sensitive microinvertebrates had broadly similar 72-h LC50 values to the most sensitive macroinvertebrates, reported in other studies. However, the most tolerant microinvertebrates tested were much more sensitive than the most tolerant macroinvertebrates and the microinvertebrates studied were more sensitive than most freshwater fish. Temperature affected the acute lethal toxicity of salinity but only to a small degree. In three of four species (the exception being Hydra viridissima), the effects of salinity on growth, development and/or reproduction at concentrations below their 72-h LC50 values were observed. However, different endpoints responded differently to salinity. The demonstrated effect of salinity on microinvertebrates has the potential to indirectly affect fish and salt-tolerant macroinvertebrates via changes to their prey species or ecological functions performed by microinvertebrates.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LWJLPT5K\\mf06046.html},
  journal = {Marine and Freshwater Research},
  language = {en},
  number = {11}
}

@article{kelsallKernelEstimationRelative1995,
  title = {Kernel {{Estimation}} of {{Relative Risk}}},
  author = {Kelsall, Julia E. and Diggle, Peter J.},
  year = {1995},
  month = mar,
  volume = {1},
  pages = {3},
  issn = {13507265},
  doi = {10.2307/3318678},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VBNNWBC7\\Kelsall and Diggle - 1995 - Kernel Estimation of Relative Risk.pdf},
  journal = {Bernoulli},
  language = {en},
  number = {1/2}
}

@article{kimeldorfCorrespondenceBayesianEstimation1970,
  title = {A {{Correspondence Between Bayesian Estimation}} on {{Stochastic Processes}} and {{Smoothing}} by {{Splines}}},
  author = {Kimeldorf, George S. and Wahba, Grace},
  year = {1970},
  month = apr,
  volume = {41},
  pages = {495--502},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177697089},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EHWKUKCJ\\Kimeldorf and Wahba - 1970 - A Correspondence Between Bayesian Estimation on St.pdf;C\:\\Users\\devan\\Zotero\\storage\\D3MTVV72\\1177697089.html},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR254999},
  number = {2},
  zmnumber = {0193.45201}
}

@article{kingBayesianApproachFitting2012,
  title = {A {{Bayesian Approach}} to {{Fitting Gibbs Processes}} with {{Temporal Random Effects}}},
  author = {King, Ruth and Illian, Janine B. and King, Stuart E. and Nightingale, Glenna F. and Hendrichsen, Ditte K.},
  year = {2012},
  month = dec,
  volume = {17},
  pages = {601--622},
  issn = {1085-7117, 1537-2693},
  doi = {10.1007/s13253-012-0111-0},
  abstract = {We consider spatial point pattern data that have been observed repeatedly over a period of time in an inhomogeneous environment. Each spatial point pattern can be regarded as a ``snapshot'' of the underlying point process at a series of times. Thus, the number of points and corresponding locations of points differ for each snapshot. Each snapshot can be analysed independently, but in many cases, there may be little information in the data relating to model parameters, particularly parameters relating to the interaction between points. Thus, we develop an integrated approach, simultaneously analysing all snapshots within a single robust and consistent analysis. We assume that sufficient time has passed between observation dates so that the spatial point patterns can be regarded as independent replicates, given spatial covariates. We develop a joint mixed effects Gibbs point process model for the replicates of spatial point patterns by considering environmental covariates in the analysis as fixed effects, to model the heterogeneous environment, with a random effect (or hierarchical) component to account for the different observation days for the intensity function. We demonstrate how the model can be fitted within a Bayesian framework using an auxiliary variable approach to deal with the issue of the random effects component. We apply the methods to a dataset of muskoxen herds and demonstrate the increased precision of the parameter estimates when considering all available data within a single integrated analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4BTFG3HF\\King et al. - 2012 - A Bayesian Approach to Fitting Gibbs Processes wit.pdf;C\:\\Users\\devan\\Zotero\\storage\\D93IIBT5\\King et al. - 2012 - A Bayesian Approach to Fitting Gibbs Processes wit.pdf},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  language = {en},
  number = {4}
}

@article{kleiberVisualizingCountData2016,
  title = {Visualizing {{Count Data Regressions Using Rootograms}}},
  author = {Kleiber, Christian and Zeileis, Achim},
  year = {2016},
  month = jul,
  volume = {70},
  pages = {296--303},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1173590},
  abstract = {The rootogram is a graphical tool associated with the work of J. W. Tukey that was originally used for assessing goodness of fit of univariate distributions. Here, we extend the rootogram to regression models and show that this is particularly useful for diagnosing and treating issues such as overdispersion and/or excess zeros in count data models. We also introduce a weighted version of the rootogram that can be applied out of sample or to (weighted) subsets of the data, for example, in finite mixture models. An empirical illustration revisiting a well-known dataset from ethology is included, for which a negative binomial hurdle model is employed. Supplementary materials providing two further illustrations are available online: the first, using data from public health, employs a two-component finite mixture of negative binomial models; the second, using data from finance, involves underdispersion. An R implementation of our tools is available in the R package countreg. It also contains the data and replication code.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LWHDTA3I\\Kleiber and Zeileis - 2016 - Visualizing Count Data Regressions Using Rootogram.pdf;C\:\\Users\\devan\\Zotero\\storage\\TV5GRNA9\\Kleiber and Zeileis - 2016 - Visualizing Count Data Regressions Using Rootogram.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {3}
}

@book{klugman2012loss,
  title = {Loss {{Models}}: {{From Data}} to {{Decisions}}},
  author = {Klugman, S.A. and Panjer, H.H. and Willmot, G.E.},
  year = {2012},
  publisher = {{Wiley}},
  isbn = {978-1-118-41165-0},
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}}
}

@article{knorr-heldSharedComponentModel2001,
  title = {A Shared Component Model for Detecting Joint and Selective Clustering of Two Diseases},
  author = {Knorr-Held, Leonhard and Best, Nicola G.},
  year = {2001},
  month = jan,
  volume = {164},
  pages = {73--85},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/1467-985X.00187},
  abstract = {The study of spatial variations in disease rates is a common epidemiological approach used to describe the geographical clustering of diseases and to generate hypotheses about the possible `causes' which could explain apparent differences in risk. Recent statistical and computational developments have led to the use of realistically complex models to account for overdispersion and spatial correlation. However, these developments have focused almost exclusively on spatial modelling of a single disease. Many diseases share common risk factors (smoking being an obvious example) and, if similar patterns of geographical variation of related diseases can be identi\textregistered ed, this may provide more convincing evidence of real clustering in the underlying risk surface. We propose a shared component model for the joint spatial analysis of two diseases. The key idea is to separate the underlying risk surface for each disease into a shared and a disease-speci\textregistered c component. The various components of this formulation are modelled simultaneously by using spatial cluster models implemented via reversible jump Markov chain Monte Carlo methods. We illustrate the methodology through an analysis of oral and oesophageal cancer mortality in the 544 districts of Germany, 1986{$\pm$}1990.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D5BRBW8M\\Knorr-Held and Best - 2001 - A Shared Component Model for Detecting Joint and S.pdf;C\:\\Users\\devan\\Zotero\\storage\\UFYN3QIH\\Knorr‐Held and Best - 2001 - A shared component model for detecting joint and s.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZNTVKTYK\\Knorr‐Held and Best - 2001 - A shared component model for detecting joint and s.pdf},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  language = {en},
  number = {1}
}

@article{kourtzPredictingDailyOccurrence,
  title = {Predicting the Daily Occurrence of People-Caused Forest Fires.},
  author = {Kourtz, B ToddandP H},
  pages = {26},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CJ52E4ZL\\Kourtz - Predicting the daily occurrence of people-caused f.pdf}
}

@techreport{kourtzPredictingDailyOccurrence1991,
  title = {Predicting the {{Daily Occurrence}} of {{Lightning}}-{{Caused Forest Fires}}},
  author = {Kourtz, B and Todd, J. B.},
  year = {1991},
  address = {{Ottawa, ON}},
  institution = {{Forestry Canada}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8IWZJC77\\Kourtz Todd 1991 daily lightning fop.pdf}
}

@article{kovalchikSearchingGOATTennis2016,
  title = {Searching for the {{GOAT}} of Tennis Win Prediction},
  author = {Kovalchik, Stephanie Ann},
  year = {2016},
  month = jan,
  volume = {12},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2015-0059},
  abstract = {Sports forecasting models \textendash{} beyond their interest to bettors \textendash{} are important resources for sports analysts and coaches. Like the best athletes, the best forecasting models should be rigorously tested and judged by how well their performance holds up against top competitors. Although a number of models have been proposed for predicting match outcomes in professional tennis, their comparative performance is largely unknown. The present paper tests the predictive performance of 11 published forecasting models for predicting the outcomes of 2395 singles matches during the 2014 season of the Association of Tennis Professionals Tour. The evaluated models fall into three categories: regressionbased, point-based, and paired comparison models. Bookmaker predictions were used as a performance benchmark. Using only 1 year of prior performance data, regression models based on player ranking and an Elo approach developed by FiveThirtyEight were the most accurate approaches. The FiveThirtyEight model predictions had an accuracy of 75\% for matches of the most highly-ranked players, which was competitive with the bookmakers. The inclusion of careerto-date improved the FiveThirtyEight model predictions for lower-ranked players (from 59\% to 64\%) but did not change the performance for higher-ranked players. All models were 10\textendash 20 percentage points less accurate at predicting match outcomes among lower-ranked players than matches with the top players in the sport. The gap in performance according to player ranking and the simplicity of the information used in Elo ratings highlight directions for further model development that could improve the practical utility and generalizability of forecasting in tennis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JPLQJRBB\\Kovalchik - 2016 - Searching for the GOAT of tennis win prediction.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {3}
}

@article{krainskiRINLATutorialSPDE,
  title = {The {{R}}-{{INLA}} Tutorial on {{SPDE}} Models},
  author = {Krainski, Elias T and Lindgren, Finn and Simpson, Daniel},
  pages = {137},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QXUX6KKD\\Krainski et al. - The R-INLA tutorial on SPDE models.pdf},
  language = {en}
}

@article{krawchukGlobalPyrogeographyCurrent2009,
  title = {Global {{Pyrogeography}}: The {{Current}} and {{Future Distribution}} of {{Wildfire}}},
  shorttitle = {Global {{Pyrogeography}}},
  author = {Krawchuk, Meg A. and Moritz, Max A. and Parisien, Marc-Andr{\'e} and Dorn, Jeff Van and Hayhoe, Katharine},
  year = {2009},
  month = apr,
  volume = {4},
  pages = {e5102},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005102},
  abstract = {Climate change is expected to alter the geographic distribution of wildfire, a complex abiotic process that responds to a variety of spatial and environmental gradients. How future climate change may alter global wildfire activity, however, is still largely unknown. As a first step to quantifying potential change in global wildfire, we present a multivariate quantification of environmental drivers for the observed, current distribution of vegetation fires using statistical models of the relationship between fire activity and resources to burn, climate conditions, human influence, and lightning flash rates at a coarse spatiotemporal resolution (100 km, over one decade). We then demonstrate how these statistical models can be used to project future changes in global fire patterns, highlighting regional hotspots of change in fire probabilities under future climate conditions as simulated by a global climate model. Based on current conditions, our results illustrate how the availability of resources to burn and climate conditions conducive to combustion jointly determine why some parts of the world are fire-prone and others are fire-free. In contrast to any expectation that global warming should necessarily result in more fire, we find that regional increases in fire probabilities may be counter-balanced by decreases at other locations, due to the interplay of temperature and precipitation variables. Despite this net balance, our models predict substantial invasion and retreat of fire across large portions of the globe. These changes could have important effects on terrestrial ecosystems since alteration in fire activity may occur quite rapidly, generating ever more complex environmental challenges for species dispersing and adjusting to new climate conditions. Our findings highlight the potential for widespread impacts of climate change on wildfire, suggesting severely altered fire regimes and the need for more explicit inclusion of fire in research on global vegetation-climate change dynamics and conservation planning.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\747FV3AD\\Krawchuk et al. - 2009 - Global Pyrogeography the Current and Future Distr.pdf;C\:\\Users\\devan\\Zotero\\storage\\44ICFYHI\\article.html},
  journal = {PLOS ONE},
  keywords = {Climate change,Ecosystems,Fire engineering,Fire suppression technology,Productivity (ecology),Simulation and modeling,Statistical models,Wildfires},
  language = {en},
  number = {4}
}

@article{kriderLightningDirectionFindingSystems1980,
  title = {Lightning {{Direction}}-{{Finding Systems}} for {{Forest Fire Detection}}},
  author = {Krider, E. P. and Noggle, R. C. and Pifer, A. E. and Vance, D. L.},
  year = {1980},
  month = sep,
  volume = {61},
  pages = {980--986},
  issn = {0003-0007},
  doi = {10.1175/1520-0477(1980)061<0980:LDFSFF>2.0.CO;2},
  abstract = {Extensive networks of magnetic direction-finding (DF) stations have been installed throughout the western United States and Alaska to facilitate early detection of lightning-caused fires. Each station contains a new wideband direction-finder that responds primarily to cloud-to-ground lightning and discriminates against cloud discharges and background noise. Good angle accuracy is obtained by measuring the lightning direction at just the time the return-stroke electromagnetic field reaches its initial peak. Lightning locations are calculated from the intersections of direction vectors and/or from the ratio of signal strengths recorded simultaneously at two, three, or four DF sites. The development of these systems has proved to be a significant aid in the detection of lightning-caused fires and in fire weather forecasting.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C9VFCAHJ\\Krider et al. - 1980 - Lightning Direction-Finding Systems for Forest Fir.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZWG7UYKV\\1520-0477(1980)0610980LDFSFF2.0.html},
  journal = {Bulletin of the American Meteorological Society},
  number = {9}
}

@article{krolTutorialJointModeling2017,
  title = {Tutorial in {{Joint Modeling}} and {{Prediction}}: A {{Statistical Software}} for {{Correlated Longitudinal Outcomes}}, {{Recurrent Events}} and a {{Terminal Event}}},
  shorttitle = {Tutorial in {{Joint Modeling}} and {{Prediction}}},
  author = {Kr{\'o}l, Agnieszka and Mauguen, Audrey and Mazroui, Yassin and Laurent, Alexandre and Michiels, Stefan and Rondeau, Virginie},
  year = {2017},
  month = jan,
  doi = {10.18637/jss.v081.i03},
  abstract = {Extensions in the field of joint modeling of correlated data and dynamic predictions improve the development of prognosis research. The R package frailtypack provides estimations of various joint models for longitudinal data and survival events. In particular, it fits models for recurrent events and a terminal event (frailtyPenal), models for two survival outcomes for clustered data (frailtyPenal), models for two types of recurrent events and a terminal event (multivPenal), models for a longitudinal biomarker and a terminal event (longiPenal) and models for a longitudinal biomarker, recurrent events and a terminal event (trivPenal). The estimators are obtained using a standard and penalized maximum likelihood approach, each model function allows to evaluate goodness-of-fit analyses and plots of baseline hazard functions. Finally, the package provides individual dynamic predictions of the terminal event and evaluation of predictive accuracy. This paper presents theoretical models with estimation techniques, applies the methods for predictions and illustrates frailtypack functions details with examples.},
  archivePrefix = {arXiv},
  eprint = {1701.03675},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\87VY8E43\\Król et al. - 2017 - Tutorial in Joint Modeling and Prediction a Stati.pdf;C\:\\Users\\devan\\Zotero\\storage\\MPLQVTJN\\Król et al. - 2017 - Tutorial in Joint Modeling and Prediction a Stati.pdf},
  journal = {arXiv:1701.03675 [stat]},
  keywords = {Statistics - Computation},
  language = {en},
  primaryClass = {stat}
}

@incollection{kruschkeBibliography2015,
  title = {Bibliography},
  booktitle = {Doing {{Bayesian Data Analysis}} ({{Second Edition}})},
  editor = {Kruschke, John K.},
  year = {2015},
  month = jan,
  pages = {737--745},
  publisher = {{Academic Press}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-405888-0.10000-5},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CISZEZ9D\\B9780124058880100005.html},
  isbn = {978-0-12-405888-0}
}

@book{kruschkeDoingBayesianData2015,
  title = {Doing {{Bayesian Data Analysis}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}},
  author = {Kruschke, John K.},
  year = {2015},
  edition = {Second},
  publisher = {{Academic Press}},
  address = {{Boston}},
  isbn = {978-0-12-405888-0}
}

@incollection{kruschkeFrontMatter2015,
  title = {Front {{Matter}}},
  booktitle = {Doing {{Bayesian Data Analysis}} ({{Second Edition}})},
  editor = {Kruschke, John K.},
  year = {2015},
  month = jan,
  pages = {i-ii},
  publisher = {{Academic Press}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-405888-0.09999-2},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BUBTBHNA\\Kruschke - 2015 - Front Matter.pdf;C\:\\Users\\devan\\Zotero\\storage\\ENR5Q765\\B9780124058880099992.html},
  isbn = {978-0-12-405888-0}
}

@article{kruschkeRejectingAcceptingParameter,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K},
  pages = {11},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6Y4MIFNF\\Kruschke - Rejecting or Accepting Parameter Values in Bayesia.pdf;C\:\\Users\\devan\\Zotero\\storage\\YPXEPZN5\\Kruschke - Rejecting or Accepting Parameter Values in Bayesia.pdf},
  language = {en}
}

@misc{krzywickiNHLShotQuality2010,
  title = {{{NHL Shot Quality}} 2009-10: {{A}} Look at Shot Angles and Rebounds},
  author = {Krzywicki, Ken},
  year = {2010},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HSWRBQ6X\\SQ-RS0910-Krzywicki.pdf},
  howpublished = {http://hockeyanalytics.com/Research\_files/SQ-RS0910-Krzywicki.pdf},
  journal = {Hockey Analytics}
}

@misc{krzywickiShotQualityModel2005,
  title = {Shot {{Quality Model}}: {{A}} Logistic Regression Approach to Assessing {{NHL}} Shots on Goal},
  author = {Krzywicki, Ken},
  year = {2005},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SCTWMNLM\\Shot_Quality_Krzywicki.pdf},
  howpublished = {http://hockeyanalytics.com/Research\_files/Shot\_Quality\_Krzywicki.pdf},
  journal = {Hockey Analytics}
}

@article{kuoVariableSelectionRegression1998,
  title = {Variable {{Selection}} for {{Regression Models}}},
  author = {Kuo, Lynn and Mallick, Bani},
  year = {1998},
  volume = {60},
  pages = {65--81},
  abstract = {A simple method for subset selection of independent variables in regression models is proposed. We expand the usual regression equation to an equation that incorporates all possible subsets of predictors by adding indicator variables as parameters. The vector of indicator variables dictates which predictors to include. Several choices of priors can be employed for the unknown regression coe cients and the unknown indicator parameters. The posterior distribution of the indicator vector is approximated by means of the Markov chain Monte Carlo algorithm. We select subsets with high posterior probabilities. In addition to linear models, we consider generalized linear models.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BWJFD388\\Kuo and Mallick - Variable Selection for Regression Models.pdf},
  journal = {Sankhya B},
  language = {en}
}

@article{langBayesianPSplines2004,
  title = {Bayesian {{P}}-{{Splines}}},
  author = {Lang, Stefan and Brezger, Andreas},
  year = {2004},
  month = mar,
  volume = {13},
  pages = {183--212},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/1061860043010},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZHT6SE63\\Lang and Brezger - 2004 - Bayesian P-Splines.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {1}
}

@article{laubHawkesProcesses2015,
  title = {Hawkes {{Processes}}},
  author = {Laub, Patrick J. and Taimre, Thomas and Pollett, Philip K.},
  year = {2015},
  month = jul,
  abstract = {Hawkes processes are a particularly interesting class of stochastic process that have been applied in diverse areas, from earthquake modelling to financial analysis. They are point processes whose defining characteristic is that they `self-excite', meaning that each arrival increases the rate of future arrivals for some period of time. Hawkes processes are well established, particularly within the financial literature, yet many of the treatments are inaccessible to one not acquainted with the topic. This survey provides background, introduces the field and historical developments, and touches upon all major aspects of Hawkes processes.},
  archivePrefix = {arXiv},
  eprint = {1507.02822},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7JDN4P2X\\Laub et al. - 2015 - Hawkes Processes.pdf;C\:\\Users\\devan\\Zotero\\storage\\YVHRERNR\\Laub et al. - 2015 - Hawkes Processes.pdf},
  journal = {arXiv:1507.02822 [math, q-fin, stat]},
  keywords = {Mathematics - Probability,Quantitative Finance - Statistical Finance,Statistics - Applications},
  language = {en},
  primaryClass = {math, q-fin, stat}
}

@article{lawEcologicalInformationSpatial2009,
  title = {Ecological Information from Spatial Patterns of Plants: Insights from Point Process Theory},
  shorttitle = {Ecological Information from Spatial Patterns of Plants},
  author = {Law, Richard and Illian, Janine and Burslem, David F. R. P. and Gratzer, Georg and Gunatilleke, C. V. S. and Gunatilleke, I. A. U. N.},
  year = {2009},
  month = jul,
  volume = {97},
  pages = {616--628},
  issn = {00220477, 13652745},
  doi = {10.1111/j.1365-2745.2009.01510.x},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IQC2JIGS\\Law et al. - 2009 - Ecological information from spatial patterns of pl.pdf},
  journal = {Journal of Ecology},
  language = {en},
  number = {4}
}

@article{lawEffectsMidpointImputation1992,
  title = {Effects of Mid-Point Imputation on the Analysis of Doubly Censored Data},
  author = {Law, C. G. and Brookmeyer, R.},
  year = {1992},
  month = sep,
  volume = {11},
  pages = {1569--1578},
  issn = {0277-6715},
  abstract = {Doubly censored data arise in some cohort studies of the AIDS incubation period because the time of infection may be known only up to an interval defined by two successive screening tests for HIV antibody. A simple analytic approach is to impute the infection time by the mid-point of the interval and then apply standard survival techniques for right censored data. The objective of this paper is to investigate the statistical properties of such a mid-point imputation approach. We investigated the asymptotic bias of the Kaplan-Meier estimate, coverage probabilities of associated confidence intervals, bias in hazard ratio, and the size of the logrank test. We show that the statistical properties of mid-point imputation depend strongly on the underlying distributions of infection times and the incubation periods, and the width of the interval between screening tests. In the absence of treatment, the median incubation period of HIV infection is approximately 10 years, and we conclude that, for this situation, mid-point imputation is a reasonable procedure for interval widths of 2 years or less.},
  journal = {Statistics in Medicine},
  keywords = {Bias,Confidence Intervals,Data Interpretation; Statistical,Effect Modifier; Epidemiologic,HIV Infections,HIV-1,Logistic Models,Mass Screening,Proportional Hazards Models,Survival Analysis,Time Factors},
  language = {eng},
  number = {12},
  pmid = {1439361}
}

@article{lawlessNegativeBinomialMixed1987,
  title = {Negative Binomial and Mixed Poisson Regression},
  author = {Lawless, Jerald F.},
  year = {1987},
  month = sep,
  volume = {15},
  pages = {209--225},
  issn = {03195724, 1708945X},
  doi = {10.2307/3314912},
  abstract = {A number of methods have been proposed for dealing with extra-Poisson variation doing regression analysis of count data. This paper studies negative-binomial regre models and examines efficiency and robustness properties of inference procedures bas them. The methods are compared with quasilikelihood methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\895LKBLY\\Lawless - 1987 - Negative binomial and mixed poisson regression.pdf},
  journal = {Canadian Journal of Statistics},
  language = {en},
  number = {3}
}

@article{ledoLianasSoilNutrients2016,
  title = {Lianas and Soil Nutrients Predict Fine-Scale Distribution of above-Ground Biomass in a Tropical Moist Forest},
  author = {Ledo, Alicia and Illian, Janine B. and Schnitzer, Stefan A. and Wright, S. Joseph and Dalling, James W. and Burslem, David F. R. P.},
  year = {2016},
  volume = {104},
  pages = {1819--1828},
  issn = {1365-2745},
  doi = {10.1111/1365-2745.12635},
  abstract = {Prediction of carbon dynamics in response to global climate change requires an understanding of the processes that govern the distribution of carbon stocks. Above-ground biomass (AGB) in tropical forests is regulated by variation in soil fertility, climate, species composition and topography at regional scales, but the drivers of fine-scale variation in tropical forest AGB are poorly understood. The factors that control the growth and mortality of individual trees may be obscured by the low resolution of studies at regional scales. In this paper, we evaluated the effects of soil nutrients, topography and liana abundance on the fine-scale spatial distribution of AGB and density of trees for a lowland tropical moist forest in Panama using additive regression models. Areas with larger values of AGB were negatively associated with the presence of lianas, which may reflect competition with lianas and/or the association of lianas with disturbed or open-canopy patches within forests. AGB was positively associated with soils possessing higher pH and K concentrations, reflecting the importance of below-ground resource availability on AGB independently of stem density. Synthesis. Our results shed new light on the factors that influence fine-scale tree AGB and carbon stocks in tropical forests: liana abundance is the strongest predictor, having a negative impact on tree AGB. The availability of soil nutrients was also revealed as an important driver of fine-scale spatial variation in tree AGB.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3G8834GU\\Ledo et al. - 2016 - Lianas and soil nutrients predict fine-scale distr.pdf;C\:\\Users\\devan\\Zotero\\storage\\WFKXPZSE\\1365-2745.html},
  journal = {Journal of Ecology},
  keywords = {above-ground biomass spatial distribution,carbon dynamics,carbon stocks,INLA approach,liana,resource competition,soil nutrients,spatial statistics},
  language = {en},
  number = {6}
}

@inproceedings{leeAlgorithmsNonnegativeMatrix2000,
  title = {Algorithms for {{Non}}-Negative {{Matrix Factorization}}},
  booktitle = {13th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lee, Daniel D and Seung, H Sebastian},
  year = {2000},
  publisher = {{MIT Press}},
  address = {{Denver, Colorado}},
  abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JZDG55E3\\Lee and Seung - Algorithms for Non-negative Matrix Factorization.pdf},
  language = {en}
}

@article{leeLearningPartsObjects1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {788--791},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44565},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2CBZ7XWI\\Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf;C\:\\Users\\devan\\Zotero\\storage\\JF6FS3TL\\Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf},
  journal = {Nature},
  language = {en},
  number = {6755}
}

@article{leeLocalEMAlgorithmSpatiotemporal2017,
  title = {A Local-{{EM}} Algorithm for Spatio-Temporal Disease Mapping with Aggregated Data},
  author = {Lee, Jonathan S.W. and Nguyen, Paul and Brown, Patrick E. and Stafford, Jamie and {Saint-Jacques}, Nathalie},
  year = {2017},
  month = aug,
  volume = {21},
  pages = {75--95},
  issn = {22116753},
  doi = {10.1016/j.spasta.2017.05.001},
  abstract = {Spatial data on disease incidence locations are often aggregated to regional counts to preserve privacy, and spatio-temporal modelling of such can be problematic when there are boundary changes over the study period. Here an inhomogeneous Poisson process with intensity depending on variations in population (known a priori) and a smoothly varying relative risk is estimated with a local-Expectation\textendash Maximization (or local-EM) algorithm. Using incidence data for male bladder cancer in Nova Scotia, Canada, the question of whether the data are consistent with spatially varying but temporally constant relative risk is examined. Areas where there is evidence that relative risk is substantially greater than 1 are identified with the intention of assessing the possible presence of environmental risk factors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9873QI5B\\Lee et al. - 2017 - A local-EM algorithm for spatio-temporal disease m.pdf;C\:\\Users\\devan\\Zotero\\storage\\9F6B63CG\\Lee et al. - 2017 - A local-EM algorithm for spatio-temporal disease m.pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@article{leiningerBayesianInferenceModel2017,
  title = {Bayesian {{Inference}} and {{Model Assessment}} for {{Spatial Point Patterns Using Posterior Predictive Samples}}},
  author = {Leininger, Thomas J. and Gelfand, Alan E.},
  year = {2017},
  month = mar,
  volume = {12},
  pages = {1--30},
  issn = {1936-0975},
  doi = {10.1214/15-BA985},
  abstract = {Spatial point pattern data describes locations of events observed over a given domain, with the number of and locations of these events being random. Historically, data analysis for spatial point patterns has focused on rejecting complete spatial randomness and then on fitting a richer model specification. From a Bayesian standpoint, the literature is growing but primarily considers versions of Poisson processes, focusing on specifications for the intensity. However, the Bayesian literature on, e.g., clustering or inhibition processes is limited, primarily attending to model fitting. There is little attention given to full inference and scant with regard to model adequacy or model comparison.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KCKI3C7K\\Leininger and Gelfand - 2017 - Bayesian Inference and Model Assessment for Spatia.pdf},
  journal = {Bayesian Analysis},
  language = {en},
  number = {1}
}

@article{leleEstimabilityLikelihoodInference2010,
  title = {Estimability and {{Likelihood Inference}} for {{Generalized Linear Mixed Models Using Data Cloning}}},
  author = {Lele, Subhash R. and Nadeem, Khurram and Schmuland, Byron},
  year = {2010},
  month = dec,
  volume = {105},
  pages = {1617--1625},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2010.tm09757},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F6YTSCUI\\Lele et al. - 2010 - Estimability and Likelihood Inference for Generali.pdf;C\:\\Users\\devan\\Zotero\\storage\\NPPW7Z4F\\Lele et al. - 2010 - Estimability and Likelihood Inference for Generali.pdf;C\:\\Users\\devan\\Zotero\\storage\\W3G449F2\\Lele et al. - 2010 - Estimability and Likelihood Inference for Generali.pdf;C\:\\Users\\devan\\Zotero\\storage\\X2IS8TKF\\Lele et al. - 2010 - Estimability and Likelihood Inference for Generali.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {492}
}

@article{lemeyBayesianPhylogeographyFinds2009,
  title = {Bayesian {{Phylogeography Finds Its Roots}}},
  author = {Lemey, Philippe and Rambaut, Andrew and Drummond, Alexei J. and Suchard, Marc A.},
  year = {2009},
  month = sep,
  volume = {5},
  pages = {e1000520},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000520},
  abstract = {As a key factor in endemic and epidemic dynamics, the geographical distribution of viruses has been frequently interpreted in the light of their genetic histories. Unfortunately, inference of historical dispersal or migration patterns of viruses has mainly been restricted to model-free heuristic approaches that provide little insight into the temporal setting of the spatial dynamics. The introduction of probabilistic models of evolution, however, offers unique opportunities to engage in this statistical endeavor. Here we introduce a Bayesian framework for inference, visualization and hypothesis testing of phylogeographic history. By implementing character mapping in a Bayesian software that samples time-scaled phylogenies, we enable the reconstruction of timed viral dispersal patterns while accommodating phylogenetic uncertainty. Standard Markov model inference is extended with a stochastic search variable selection procedure that identifies the parsimonious descriptions of the diffusion process. In addition, we propose priors that can incorporate geographical sampling distributions or characterize alternative hypotheses about the spatial dynamics. To visualize the spatial and temporal information, we summarize inferences using virtual globe software. We describe how Bayesian phylogeography compares with previous parsimony analysis in the investigation of the influenza A H5N1 origin and H5N1 epidemiological linkage among sampling localities. Analysis of rabies in West African dog populations reveals how virus diffusion may enable endemic maintenance through continuous epidemic cycles. From these analyses, we conclude that our phylogeographic framework will make an important asset in molecular epidemiology that can be easily generalized to infer biogeogeography from genetic data for many organisms.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9D8GUWSD\\Lemey et al. - 2009 - Bayesian Phylogeography Finds Its Roots.pdf;C\:\\Users\\devan\\Zotero\\storage\\CJ3JP48P\\article.html},
  journal = {PLOS Computational Biology},
  keywords = {Animal phylogenetics,Avian influenza,Phylogenetic analysis,Phylogenetics,Phylogeography,Rabies virus,Trees,Viral evolution},
  language = {en},
  number = {9}
}

@article{lewisEstimatingBayesFactors1997,
  title = {Estimating {{Bayes Factors}} via {{Posterior Simulation}} with the {{Laplace}}\textemdash{{Metropolis Estimator}}},
  author = {Lewis, Steven M. and Raftery, Adrian E.},
  year = {1997},
  month = jun,
  volume = {92},
  pages = {648--655},
  issn = {0162-1459},
  doi = {10.1080/01621459.1997.10474016},
  abstract = {The key quantity needed for Bayesian hypothesis testing and model selection is the integrated, or marginal, likelihood of a model. We describe a way to use posterior simulation output to estimate integrated likelihoods. We describe the basic Laplace\textemdash Metropolis estimator for models without random effects. For models with random effects, we introduce the compound Laplace-Metropolis estimator. We apply this estimator to data from the World Fertility Survey and show it to give accurate results. Batching of simulation output is used to assess the uncertainty involved in using the compound Laplace-Metropolis estimator. The method allows us to test for the effects of independent variables in a random-effects model and also to test for the presence of the random effects.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GJAAQR72\\Lewis and Raftery - 1997 - Estimating Bayes Factors via Posterior Simulation .pdf;C\:\\Users\\devan\\Zotero\\storage\\RURH3982\\01621459.1997.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Compound Laplace—Metropolis estimator,Integrated likelihood,Marginal likelihood,Markov chain Monte Carlo,Random-effects model,World Fertility Survey},
  number = {438}
}

@article{liLogGaussianCox2012,
  title = {Log {{Gaussian Cox}} Processes and Spatially Aggregated Disease Incidence Data},
  author = {Li, Ye and Brown, Patrick and Gesink, Dionne C and Rue, H{\aa}vard},
  year = {2012},
  month = oct,
  volume = {21},
  pages = {479--507},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280212446326},
  abstract = {This article presents a methodology for modeling aggregated disease incidence data with the spatially continuous log-Gaussian Cox process. Statistical models for spatially aggregated disease incidence data usually assign the same relative risk to all individuals in the same reporting region (census areas or postal regions). A further assumption that the relative risks in two regions are independent given their neighbor's risks (the Markov assumption) makes the commonly used Besag\textendash York\textendash Mollie\textasciiacute{} model computationally simple. The continuous model proposed here uses a data augmentation step to sample from the posterior distribution of the exact locations at each step of an Markov chain Monte Carlo algorithm, and models the exact locations with an log-Gaussian Cox process. A simulation study shows the logGaussian Cox process model consistently outperforming the Besag\textendash York\textendash Mollie\textasciiacute{} model. The method is illustrated by making inference on the spatial distribution of syphilis risk in North Carolina. The effect of several known social risk factors are estimated, and areas with risk well in excess of that expected given these risk factors are identified.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2QU35ZNV\\Li et al. - 2012 - Log Gaussian Cox processes and spatially aggregate.pdf;C\:\\Users\\devan\\Zotero\\storage\\EQP3ILMA\\Li et al. - 2012 - Log Gaussian Cox processes and spatially aggregate.pdf;C\:\\Users\\devan\\Zotero\\storage\\V48T2JM4\\Li et al. - 2012 - Log Gaussian Cox processes and spatially aggregate.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {5}
}

@article{lindgrenBayesianSpatialModelling2015,
  title = {Bayesian {{Spatial Modelling}} with {{R}}-{{INLA}}},
  author = {Lindgren, Finn and Rue, H{\aa}vard},
  year = {2015},
  month = feb,
  volume = {63},
  pages = {1--25},
  issn = {1548-7660},
  doi = {10.18637/jss.v063.i19},
  copyright = {Copyright (c) 2013 Finn Lindgren, H\aa vard Rue},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZYYASWUR\\Lindgren and Rue - 2015 - Bayesian Spatial Modelling with R-INLA.pdf;C\:\\Users\\devan\\Zotero\\storage\\7VUVZP9Z\\v063i19.html},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {1}
}

@article{lindgrenBayesianSpatialModelling2015a,
  title = {Bayesian {{Spatial Modelling}} with {{{\emph{R}}}} - {{{\textbf{INLA}}}}},
  author = {Lindgren, Finn and Rue, H{\aa}vard},
  year = {2015},
  volume = {63},
  issn = {1548-7660},
  doi = {10.18637/jss.v063.i19},
  abstract = {The principles behind the interface to continuous domain spatial models in the RINLA software package for R are described. The Integrated Nested Laplace Approximation (INLA) approach proposed by Rue, Martino, and Chopin (2009) is a computationally effective alternative to MCMC for Bayesian inference. INLA is designed for latent Gaussian models, a very wide and flexible class of models ranging from (generalized) linear mixed to spatial and spatio-temporal models. Combined with the Stochastic Partial Differential Equation approach (SPDE, Lindgren and Lindgren 2011), one can accommodate all kinds of geographically referenced data, including areal and geostatistical ones, as well as spatial point process data. The implementation interface covers stationary spatial models, non-stationary spatial models, and also spatio-temporal models, and is applicable in epidemiology, ecology, environmental risk assessment, as well as general geostatistics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DA2TSM8P\\Lindgren and Rue - 2015 - Bayesian Spatial Modelling with iRi - bINLA.pdf;C\:\\Users\\devan\\Zotero\\storage\\Q47T5DRY\\Lindgren and Rue - 2015 - Bayesian Spatial Modelling with iRi - bINLA.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {19}
}

@article{lindgrenContinuousDomainSpatial,
  title = {Continuous {{Domain Spatial Models}} In},
  author = {Lindgren, Finn},
  pages = {8},
  file = {C\:\\Users\\devan\\Zotero\\storage\\L9I4YQVQ\\Lindgren - Continuous Domain Spatial Models in.pdf;C\:\\Users\\devan\\Zotero\\storage\\X9P99KPZ\\Lindgren - Continuous Domain Spatial Models in.pdf},
  language = {en}
}

@article{lindgrenExplicitLinkGaussian2011,
  title = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields: The Stochastic Partial Differential Equation Approach: {{Link}} between {{Gaussian Fields}} and {{Gaussian Markov Random Fields}}},
  shorttitle = {An Explicit Link between {{Gaussian}} Fields and {{Gaussian Markov}} Random Fields},
  author = {Lindgren, Finn and Rue, H{\aa}vard and Lindstr{\"o}m, Johan},
  year = {2011},
  month = sep,
  volume = {73},
  pages = {423--498},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2011.00777.x},
  abstract = {Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in R2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Mat\'ern class, provide an explicit link , for any triangulation of Rd , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9EKJB84H\\Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf;C\:\\Users\\devan\\Zotero\\storage\\FHZXYSPR\\Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {4}
}

@article{lindleyStatisticalParadox1957,
  title = {A {{Statistical Paradox}}},
  author = {Lindley, D. V.},
  year = {1957},
  month = jun,
  volume = {44},
  pages = {187--192},
  issn = {0006-3444},
  doi = {10.1093/biomet/44.1-2.187},
  abstract = {D. V. LINDLEY;  A STATISTICAL PARADOX, Biometrika, Volume 44, Issue 1-2, 1 June 1957, Pages 187\textendash 192, https://doi.org/10.1093/biomet/44.1-2.187},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RUVIXDK3\\Lindley - 1957 - A STATISTICAL PARADOX.pdf;C\:\\Users\\devan\\Zotero\\storage\\G3E68SNW\\269805.html},
  journal = {Biometrika},
  language = {en},
  number = {1-2}
}

@misc{linNNLMFastVersatile2019,
  title = {{{NNLM}}: {{Fast}} and {{Versatile Non}}-{{Negative Matrix Factorization}}},
  author = {Lin, Xihui and Boutros, Paul C.},
  year = {2019}
}

@article{littleModelingDropOutMechanism1995,
  title = {Modeling the {{Drop}}-{{Out Mechanism}} in {{Repeated}}-{{Measures Studies}}},
  author = {Little, Roderick J. A.},
  year = {1995},
  month = sep,
  volume = {90},
  pages = {1112--1121},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1995.10476615},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FU6U2TU5\\Little - 1995 - Modeling the Drop-Out Mechanism in Repeated-Measur.pdf;C\:\\Users\\devan\\Zotero\\storage\\WD86QV5N\\Little - 1995 - Modeling the Drop-Out Mechanism in Repeated-Measur.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {431}
}

@article{liuSharedFrailtyModels2004,
  title = {Shared {{Frailty Models}} for {{Recurrent Events}} and a {{Terminal Event}}},
  author = {Liu, Lei and Wolfe, Robert A. and Huang, Xuelin},
  year = {2004},
  month = sep,
  volume = {60},
  pages = {747--756},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2004.00225.x},
  abstract = {There has been an increasing interest in the analysis of recurrent event data (Cook and Lawless, 2002, Statistical Methods in Medical Research 11, 141\textendash 166). In many situations, a terminating event such as death can happen during the follow-up period to preclude further occurrence of the recurrent events. Furthermore, the death time may be dependent on the recurrent event history. In this article we consider frailty proportional hazards models for the recurrent and terminal event processes. The dependence is modeled by conditioning on a shared frailty that is included in both hazard functions. Covariate effects can be taken into account in the model as well. Maximum likelihood estimation and inference are carried out through a Monte Carlo EM algorithm with Metropolis\textendash Hastings sampler in the E-step. An analysis of hospitalization and death data for waitlisted dialysis patients is presented to illustrate the proposed methods. Methods to check the validity of the proposed model are also demonstrated. This model avoids the difficulties encountered in alternative approaches which attempt to specify a dependent joint distribution with marginal proportional hazards and yields an estimate of the degree of dependence.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DTRHLKDV\\Liu et al. - 2004 - Shared Frailty Models for Recurrent Events and a T.pdf;C\:\\Users\\devan\\Zotero\\storage\\VGG95KC4\\Liu et al. - 2004 - Shared Frailty Models for Recurrent Events and a T.pdf;C\:\\Users\\devan\\Zotero\\storage\\YPEH2IEC\\Liu et al. - 2004 - Shared Frailty Models for Recurrent Events and a T.pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{liuSharedRandomEffects2007,
  title = {A Shared Random Effects Model for Censored Medical Costs and Mortality},
  author = {Liu, Lei and Wolfe, Robert A. and Kalbfleisch, John D.},
  year = {2007},
  month = jan,
  volume = {26},
  pages = {139--155},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.2535},
  abstract = {In this paper, we propose a model for medical costs recorded at regular time intervals, e.g. every month, as repeated measures in the presence of a terminating event, such as death. Prior models have related monthly medical costs to time since entry, with extra costs at the \"ynal observations at the time of death. Our joint model for monthly medical costs and survival time incorporates two important new features. First, medical cost and survival may be correlated because more `frail' patients tend to accumulate medical costs faster and die earlier. A joint random e ects model is proposed to account for the correlation between medical costs and survival by a shared random e ect. Second, monthly medical costs usually increase during the time period prior to death because of the intensive care for dying patients. We present a method for estimating the pattern of cost prior to death, which is applicable if the pattern can be characterized as an additive e ect that is limited to a \"yxed time interval, say b units of time before death. This `turn back time' method for censored observations censors cost data b units of time before the actual censoring time, while keeping the actual censoring time for the survival data. Time-dependent covariates can be included. Maximum likelihood estimation and inference are carried out through a Monte Carlo EM algorithm with a Metropolis\textendash Hastings sampler in the E-step. An analysis of monthly outpatient EPO medical cost data for dialysis patients is presented to illustrate the proposed methods. Copyright ? 2006 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3X7PWZYN\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf;C\:\\Users\\devan\\Zotero\\storage\\9PWFE9ZA\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf;C\:\\Users\\devan\\Zotero\\storage\\C85ZHX3E\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf;C\:\\Users\\devan\\Zotero\\storage\\H6EDS6HW\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf;C\:\\Users\\devan\\Zotero\\storage\\LZVMP33I\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf;C\:\\Users\\devan\\Zotero\\storage\\NRT8GMDW\\Liu et al. - 2007 - A shared random effects model for censored medical.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {1}
}

@article{lombardoPointProcessbasedModeling2018,
  title = {Point Process-Based Modeling of Multiple Debris Flow Landslides Using {{INLA}}: An Application to the 2009 {{Messina}} Disaster},
  shorttitle = {Point Process-Based Modeling of Multiple Debris Flow Landslides Using {{INLA}}},
  author = {Lombardo, Luigi and Opitz, Thomas and Huser, Rapha{\"e}l},
  year = {2018},
  month = jul,
  volume = {32},
  pages = {2179--2198},
  issn = {1436-3240, 1436-3259},
  doi = {10.1007/s00477-018-1518-0},
  abstract = {We develop a stochastic modeling approach based on spatial point processes of log-Gaussian Cox type for a collection of around 5000 landslide events provoked by a precipitation trigger in Sicily, Italy. Through the embedding into a hierarchical Bayesian estimation framework, we can use the integrated nested Laplace approximation methodology to make inference and obtain the posterior estimates of spatially distributed covariate and random effects. Several mapping units are useful to partition a given study area in landslide prediction studies. These units hierarchically subdivide the geographic space from the highest grid-based resolution to the stronger morphodynamic-oriented slope units. Here we integrate both mapping units into a single hierarchical model, by treating the landslide triggering locations as a random point pattern. This approach diverges fundamentally from the unanimously used presence\textendash absence structure for areal units since we focus on modeling the expected landslide count jointly within the two mapping units. Predicting this landslide intensity provides more detailed and complete information as compared to the classically used susceptibility mapping approach based on relative probabilities. To illustrate the model's versatility, we compute absolute probability maps of landslide occurrences and check their predictive power over space. While the landslide community typically produces spatial predictive models for landslides only in the sense that covariates are spatially distributed, no actual spatial dependence has been explicitly integrated so far. Our novel approach features a spatial latent effect defined at the slope unit level, allowing us to assess the spatial influence that remains unexplained by the covariates in the model. For rainfall-induced landslides in regions where the raingauge network is not sufficient to capture the spatial distribution of the triggering precipitation event, this latent effect provides valuable imaging support on the unobserved rainfall pattern.},
  journal = {Stochastic Environmental Research and Risk Assessment},
  language = {en},
  number = {7}
}

@article{longCharacterizingForestFragmentation2010,
  title = {Characterizing Forest Fragmentation: {{Distinguishing}} Change in Composition from Configuration},
  shorttitle = {Characterizing Forest Fragmentation},
  author = {Long, Jed A. and Nelson, Trisalyn A. and Wulder, Michael A.},
  year = {2010},
  month = jul,
  volume = {30},
  pages = {426--435},
  issn = {01436228},
  doi = {10.1016/j.apgeog.2009.12.002},
  abstract = {Forest fragmentation can generally be considered as two components: 1) compositional change representing forest loss, and 2) configurational change or change in the arrangement of forest land cover. Forest loss and configurational change occur simultaneously, resulting in difficulties isolating the impacts of each component. Measures of forest fragmentation typically consider forest loss and configurational change together. The ecological responses to forest loss and configurational change are different, thus motivating the creation of measures capable of isolating these separate components. In this research, we develop and demonstrate a measure, the proportion of landscape displacement from configuration (Py), to quantify the relative contributions of forest loss and configurational change to forest fragmentation. Landscapes with statistically significant forest loss or configurational change are identified using neutral landscape simulations to generate underlying distributions for Py. The new measure, Py, is applied to a forest landscape where substantial forest loss has occurred from mountain pine beetle mitigation and salvage harvesting. The percent of forest cover and six LPIs (edge density, number of forest patches, area of largest forest patch, mean perimeter area ratio, corrected mean perimeter area ratio, and aggregation index) are used to quantify forest fragmentation and change. In our study area, significant forest loss occurs more frequently than significant configurational change. The Py method we demonstrate is effective at identifying landscapes undergoing significant forest loss, significant configurational change, or experiencing a combination of both loss and configurational change.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TDCHU6P8\\Long et al. - 2010 - Characterizing forest fragmentation Distinguishin.pdf},
  journal = {Applied Geography},
  language = {en},
  number = {3}
}

@article{longCriticalExaminationIndices2014,
  title = {A Critical Examination of Indices of Dynamic Interaction for Wildlife Telemetry Studies},
  author = {Long, Jed A. and Nelson, Trisalyn A. and Webb, Stephen L. and Gee, Kenneth L.},
  year = {2014},
  volume = {83},
  pages = {1216--1233},
  issn = {1365-2656},
  doi = {10.1111/1365-2656.12198},
  abstract = {Wildlife scientists continue to be interested in studying ways to quantify how the movements of animals are interdependent \textendash{} dynamic interaction. While a number of applied studies of dynamic interaction exist, little is known about the comparative effectiveness and applicability of available methods used for quantifying interactions between animals. We highlight the formulation, implementation and interpretation of a suite of eight currently available indices of dynamic interaction. Point- and path-based approaches are contrasted to demonstrate differences between methods and underlying assumptions on telemetry data. Correlated and biased correlated random walks were simulated at a range of sampling resolutions to generate scenarios with dynamic interaction present and absent. We evaluate the effectiveness of each index at identifying different types of interactive behaviour at each sampling resolution. Each index is then applied to an empirical telemetry data set of three white-tailed deer (Odocoileus virginianus) dyads. Results from the simulated data show that three indices of dynamic interaction reliant on statistical testing procedures are susceptible to Type I error, which increases at fine sampling resolutions. In the white-tailed deer examples, a recently developed index for quantifying local-level cohesive movement behaviour (the di index) provides revealing information on the presence of infrequent and varying interactions in space and time. Point-based approaches implemented with finely sampled telemetry data overestimate the presence of interactions (Type I errors). Indices producing only a single global statistic (7 of the 8 indices) are unable to quantify infrequent and varying interactions through time. The quantification of infrequent and variable interactive behaviour has important implications for the spread of disease and the prevalence of social behaviour in wildlife. Guidelines are presented to inform researchers wishing to study dynamic interaction patterns in their own telemetry data sets. Finally, we make our code openly available, in the statistical software R, for computing each index of dynamic interaction presented herein.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LGK39DLN\\Long et al. - 2014 - A critical examination of indices of dynamic inter.pdf;C\:\\Users\\devan\\Zotero\\storage\\23LPRR42\\1365-2656.html},
  journal = {Journal of Animal Ecology},
  keywords = {biased random walk,contact rate,GPS telemetry,Odocoileus virginianus,proximity,sampling resolution,simulation,static interaction},
  language = {en},
  number = {5}
}

@article{longRegionalizationLandscapePattern2010,
  title = {Regionalization of {{Landscape Pattern Indices Using Multivariate Cluster Analysis}}},
  author = {Long, Jed and Nelson, Trisalyn and Wulder, Michael},
  year = {2010},
  month = jul,
  volume = {46},
  pages = {134--142},
  issn = {0364-152X, 1432-1009},
  doi = {10.1007/s00267-010-9510-6},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8EZT3WI3\\Long et al. - 2010 - Regionalization of Landscape Pattern Indices Using.pdf},
  journal = {Environmental Management},
  language = {en},
  number = {1}
}

@article{longReviewQuantitativeMethods2013,
  title = {A Review of Quantitative Methods for Movement Data},
  author = {Long, Jed A. and Nelson, Trisalyn A.},
  year = {2013},
  month = feb,
  volume = {27},
  pages = {292--318},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2012.682578},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VEFDEYSA\\Long and Nelson - 2013 - A review of quantitative methods for movement data.pdf},
  journal = {International Journal of Geographical Information Science},
  language = {en},
  number = {2}
}

@article{lotwickMethodsAnalysingSpatial1982,
  title = {Methods for {{Analysing Spatial Processes}} of {{Several Types}} of {{Points}}},
  author = {Lotwick, H. W. and Silverman, B. W.},
  year = {1982},
  volume = {44},
  pages = {406--413},
  abstract = {Two approaches are described to the analysis of spatial patterns consisting of several types of points. The first approach uses a method of asymptotically unbiased estimation of the second moment distribution; the second uses methods based on regions of empty space in the patterns. Some distributional results are given in each case, and a method of Monte Carlo testing conditional on the marginal structure is described. The methods are illustrated by being applied to some physiological data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\723LUQJ5\\Lotwick and Silverman - 1982 - Methods for Analysing Spatial Processes of Several.pdf;C\:\\Users\\devan\\Zotero\\storage\\P9ECT8VG\\Lotwick and Silverman - 1982 - Methods for Analysing Spatial Processes of Several.pdf;C\:\\Users\\devan\\Zotero\\storage\\Z824L3T5\\Lotwick and Silverman - 1982 - Methods for Analysing Spatial Processes of Several.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {3}
}

@article{lundyAnalyzingHeapedCounts2018,
  title = {Analyzing {{Heaped Counts Versus Longitudinal Presence}}/{{Absence Data}} in {{Joint Zero}}-Inflated {{Discrete Regression Models}}},
  author = {Lundy, Erin R. and Dean, C. B.},
  year = {2018},
  month = aug,
  pages = {004912411878255},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124118782550},
  abstract = {Multiple outcome recurrent event data are typical in social sciences, where several outcomes on an individual are collected. In situations where aggregated counts of events over a long observation period are recorded, rounding is common, leading to counts being heaped at rounded values. We consider situations where multiple outcome recurrent event data are recorded as binary responses indicating presence/absence of events between periodic assessments. By analyzing these jointly through linkage via random effects, we show that a joint outcome analysis of the presence/absence data, that are less prone to recall errors, provides high relative efficiency, compared to the analysis of true counts. Motivated by a study of criminal behavior, we demonstrate the utility of such joint analyses, including that the analysis of longitudinal presence/absence data eliminates the bias arising from the analysis of heaped count data, and hence incorrect conclusions concerning possible risk factors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IIJP47U3\\Lundy and Dean - 2018 - Analyzing Heaped Counts Versus Longitudinal Presen.pdf},
  journal = {Sociological Methods \& Research},
  language = {en}
}

@phdthesis{lundyJointAnalysisZeroheavy2016,
  title = {Joint {{Analysis}} of {{Zero}}-Heavy {{Longitudinal Outcomes}}: {{Models}} and {{Comparison}} of {{Study Designs}}},
  author = {Lundy, Erin R},
  year = {2016},
  collaborator = {Dean, Charmaine B.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WIU2W2ZC\\Lundy - Joint Analysis of Zero-heavy Longitudinal Outcomes.pdf},
  language = {en},
  school = {The University of Western Ontario}
}

@article{luoJointModelDiagnostic2014,
  title = {Joint Model for a Diagnostic Test without a Gold Standard in the Presence of a Dependent Terminal Event},
  author = {Luo, Sheng and Su, Xiao and DeSantis, Stacia M. and Huang, Xuelin and Yi, Min and Hunt, Kelly K.},
  year = {2014},
  month = jul,
  volume = {33},
  pages = {2554--2566},
  issn = {02776715},
  doi = {10.1002/sim.6101},
  abstract = {Breast cancer patients after breast conservation therapy often develop ipsilateral breast tumor relapse (IBTR), whose classification (true local recurrence versus new ipsilateral primary tumor) is subject to error and there is no available gold standard. Some patients may die due to breast cancer before IBTR develops. Because this terminal event may be related to the individual patient's unobserved disease status and time to IBTR, the terminal mechanism is non-ignorable. This article presents a joint analysis framework to model the binomial regression with misclassified binary outcome and the correlated time to IBTR, subject to a dependent terminal event and in the absence of a gold standard. Shared random effects are used to link together two survival times. The proposed approach is evaluated by a simulation study and is applied to a breast cancer dataset consisting of 4,477 breast cancer patients. The proposed joint model can be conveniently fit using adaptive Gaussian quadrature tools implemented in SAS procedure NLMIXED.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4NRU5YU8\\Luo et al. - 2014 - Joint model for a diagnostic test without a gold s.pdf;C\:\\Users\\devan\\Zotero\\storage\\6VDHD9ZH\\Luo et al. - 2014 - Joint model for a diagnostic test without a gold s.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {15}
}

@inproceedings{luoPerformancesLOOWAIC2017,
  title = {Performances of {{LOO}} and {{WAIC}} as {{IRT Model Selection Methods}}},
  author = {Luo, Yong and {Al-Harbi}, Khaleel},
  year = {2017},
  abstract = {The widely available information criterion (WAIC) and leave-one-out cross-validation (LOO) are considered fully Bayesian model selection methods due to their utilization of the whole posterior distribution other than the point estimates. Despite their theoretical advantage of being fully Bayesian, how such an advantage translates into practical performance remains unknown. In this paper, we conducted a simulation study to compare the performances of WAIC and LOO with other four commonly used methods, which are the likelihood ratio test (LRT), AIC, BIC, and DIC, in the context of dichotomous IRT model selection. We also used a real data set to illustrate that those six model selection methods can lead to different conclusions. The findings suggest that WAIC and LOO perform better than the other four methods, especially when the data were generated with 3PLM. In addition, it was found that AIC, one of the most widely used model selection method, can become inconsistent with different sample sizes and test lengths.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HI9P87JM\\Luo and Al-Harbi - 2017 - Performances of LOO and WAIC as IRT Model Selectio.pdf},
  keywords = {Aicardi's syndrome,Akaike information criterion,Bayes factor,Bayesian information criterion,Bayesian network,Clinical Trial Interactive Response Technology Documentation,Cross Reactions,Cross-validation (statistics),Deviance information criterion,Disseminated Intravascular Coagulation,Estimated,imidazole mustard,Item response theory,Long-running transaction,Model selection,Performance,Sample Size,Simulation}
}

@article{luTrackingEchovirusEleven2020,
  title = {Tracking Echovirus Eleven Outbreaks in {{Guangdong}}, {{China}}: A Metatranscriptomic, Phylogenetic, and Epidemiological Study},
  shorttitle = {Tracking Echovirus Eleven Outbreaks in {{Guangdong}}, {{China}}},
  author = {Lu, Jing and Kang, Min and Zeng, Hanri and Zhong, Yuwen and Fang, Ling and Zheng, Xiaoling and Liu, Leng and Yi, Lina and Lin, Huifang and Peng, Jingju and Li, Caixia and Zhang, Yingtao and Sun, Limei and Luo, Shuhua and Xiao, Jianpeng and Munnink, Bas B Oude and Koopmans, Marion P G and Wu, Jie and Zhang, Yong and Zhang, Yonghui and Song, Tie and Li, Hui and Zheng, Huanying},
  year = {2020},
  month = jan,
  volume = {6},
  pages = {veaa029},
  issn = {2057-1577},
  doi = {10.1093/ve/veaa029},
  abstract = {Abstract             In April 2019, a suspect cluster of enterovirus cases was reported in a neonatology department in Guangdong, China, resulting in five deaths. We aimed to investigate the pathogen profiles in fatal cases, the circulation and transmission pattern of the viruses by combining metatranscriptomic, phylogenetic, and epidemiological analyses. Metatranscriptomic sequencing was used to characterize the enteroviruses. Clinical and environmental surveillance in the local population was performed to understand the prevalence and genetic diversity of the viruses in the local population. The possible source(s), evolution, transmission, and recombination of the viruses were investigated by incorporating genomes from the current outbreak, from local retrospective surveillance, and from public databases. Metatranscriptomic analysis identified Echovirus 11 (E11) in three fatal cases. Seroprevalence of neutralization antibody to E11 was 35 to 44 per cent in 3\textendash 15 age groups of general population, and the viruses were associated with various clinical symptoms. From the viral phylogeny, nosocomial transmissions were identified and all E11 2019 outbreak strains were closely related with E11 strains circulating in local population 2017\textendash 19. Frequent recombination occurred among the 2019 Guangdong E11 outbreak strains and various genotypes in enterovirus B species. This study provides an example of combining advanced genetic technology and epidemiological surveillance in pathogen diagnosis, source(s), and transmission tracing during an infectious disease outbreak. The result highlights the hidden E11 circulation and the risk of viral transmission and infection in the young age population in China. Frequent recombination between Guangdong-like strains and other enterovirus genotypes also implies the prevalence of these emerging E11 strains.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\J7R7XEQE\\Lu et al. - 2020 - Tracking echovirus eleven outbreaks in Guangdong, .pdf},
  journal = {Virus Evolution},
  language = {en},
  number = {1}
}

@article{lykouWinBUGSTutorial2011,
  title = {{{WinBUGS}}: A Tutorial},
  shorttitle = {{{WinBUGS}}},
  author = {Lykou, Anastasia and Ntzoufras, Ioannis},
  year = {2011},
  volume = {3},
  pages = {385--396},
  issn = {1939-0068},
  doi = {10.1002/wics.176},
  abstract = {The reinvention of Markov chain Monte Carlo (MCMC) methods and their implementation within the Bayesian framework in the early 1990s has established the Bayesian approach as one of the standard methods within the applied quantitative sciences. Their extensive use in complex real life problems has lead to the increased demand for a friendly and easily accessible software, which implements Bayesian models by exploiting the possibilities provided by MCMC algorithms. WinBUGS is the software that covers this increased need. It is the Windows version of BUGS (Bayesian inference using Gibbs sampling) package appeared in the mid-1990s. It is a free and a relatively easy tool that estimates the posterior distribution of any parameter of interest in complicated Bayesian models. In this article, we present an overview of the basic features of WinBUGS, including information for the model and prior specification, the code and its compilation, and the analysis and the interpretation of the MCMC output. Some simple examples and the Bayesian implementation of the Lasso are illustrated in detail. WIREs Comp Stat 2011 3 385\textendash 396 DOI: 10.1002/wics.176 This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Bayesian Methods and Theory Statistical and Graphical Methods of Data Analysis {$>$} Markov Chain Monte Carlo (MCMC) Software for Computational Statistics {$>$} Software/Statistical Software},
  copyright = {Copyright \textcopyright{} 2011 John Wiley \& Sons, Inc.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XD8J9AS5\\Lykou and Ntzoufras - 2011 - WinBUGS a tutorial.pdf;C\:\\Users\\devan\\Zotero\\storage\\E8MGZ72S\\wics.html},
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  keywords = {Bayesian statistics,MCMC},
  language = {en},
  number = {5}
}

@article{macdonaldEvaluatingNHLGoalies2012,
  title = {Evaluating {{NHL Goalies}}, {{Skaters}}, and {{Teams Using Weighted Shots}}},
  author = {Macdonald, Brian and Lennon, Craig and Sturdivant, Rodney},
  year = {2012},
  month = may,
  abstract = {In this paper, we develop a logistic regression model to estimate the probability that a particular shot in an NHL game will result in a goal, and use the results to evaluate the performance of NHL skaters, goalies, and teams. We weight each shot based on the estimated probabilities obtained from our model, call this statistic ``weighted shots'', and use advanced statistics based on weighted shots as the basis of our evaluation. We also analyze whether advanced statistics based on weighted shots outperform traditional statistics as an indicator of future performance of skaters, goalies, and teams. In general, statistics based on weighted shots perform well, but not better than traditional statistics. We conclude that weighted shots should not be viewed as a replacement for those statistics, but can be used in conjunction with those statistics. Finally, we use weighted shots as the dependent variable in an adjusted plus-minus model. The results are estimates of each player's offensive and defensive contribution to his team's weighted shots during even strength, power play, and short handed situations, independent of the strength of his teammates, the strength of his opponents, and the zone in which his shifts begin.},
  archivePrefix = {arXiv},
  eprint = {1205.1746},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2ZAN34QW\\Macdonald et al. - 2012 - Evaluating NHL Goalies, Skaters, and Teams Using W.pdf},
  journal = {arXiv:1205.1746 [stat]},
  keywords = {Primary: 629PP. Secondary: 62J12,Statistics - Applications},
  language = {en},
  primaryClass = {stat}
}

@article{macdonaldExpectedGoalsModel2012,
  title = {An {{Expected Goals Model}} for {{Evaluating NHL Teams}} and {{Players}}},
  author = {Macdonald, Brian},
  year = {2012},
  pages = {8},
  abstract = {One difficulty with analyzing performance in hockey is the relatively low scoring rates compared to sports like basketball. Fenwick rating (shots plus missed shots) and Corsi rating (shots, missed shots, blocked shots) have been used to analyze players and teams because they have been shown to be better than goals as a predictor of future goals. In this paper, we use variables like faceoffs, hits, and other statistics as predictor variables in addition to goals, shots, missed shots, and blocked shots, to predict goals. Our models outperform previous models with regard to mean squared error of actual goals and predicted goals. The results can be interpreted as expected goals and can be used in adjusted plus-minus models instead of goals. We use ridge regression to estimate a player's contribution to his team's expected goals per 60 minutes, independent of his teammates, opponents, and the zone in which his shifts begin. We also give adjusted plus-minus estimates based on goals, shots, Fenwick rating, and Corsi rating and use these results alongside the results for expected goals to provide an additional means by which NHL analysts, decision-makers, and fans can measure how valuable a player is to his team.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BQQ5GPXT\\Macdonald - 2012 - An Expected Goals Model for Evaluating NHL Teams a.pdf},
  language = {en}
}

@article{macnabAutoregressiveSpatialSmoothing2001,
  title = {Autoregressive {{Spatial Smoothing}} and {{Temporal Spline Smoothing}} for {{Mapping Rates}}},
  author = {MacNab, Ying C. and Dean, C. B.},
  year = {2001},
  month = sep,
  volume = {57},
  pages = {949--956},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2001.00949.x},
  abstract = {T. his article proposes generalized additive mixed models for the analysis of geographic and temporal variability of mortality rates. This class of models accommodates random spatial effects and fixed and random temporal components. Spatiotemporal models that use autoregressive local smoothing across the spatial dimension and B-spline smoothing over the temporal dimension are developed. The objective is the identification of temporal trends and the production of a series of smoothed maps from which spatial patterns of mortality risks can be monitored over time. Regions with consistently high rate estimates may be followed for further investigation. The methodology is illustrated by analysis of British Columbia infant mortality data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SVT5K97H\\MacNab and Dean - 2001 - Autoregressive Spatial Smoothing and Temporal Spli.pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{macnabBayesianSharedComponent2010,
  title = {On {{Bayesian}} Shared Component Disease Mapping and Ecological Regression with Errors in Covariates},
  author = {MacNab, Ying C.},
  year = {2010},
  pages = {n/a-n/a},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3875},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RXX7LTSS\\MacNab - 2010 - On Bayesian shared component disease mapping and e.pdf},
  journal = {Statistics in Medicine},
  language = {en}
}

@misc{macphailSurveyedRailwaysHighways2015,
  title = {Surveyed {{Railways}}, {{Highways}} and {{Roads}}},
  author = {Macphail, Scott},
  year = {2015},
  howpublished = {https://catalogue.data.gov.bc.ca/dataset/surveyed-railways-highways-and-roads},
  journal = {BC Data Catalogue}
}

@article{magnussenInterIntraannualProfiles2012,
  title = {Inter- and Intra-Annual Profiles of Fire Regimes in the Managed Forests of {{Canada}} and Implications for Resource Sharing},
  author = {Magnussen, Steen and Taylor, Stephen W.},
  year = {2012},
  volume = {21},
  pages = {328},
  issn = {1049-8001},
  doi = {10.1071/WF11026},
  abstract = {Year-to-year variation in fire activity in Canada constitutes a key challenge for fire management agencies. Interagency sharing of fire management resources has been ongoing on regional, national and international scales in Canada for several decades to better cope with peaks in resource demand. Inherent stressors on these schemes determined by the fire regimes in constituent jurisdictions are not well known, nor described by averages. We developed a statistical framework to examine the likelihood of regional synchrony of peaks in fire activity at a timescale of 1 week. Year-to-year variations in important fire regime variables and 48 regions in Canada are quantified by a joint distribution and profiled at the Provincial or Territorial level. The fire regime variables capture the timing of the fire season, the average number of fires, area burned, and the timing and extent of annual maxima. The onset of the fire season was strongly correlated with latitude and longitude. Regional synchrony in the timing of the maximum burned area within fire seasons delineates opportunities for and limitations to sharing of fire suppression resources during periods of stress that were quantified in Monte Carlo simulations from the joint distribution.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LQLT2C6K\\Magnussen and Taylor - 2012 - Inter- and intra-annual profiles of fire regimes i.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {4}
}

@article{magnussenInterIntraannualProfiles2012a,
  title = {Inter- and Intra-Annual Profiles of Fire Regimes in the Managed Forests of {{Canada}} and Implications for Resource Sharing},
  author = {Magnussen, Steen and Taylor, Stephen W.},
  year = {2012},
  month = jul,
  volume = {21},
  pages = {328--341},
  issn = {1448-5516},
  doi = {10.1071/WF11026},
  abstract = {Year-to-year variation in fire activity in Canada constitutes a key challenge for fire management agencies. Interagency sharing of fire management resources has been ongoing on regional, national and international scales in Canada for several decades to better cope with peaks in resource demand. Inherent stressors on these schemes determined by the fire regimes in constituent jurisdictions are not well known, nor described by averages. We developed a statistical framework to examine the likelihood of regional synchrony of peaks in fire activity at a timescale of 1 week. Year-to-year variations in important fire regime variables and 48 regions in Canada are quantified by a joint distribution and profiled at the Provincial or Territorial level. The fire regime variables capture the timing of the fire season, the average number of fires, area burned, and the timing and extent of annual maxima. The onset of the fire season was strongly correlated with latitude and longitude. Regional synchrony in the timing of the maximum burned area within fire seasons delineates opportunities for and limitations to sharing of fire suppression resources during periods of stress that were quantified in Monte Carlo simulations from the joint distribution.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RJ59P24M\\WF11026.html},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {4}
}

@article{magnussenPredictionDailyLightning2012,
  title = {Prediction of Daily Lightning- and Human-Caused Fires in {{British Columbia}}},
  author = {Magnussen, S. and Taylor, S. W.},
  year = {2012},
  month = jul,
  volume = {21},
  pages = {342--356},
  issn = {1448-5516},
  doi = {10.1071/WF11088},
  abstract = {Daily records of the location and timing of human- and lightning-caused fires in British Columbia from 1981 to 2000 were used to estimate the probability of fire occurrence within 950 20 \texttimes{} 20-km spatial units (\textasciitilde 950 000 km2) using a binary logistic regression modelling framework. Explanatory variables included lightning strikes, forest cover, surface weather observations, atmospheric stability indices and fuel moisture codes of the Canadian Fire Weather Index System. Because the influence of the explanatory variables in the models varied from year to year, model coefficients were estimated for each year. The arithmetic mean of the model coefficients was used for making daily predictions in a future year. A confidence interval around the mean or a quantile was derived from the ensemble of 20 model predictions. A leave-1-year-out cross-validation procedure was used to assess model performance for random years. The daily number of lightning-caused fires was reasonably well predicted at the provincial level (R = 0.83) and slightly less well predicted for a smaller (75 000 km2) administrative region. The daily number of human-caused fires was less well predicted at both the provincial (R = 0.55) and the regional level. The ability to estimate confidence intervals from the ensemble of model predictions is an advantage of the year-specific approach.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HDUV897Z\\WF11088.html},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {4}
}

@article{magnussenPredictionDailyLightning2012a,
  title = {Prediction of Daily Lightning- and Human-Caused Fires in {{British Columbia}}},
  author = {Magnussen, S. and Taylor, S. W.},
  year = {2012},
  volume = {21},
  pages = {342},
  issn = {1049-8001},
  doi = {10.1071/WF11088},
  abstract = {Daily records of the location and timing of human- and lightning-caused fires in British Columbia from 1981 to 2000 were used to estimate the probability of fire occurrence within 950 20 \^A 20-km spatial units (,950 000 km2) using a binary logistic regression modelling framework. Explanatory variables included lightning strikes, forest cover, surface weather observations, atmospheric stability indices and fuel moisture codes of the Canadian Fire Weather Index System. Because the influence of the explanatory variables in the models varied from year to year, model coefficients were estimated for each year. The arithmetic mean of the model coefficients was used for making daily predictions in a future year. A confidence interval around the mean or a quantile was derived from the ensemble of 20 model predictions. A leave1-year-out cross-validation procedure was used to assess model performance for random years. The daily number of lightning-caused fires was reasonably well predicted at the provincial level (R {$\frac{1}{4}$} 0.83) and slightly less well predicted for a smaller (75 000 km2) administrative region. The daily number of human-caused fires was less well predicted at both the provincial (R {$\frac{1}{4}$} 0.55) and the regional level. The ability to estimate confidence intervals from the ensemble of model predictions is an advantage of the year-specific approach.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C4R3FRA8\\Magnussen and Taylor - 2012 - Prediction of daily lightning- and human-caused fi.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {4}
}

@misc{makowskiBayestestRDescribingEffects2019,
  title = {{{bayestestR}}: {{Describing Effects}} and Their {{Uncertainty}}, {{Existence}} and {{Significance}} within the {{Bayesian Framework}}},
  shorttitle = {{{bayestestR}}},
  author = {Makowski, Dominique and {Ben-Shachar}, Mattan and L{\"u}decke, Daniel},
  year = {2019},
  month = aug,
  doi = {10.21105/joss.01541},
  abstract = {Makowski et al., (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541, https://doi.org/10.21105/joss.01541},
  file = {C\:\\Users\\devan\\Zotero\\storage\\N7DCPLTU\\Makowski et al. - 2019 - bayestestR Describing Effects and their Uncertain.pdf;C\:\\Users\\devan\\Zotero\\storage\\U4G57KCA\\joss.html},
  howpublished = {https://joss.theoj.org},
  journal = {Journal of Open Source Software},
  language = {en}
}

@techreport{makowskiIndicesEffectExistence2019,
  title = {Indices of {{Effect Existence}} and {{Significance}} in the {{Bayesian Framework}}},
  author = {Makowski, Dominique and {Ben-Shachar}, Mattan S. and Chen, SH Annabel and L{\"u}decke, Daniel},
  year = {2019},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/2zexr},
  abstract = {Turmoil has engulfed psychological science. Causes and consequences of the reproducibility crisis are in dispute. With the hope of addressing some of its aspects, Bayesian methods are gaining increasing attention in psychological science. Some of their advantages, as opposed to the frequentist framework, are the ability to describe parameters in probabilistic terms and explicitly incorporate prior knowledge about them into the model. These issues are crucial in particular regarding the current debate about statistical significance. Bayesian methods are not necessarily the only remedy against incorrect interpretations or wrong conclusions, but there is an increasing agreement that they are one of the keys to avoid such fallacies. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices of ``significance'' should be computed or reported. This lack of a consensual index or guidelines, such as the frequentist p-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several Bayesian indices, provide intuitive visual representation of their ``behavior'' in relationship with common sources of variance such as sample size, magnitude of effects and also frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report, allowing to draw sensible recommendations for Bayesian statistics description, critical for the standardization of scientific reporting.},
  type = {Preprint}
}

@misc{makowskiUnderstandDescribeBayesian2019,
  title = {Understand and {{Describe Bayesian Models}} and {{Posterior Distributions}} Using {{bayestestR}}},
  author = {Makowski, Dominique and {Ben-Shachar}, Mattan S. and L{\"u}decke, Daniel},
  year = {2019}
}

@article{malamudApplicabilityPowerlawFrequency2006,
  title = {The Applicability of Power-Law Frequency Statistics to Floods},
  author = {Malamud, Bruce D. and Turcotte, Donald L.},
  year = {2006},
  month = may,
  volume = {322},
  pages = {168--180},
  issn = {0022-1694},
  doi = {10.1016/j.jhydrol.2005.02.032},
  abstract = {Many natural hazards satisfy power-law (fractal) frequency-size statistics to a good approximation for medium and large events. Examples include earthquakes, volcanic eruptions, asteroid impacts, landslides, and forest fires. A major question is whether this is also true for floods. A number of authors have argued in favor of the applicability of power-law statistics to floods. We discuss these arguments and consider a number of examples, using both instrumental records and paleoflood studies. In our analyses we consider both annual and partial-duration flood series. We argue that use of annual floods for statistical considerations strongly biases the flood-frequency estimates, as in some years, the annual flood will be much smaller than a number of `statistically independent' floods (partial-duration floods) in other years. We examine six USGS hydrologic stations with drainage areas from 41 to 95,300km2, representing very different climatic regions and hydrologic conditions, and with periods of records ranging from 74 to 110 water years. Excellent power-law fits to each partial-duration series are found taking Q{$\sim$}T{$\alpha$}, with Q the discharge associated with the recurrence interval T, and the power-law exponent {$\alpha$} ranging from 0.27 to 0.90. We also consider paleoflood estimates for Axehandle Alcove on the Colorado River and Bonza Alcove on the Paria River, and find that that power-law extrapolations based on instrumental partial-duration series for stations in each of these two areas is within a half order of magnitude when compared to their respective paleoflood estimates. Finally, we consider an alternative approach to extreme streamflows that has been proposed, examining the cumulative probability distribution of instrumental daily mean streamflows. We show that this distribution is in good agreement with the power-law correlation found using the partial-duration series.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IQWFG8CV\\Malamud and Turcotte - 2006 - The applicability of power-law frequency statistic.pdf;C\:\\Users\\devan\\Zotero\\storage\\4S49FQ58\\S0022169405001125.html},
  journal = {Journal of Hydrology},
  keywords = {Floods,Fractals,Paleofloods,Power law,Risk assessment,Time series analysis},
  language = {en},
  number = {1},
  series = {Hydrofractals '03}
}

@article{malamudForestFiresExample1998,
  title = {Forest {{Fires}}: {{An Example}} of {{Self}}-{{Organized Critical Behavior}}},
  shorttitle = {Forest {{Fires}}},
  author = {Malamud, Bruce D. and Morein, Gleb and Turcotte, Donald L.},
  year = {1998},
  month = sep,
  volume = {281},
  pages = {1840--1842},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.281.5384.1840},
  abstract = {Despite the many complexities concerning their initiation and propagation, forest fires exhibit power-law frequency-area statistics over many orders of magnitude. A simple forest fire model, which is an example of self-organized criticality, exhibits similar behavior. One practical implication of this result is that the frequency-area distribution of small and medium fires can be used to quantify the risk of large fires, as is routinely done for earthquakes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\42ZPZGGZ\\Malamud et al. - 1998 - Forest Fires An Example of Self-Organized Critica.pdf;C\:\\Users\\devan\\Zotero\\storage\\D6RUNL6M\\1840.html},
  journal = {Science},
  language = {en},
  number = {5384},
  pmid = {9743494}
}

@article{malinowskiIntrinsicallyWeightedMeans2012,
  title = {Intrinsically {{Weighted Means}} of {{Marked Point Processes}}},
  author = {Malinowski, Alexander and Schlather, Martin and Zhang, Zhengjun},
  year = {2012},
  month = oct,
  abstract = {For a non-stationary or non-ergodic marked point process (MPP) on \$\textbackslash R\^d\$, the definition of averages becomes ambiguous as the process might have a different stochastic behavior in different realizations (non-ergodicity) or in different areas of the observation window (non-stationarity). We investigate different definitions for the moments, including a new hierarchical definition for non-ergodic MPPs, and embed them into a family of weighted mean marks. We point out examples of application in which different weighted mean marks all have a sensible meaning. Further, asymptotic properties of the corresponding estimators are investigated as well as optimal weighting procedures.},
  archivePrefix = {arXiv},
  eprint = {1210.1335},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H7HB9VYY\\Malinowski et al. - 2012 - Intrinsically Weighted Means of Marked Point Proce.pdf;C\:\\Users\\devan\\Zotero\\storage\\A3W7P5DB\\1210.html},
  journal = {arXiv:1210.1335 [math]},
  keywords = {60G55; 37A50; 37A30,Mathematics - Probability},
  primaryClass = {math}
}

@article{marin2010,
  title = {On Resolving the {{Savage}}--{{Dickey}} Paradox},
  author = {Marin, Jean-Michel and Robert, Christian P.},
  year = {2010},
  volume = {4},
  pages = {643--654},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  journal = {Electronic Journal of Statistics}
}

@article{marinResolvingSavageDickey2010,
  title = {On Resolving the {{Savage}}\textendash{{Dickey}} Paradox},
  author = {Marin, Jean-Michel and Robert, Christian P.},
  year = {2010},
  volume = {4},
  pages = {643--654},
  issn = {1935-7524},
  doi = {10.1214/10-EJS564},
  abstract = {When testing a null hypothesis H0: \texttheta =\texttheta 0 in a Bayesian framework, the Savage\textendash Dickey ratio (Dickey, 1971) is known as a specific representation of the Bayes factor (O'Hagan and Forster, 2004) that only uses the posterior distribution under the alternative hypothesis at \texttheta 0, thus allowing for a plug-in version of this quantity. We demonstrate here that the Savage\textendash Dickey representation is in fact a generic representation of the Bayes factor and that it fundamentally relies on specific measure-theoretic versions of the densities involved in the ratio, instead of being a special identity imposing some mathematically void constraints on the prior distributions. We completely clarify the measure-theoretic foundations of the Savage\textendash Dickey representation as well as of the later generalisation of Verdinelli and Wasserman (1995). We provide furthermore a general framework that produces a converging approximation of the Bayes factor that is unrelated with the approach of Verdinelli and Wasserman (1995) and propose a comparison of this new approximation with their version, as well as with bridge sampling and Chib's approaches.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\28DF8U2K\\Marin and Robert - 2010 - On resolving the Savage–Dickey paradox.pdf;C\:\\Users\\devan\\Zotero\\storage\\G4GSCWED\\1278682959.html},
  journal = {Electronic Journal of Statistics},
  keywords = {Bayes factor,Bayesian model choice,bridge sampling,conditional distribution,hypothesis testing,Savage–Dickey ratio,zero measure set},
  language = {EN},
  mrnumber = {MR2660536},
  zmnumber = {1329.62091}
}

@article{martellCurrentPracticesNew,
  title = {Current Practices and New Challenges for Operational Researchers},
  author = {Martell, David L},
  pages = {21},
  abstract = {Forest fire management systems share much in common with urban fire, police and ambulance systems, but the spatial and temporal variability of forest fire occurrence processes and the comparatively long distances over which forest fire management takes place pose special challenges to operational researchers. This chapter describes the basic structure of a forest fire management system and the decision-making problems faced by fire managers. It describes how operations research (OR) has been applied to forest fire prevention, detection, deployment and initial attack dispatch decision-making problems; large fire management, strategic planning and fuel management, and it identifies new challenges that are amenable to OR approaches.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NJH86WCN\\Martell - Current practices and new challenges for operation.pdf},
  language = {en}
}

@article{martellLogisticModelPredicting,
  title = {A Logistic Model for Predicting Daily People-Caused Forest Fire Occurence in {{Ontario}}},
  author = {Martell, David L. and Otukol, S. and Stocks, B.J.},
  year = {1987},
  volume = {17},
  pages = {394--401},
  file = {C\:\\Users\\devan\\Zotero\\storage\\S8K9TJK7\\Martell, Otukol, Stocks - 1987 - A logistic model for predicting daily people-caused forest fire occurence in Ontario.pdf},
  journal = {Canadian Journal of Forest Research},
  keywords = {Count,Covariates,Ignition Probability,Logistic}
}

@article{martinez-minayaIntegratedNestedLaplace2019,
  title = {The {{Integrated}} Nested {{Laplace}} Approximation for Fitting Models with Multivariate Response},
  author = {{Mart{\'i}nez-Minaya}, Joaqu{\'i}n and Lindgren, Finn and {L{\'o}pez-Qu{\'i}lez}, Antonio and Simpson, Daniel and Conesa, David},
  year = {2019},
  month = jul,
  abstract = {This paper introduces a Laplace approximation to Bayesian inference in regression models for multivariate response variables. We focus on Dirichlet regression models, which can be used to analyze a set of variables on a simplex exhibiting skewness and heteroscedasticity, without having to transform the data. These data, which mainly consist of proportions or percentages of disjoint categories, are widely known as compositional data and are common in areas such as ecology, geology, and psychology. We provide both the theoretical foundations and a description of how this Laplace approximation can be implemented in the case of Dirichlet regression. The paper also introduces the package dirinla in the R-language that extends the R-INLA package, which can not deal directly with multivariate likelihoods like the Dirichlet likelihood. Simulation studies are presented to validate the good behaviour of the proposed method, while a real data case-study is used to show how this approach can be applied.},
  archivePrefix = {arXiv},
  eprint = {1907.04059},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZFIFGYXY\\Martínez-Minaya et al. - 2019 - The Integrated nested Laplace approximation for fi.pdf},
  journal = {arXiv:1907.04059 [stat]},
  keywords = {Statistics - Computation,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{martinsBayesianComputingINLA2013,
  title = {Bayesian Computing with {{INLA}}: {{New}} Features},
  shorttitle = {Bayesian Computing with {{INLA}}},
  author = {Martins, Thiago G. and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2013},
  month = nov,
  volume = {67},
  pages = {68--83},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.04.014},
  abstract = {The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. New developments in the R-INLA are formalized and it is shown how these features greatly extend the scope of models that can be analyzed by this interface. The current default method in R-INLA to approximate the posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration, is discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CTWDMZU9\\Martins et al. - 2013 - Bayesian computing with INLA New features.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@misc{masisakAnalyticsNotStatistics2015,
  title = {Analytics, Not Statistics, Driving {{NHL}} Evolution},
  author = {Masisak, Corey},
  year = {2015},
  abstract = {The latest news, analysis and stories from NHL.com, the official site of the National Hockey League},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VQM25CVH\\c-754099.html},
  howpublished = {https://www.nhl.com/news/analytics-not-statistics-driving-nhl-evolution/c-754099},
  journal = {NHL.com},
  language = {en\_CA}
}

@article{mccullochJointModellingMixed2008,
  title = {Joint Modelling of Mixed Outcome Types Using Latent Variables},
  author = {McCulloch, Charles},
  year = {2008},
  month = feb,
  volume = {17},
  pages = {53--73},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280207081240},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ANA6JQHU\\McCulloch - 2008 - Joint modelling of mixed outcome types using laten.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {1}
}

@misc{mccurdyBetterWayCompute2014,
  title = {Better {{Way}} to {{Compute Score}}-{{Adjusted Fenwick}}},
  author = {McCurdy, Micah Blake},
  year = {2014},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AET76IC4\\senstats.html},
  howpublished = {https://www.hockeyviz.com/txt/senstats},
  journal = {HockeyVis}
}

@misc{mccurdyIcetimeNotCause2015,
  title = {Ice-Time {{Is Not The Cause}} of {{Score Effects}}},
  author = {McCurdy, Micah Blake},
  year = {2015},
  month = apr,
  address = {{Washington DC}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\G86858ZK\\McCurdy - Ice-time Is Not The Cause of Score Effects.pdf},
  language = {en}
}

@misc{mccurdyLeverage2016,
  title = {Leverage},
  author = {McCurdy, Micah Blake},
  year = {2016},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WVGZIYS6\\leverage.html},
  howpublished = {https://hockeyviz.com/txt/leverage},
  journal = {HockeyVis}
}

@misc{mccurdyShiftStartLocation2015,
  title = {Shift {{Start Location Adjustment}}},
  author = {McCurdy, Micah Blake},
  year = {2015},
  address = {{Rochester NY}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Q5W4CQN2\\McCurdy - Shift Start Location Adjustment.pdf},
  language = {en}
}

@misc{mccurdyShotGenerationSuppression2015,
  title = {Shot {{Generation}} and {{Suppression}} Are {{Mostly Independent}}},
  author = {McCurdy, Micah Blake},
  year = {2015},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F56E32UZ\\indepGenSupp.html},
  howpublished = {https://www.hockeyviz.com/txt/indepGenSupp},
  journal = {HockeyVis}
}

@misc{mccurdyStandardizedGoals2017,
  title = {Standardized {{Goals Against}}},
  author = {McCurdy, Micah Blake},
  year = {2017},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6DABYZ84\\sGA.html},
  howpublished = {https://hockeyviz.com/txt/sGA},
  journal = {HockeyVis}
}

@book{mcelreathStatisticalRethinkingBayesian2015,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  author = {McElreath, Richard},
  year = {2015},
  publisher = {{CRC Press}},
  isbn = {978-1-4822-5344-3},
  series = {Chapman \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}}}
}

@article{mcfaddenDisentanglingFunctionalTrait2019,
  title = {Disentangling the Functional Trait Correlates of Spatial Aggregation in Tropical Forest Trees},
  author = {McFadden, Ian R. and Bartlett, Megan K. and Wiegand, Thorsten and Turner, Benjamin L. and Sack, Lawren and Valencia, Renato and Kraft, Nathan J. B.},
  year = {2019},
  month = mar,
  volume = {100},
  pages = {e02591},
  issn = {00129658},
  doi = {10.1002/ecy.2591},
  abstract = {Environmental filtering and dispersal limitation can both maintain diversity in plant communities by aggregating conspecifics, but parsing the contribution of each process has proven difficult empirically. Here, we assess the contribution of filtering and dispersal limitation to the spatial aggregation patterns of 456 tree species in a hyperdiverse Amazonian forest and find distinct functional trait correlates of interspecific variation in these processes. Spatial point process model analysis revealed that both mechanisms are important drivers of intraspecific aggregation for the majority of species. Leaf drought tolerance was correlated with species topographic distributions in this aseasonal rainforest, showing that future increases in drought severity could significantly impact community structure. In addition, seed mass was associated with the spatial scale and density of dispersal-related aggregation. Taken together, these results suggest environmental filtering and dispersal limitation act in concert to influence the spatial and functional structure of diverse forest communities.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AI4I4399\\McFadden et al. - 2019 - Disentangling the functional trait correlates of s.pdf},
  journal = {Ecology},
  language = {en},
  number = {3}
}

@article{mellerLogGaussianCox,
  title = {Log {{Gaussian Cox Processes}}},
  author = {M{\'e}LLER, JESPER and Syversveen, Anne Randi and Waagepetersen, Rasmus Plenge},
  pages = {32},
  abstract = {Planar Cox processes directed by a log Gaussian intensity process are investigated in the univariate and multivariate cases. The appealing properties of such models are demonstrated theoretically as well as through data examples and simulations. In particular, the \textregistered rst, second and third-order properties are studied and utilized in the statistical analysis of clustered point patterns. Also empirical Bayesian inference for the underlying intensity surface is considered.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Q3N9VGFE\\MéLLER et al. - Log Gaussian Cox Processes.pdf},
  journal = {Scand J Statist},
  language = {en}
}

@article{menezesGeostatisticalAnalysisPreferential,
  title = {Geostatistical {{Analysis}} under {{Preferential Sampling}}},
  author = {Menezes, Raquel and Diggle, Peter},
  pages = {4},
  abstract = {In geostatistics it is commonly assumed that the selection of the sampling locations does not depend on the values of the spatial variable. One has preferential sampling when this assumption fails (e.g. maximum values search). We first show that the impact of a preferential design on the traditional prediction methods is not negligible. We address this problem by proposing a modelbased approach, for stationary Gaussian processes. This new parametric model is founded on a flexible class of log-Gaussian Cox processes. A numerical study is then included to compare the performance of the model proposed and the traditional geostatistical model.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XCMXGT7Q\\Menezes and Diggle - Geostatistical Analysis under Preferential Samplin.pdf},
  language = {en}
}

@article{merzbachCharacterizationSpatialPoisson1986,
  title = {A {{Characterization}} of the {{Spatial Poisson Process}} and {{Changing Time}}},
  author = {Merzbach, Ely and Nualart, David},
  year = {1986},
  volume = {14},
  pages = {1380--1390},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NG58SWYX\\Merzbach and Nualart - 1986 - A Characterization of the Spatial Poisson Process .pdf},
  journal = {The Annals of Probability},
  language = {en},
  number = {4}
}

@book{Miller2008,
  title = {Fatigue and Its Effect on Performance in Military Environments},
  author = {Miller, Nita Lewis and Matsangas, P and Shattuck, LG},
  year = {2008},
  publisher = {{Ashgate Publishing Burlington, VT}}
}

@article{millerFactorizedPointProcess2014,
  title = {Factorized {{Point Process Intensities}}: {{A Spatial Analysis}} of {{Professional Basketball}}},
  shorttitle = {Factorized {{Point Process Intensities}}},
  author = {Miller, Andrew and Bornn, Luke and Adams, Ryan and Goldsberry, Kirk},
  year = {2014},
  month = jan,
  abstract = {We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA. Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior. This makes it difficult to draw comparisons between players and make accurate player specific predictions. Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA. Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players. The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy.},
  archivePrefix = {arXiv},
  eprint = {1401.0942},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TJC5PWY5\\Miller et al. - 2014 - Factorized Point Process Intensities A Spatial An.pdf;C\:\\Users\\devan\\Zotero\\storage\\TLB2D9S2\\Miller et al. - 2014 - Factorized Point Process Intensities A Spatial An.pdf},
  journal = {arXiv:1401.0942 [stat]},
  keywords = {Statistics - Applications,Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{millerUnderstandingStochasticPartial2019,
  title = {Understanding the {{Stochastic Partial Differential Equation Approach}} to {{Smoothing}}},
  author = {Miller, David L. and Glennie, Richard and Seaton, Andrew E.},
  year = {2019},
  month = sep,
  issn = {1085-7117, 1537-2693},
  doi = {10.1007/s13253-019-00377-z},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6BHHR2M7\\Miller et al. - 2019 - Understanding the Stochastic Partial Differential .pdf},
  journal = {Journal of Agricultural, Biological and Environmental Statistics},
  language = {en}
}

@article{mohamedApproximationAggregateLosses2010,
  title = {Approximation of {{Aggregate Losses Using Simulation}}},
  author = {{Mohamed}},
  year = {2010},
  month = aug,
  volume = {6},
  pages = {233--239},
  issn = {1549-3644},
  doi = {10.3844/jmssp.2010.233.239},
  abstract = {Problem statement: The modeling of aggregate losses is one of the main objectives in actuarial theory and practice, especially in the process of making important business decisions regarding various aspects of insurance contracts. The aggregate losses over a fixed time period is often modeled by mixing the distributions of loss frequency and severity, whereby the distribution resulted from this approach is called a compound distribution. However, in many cases, realistic probability distributions for loss frequency and severity cannot be combined mathematically to derive the compound distribution of aggregate losses. Approach: This study aimed to approximate the aggregate loss distribution using simulation approach. In particular, the approximation of aggregate losses was based on a compound Poisson-Pareto distribution. The effects of deductible and policy limit on the individual loss as well as the aggregate losses were also investigated. Results: Based on the results, the approximation of compound Poisson-Pareto distribution via simulation approach agreed with the theoretical mean and variance of each of the loss frequency, loss severity and aggregate losses. Conclusion: This study approximated the compound distribution of aggregate losses using simulation approach. The investigation on retained losses and insurance claims allowed an insured or a company to select an insurance contract that fulfills its requirement. In particular, if a company wants to have an additional risk reduction, it can compare alternative policies by considering the worthiness of the additional expected total cost which can be estimated via simulation approach.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2EVXM7VH\\Mohamed - 2010 - Approximation of Aggregate Losses Using Simulation.pdf},
  journal = {Journal of Mathematics and Statistics},
  language = {en},
  number = {3}
}

@article{mohlerSelfExcitingPointProcess2011,
  title = {Self-{{Exciting Point Process Modeling}} of {{Crime}}},
  author = {Mohler, G. O. and Short, M. B. and Brantingham, P. J. and Schoenberg, F. P. and Tita, G. E.},
  year = {2011},
  month = mar,
  volume = {106},
  pages = {100--108},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2011.ap09546},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9XBL6CYL\\Mohler et al. - 2011 - Self-Exciting Point Process Modeling of Crime.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {493}
}

@article{mojalalStatisticalApplicationsHealthcare,
  title = {Statistical {{Applications}} in {{Healthcare Systems}}},
  author = {Mojalal, Maryam},
  pages = {120},
  file = {C\:\\Users\\devan\\Zotero\\storage\\I8TUT8HS\\Mojalal - Statistical Applications in Healthcare Systems.pdf},
  language = {en}
}

@article{molasJointHierarchicalGeneralized2013,
  title = {Joint Hierarchical Generalized Linear Models with Multivariate {{Gaussian}} Random Effects},
  author = {Molas, Marek and Noh, Maengseok and Lee, Youngjo and Lesaffre, Emmanuel},
  year = {2013},
  month = dec,
  volume = {68},
  pages = {239--250},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.07.011},
  abstract = {Likelihood based inference for correlated data involves the evaluation of a marginal likelihood integrating out random effects. In general this integral does not have a closed form. Moreover, its numerical evaluation might create difficulties especially when the dimension of random effects is high. H-likelihood inference has been proposed where the explicit evaluation of the integral is avoided. The approach also allows extensions handling e.g. (1) complex design experiments, (2) REML type of inference beyond the class of a linear model and (3) overdispersion modeling. The h-likelihood approach to multivariate generalized linear mixed models is extended. The h-likelihood computational algorithms is blended with a Newton\textendash Raphson procedure for the estimation of the correlation parameters. This allows that components of the joint model are interlinked via correlated Gaussian random effects. Further, correlated random effects are allowed within each component. This approach can serve as a basis for further developments of joint double hierarchical generalized linear models with correlated random effects. The methods are illustrated with a rheumatoid arthritis study dataset, where the correlation between latent trajectories of three endpoints is evaluated.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4E65EKBT\\Molas et al. - 2013 - Joint hierarchical generalized linear models with .pdf;C\:\\Users\\devan\\Zotero\\storage\\DRC4ZCBS\\Molas et al. - 2013 - Joint hierarchical generalized linear models with .pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@article{mollerAspectsSecondorderAnalysis2012,
  title = {Aspects of Second-Order Analysis of Structured Inhomogeneous Spatio-Temporal Point Processes: {{{\emph{Aspects}}}}{\emph{ of Second-Order Analysis}}},
  shorttitle = {Aspects of Second-Order Analysis of Structured Inhomogeneous Spatio-Temporal Point Processes},
  author = {M{\o}ller, Jesper and Ghorbani, Mohammad},
  year = {2012},
  month = nov,
  volume = {66},
  pages = {472--491},
  issn = {00390402},
  doi = {10.1111/j.1467-9574.2012.00526.x},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DZGL8DKT\\Møller and Ghorbani - 2012 - Aspects of second-order analysis of structured inh.pdf},
  journal = {Statistica Neerlandica},
  language = {en},
  number = {4}
}

@article{mollerLogGaussianCox1998,
  title = {Log {{Gaussian Cox Processes}}},
  author = {M{\o}ller, Jesper and Syversveen, Anne Randi and Waagepetersen, Rasmus Plenge},
  year = {1998},
  volume = {25},
  pages = {451--482},
  issn = {0303-6898},
  abstract = {Planar Cox processes directed by a log Gaussian intensity process are investigated in the univariate and multivariate cases. The appealing properties of such models are demonstrated theoretically as well as through data examples and simulations. In particular, the first, second and third-order properties are studied and utilized in the statistical analysis of clustered point patterns. Also empirical Bayesian inference for the underlying intensity surface is considered.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TJSN8EQS\\Møller et al. - 1998 - Log Gaussian Cox Processes.pdf},
  journal = {Scandinavian Journal of Statistics},
  number = {3}
}

@article{mollerModernStatisticsSpatial2007,
  title = {Modern {{Statistics}} for {{Spatial Point Processes}}},
  author = {M{\O}LLER, JESPER and WAAGEPETERSEN, RASMUS P.},
  year = {2007},
  volume = {34},
  pages = {643--684},
  abstract = {We summarize and discuss the current state of spatial point process theory and directions for future research, making an analogy with generalized linear models and random effect models, and illustrating the theory with various examples of applications. In particular, we consider Poisson, Gibbs and Cox process models, diagnostic tools and model checking, Markov chain Monte Carlo algorithms, computational methods for likelihood-based inference, and quick non-likelihood approaches to inference.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7R6RZMP2\\MØLLER and WAAGEPETERSEN - 2007 - Modern Statistics for Spatial Point Processes.pdf;C\:\\Users\\devan\\Zotero\\storage\\CX2UAMQ7\\MØLLER and WAAGEPETERSEN - 2007 - Modern Statistics for Spatial Point Processes.pdf;C\:\\Users\\devan\\Zotero\\storage\\T7DKLISX\\MØLLER and WAAGEPETERSEN - 2007 - Modern Statistics for Spatial Point Processes.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {4}
}

@article{mollerRecentDevelopmentsStatistics2016,
  title = {Some Recent Developments in Statistics for Spatial Point Patterns},
  author = {M{\o}ller, Jesper and Waagepetersen, Rasmus},
  year = {2016},
  month = sep,
  abstract = {This paper reviews developments in statistics for spatial point processes obtained within roughly the last decade. These developments include new classes of spatial point process models such as determinantal point processes, models incorporating both regularity and aggregation, and models where points are randomly distributed around latent geometric structures. Regarding parametric inference the main focus is on various types of estimating functions derived from so-called innovation measures. Optimality of such estimating functions is discussed as well as computational issues. Maximum likelihood inference for determinantal point processes and Bayesian inference are briefly considered too. Concerning non-parametric inference, we consider extensions of functional summary statistics to the case of inhomogeneous point processes as well as new approaches to simulation based inference.},
  archivePrefix = {arXiv},
  eprint = {1609.00908},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Y4N2FMFZ\\Møller and Waagepetersen - 2016 - Some recent developments in statistics for spatial.pdf},
  journal = {arXiv:1609.00908 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{mollerShotNoiseCox2003,
  title = {Shot {{Noise Cox Processes}}},
  author = {M{\o}ller, Jesper},
  year = {2003},
  volume = {35},
  pages = {614--640},
  abstract = {Shot noise Cox processes constitute a large class of Cox and Poisson cluster processes in Rd, including Neyman-Scott, Poisson-gamma and shot noise G Cox processes. It is demonstrated that, due to the structure of such models, a number of useful and general results can easily be established. The focus is on the probabilistic aspects with a view to statistical applications, particularly results for summary statistics, reduced Palm distributions, simulation with or without edge effects, conditional simulation of the intensity function and local and spatial Markov properties.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\88A9T3C9\\Møller - 2003 - Shot Noise Cox Processes.pdf},
  journal = {Advances in Applied Probability},
  language = {en},
  number = {3}
}

@article{mollerStructuredSpatioTemporalShotNoise2010,
  title = {Structured {{Spatio}}-{{Temporal Shot}}-{{Noise Cox Point Process Models}}, with a {{View}} to {{Modelling Forest Fires}}},
  author = {M{\O}LLER, JESPER and {D{\'I}AZ-AVALOS}, CARLOS},
  year = {2010},
  volume = {37},
  pages = {2--25},
  abstract = {Spatio-temporal Cox point process models with a multiplicative structure for the driving random intensity, incorporating covariate information into temporal and spatial components, and with a residual term modelled by a shot-noise process, are considered. Such models are flexible and tractable for statistical analysis, using spatio-temporal versions of intensity and inhomogeneous /\^-functions, quick estimation procedures based on composite likelihoods and minimum contrast estimation, and easy simulation techniques. These advantages are demonstrated in connection with the analysis of a relatively large data set consisting of 2796 days and 5834 spatial locations of fires. The model is compared with a spatio-temporal log-Gaussian Cox point process model, and likelihood-based methods are discussed to some extent.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JKLYHNEQ\\MØLLER and DÍAZ-AVALOS - 2010 - Structured Spatio-Temporal Shot-Noise Cox Point Pr.pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {1}
}

@article{mollerThinningSpatialPoint,
  title = {Thinning Spatial Point Processes into {{Poisson}} Processes},
  author = {M{\o}ller, Jesper and Schoenberg, Frederic Paik},
  pages = {22},
  abstract = {This paper describes methods for randomly thinning certain classes of spatial point processes. In the case of a Markov point process, the proposed method involves a dependent thinning of a spatial birth-and-death process, where clans of ancestors associated with the original points are identified, and where one simulates backwards and forwards in order to obtain the thinned process. In the case of a Cox process, a simple independent thinning technique is proposed. In both cases, the thinning results in a Poisson process if and only if the true Papangelou conditional intensity is used, and thus can be used as a diagnostic for assessing the goodness-of-fit of a spatial point process model. Several examples, including clustered and inhibitive point processes, are considered.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DKXJPUDH\\Møller and Schoenberg - Thinning spatial point processes into Poisson proc.pdf;C\:\\Users\\devan\\Zotero\\storage\\JE52M74U\\MØLLER and SCHOENBERG - 2010 - THINNING SPATIAL POINT PROCESSES INTO POISSON PROC.pdf},
  language = {en}
}

@article{mollerTRANSFORMINGSPATIALPOINT2012,
  title = {{{TRANSFORMING SPATIAL POINT PROCESSES INTO POISSON PROCESSES USING RANDOM SUPERPOSITION}}},
  author = {M{\O}LLER, JESPER and BERTHELSEN, KASPER K.},
  year = {2012},
  volume = {44},
  pages = {42--62},
  abstract = {Most finite spatial point process models specified by a density are locally stable, implying that the Papangelou intensity is bounded by some integrable function fi defined on the space for the points of the process. It is possible to superpose a locally stable spatial point process X with a complementary spatial point process Y to obtain a Poisson process X U Y with intensity function fi. Underlying this is a bivariate spatial birth death process (Xt, Y,) which converges towards the distribution of (X, Y). We study the joint distribution of X and Y, and their marginal and conditional distributions. In particular, we introduce a fast and easy simulation procedure for Y conditional on X. This may be used for model checking: given a model for the Papangelou intensity of the original spatial point process, this model is used to generate the complementary process, and the resulting superposition is a Poisson process with intensity function fi if and only if the true Papangelou intensity is used. Whether the superposition is actually such a Poisson process can easily be examined using well-known results and fast simulation procedures for Poisson processes. We illustrate this approach to model checking in the case of a Strauss process.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X9K4RJ46\\MØLLER and BERTHELSEN - 2012 - TRANSFORMING SPATIAL POINT PROCESSES INTO POISSON .pdf},
  journal = {Advances in Applied Probability},
  language = {en},
  number = {1}
}

@book{montgomery2009statistical,
  title = {Statistical Quality Control},
  author = {Montgomery, Douglas C},
  year = {2009},
  volume = {7},
  publisher = {{Wiley New York}}
}

@article{morelPhylogeneticAnalysisSARSCoV22020,
  title = {Phylogenetic Analysis of {{SARS}}-{{CoV}}-2 Data Is Difficult},
  author = {Morel, Benoit and Barbera, Pierre and Czech, Lucas and Bettisworth, Ben and H{\"u}bner, Lukas and Lutteropp, Sarah and Serdari, Dora and Kostaki, Evangelia-Georgia and Mamais, Ioannis and Kozlov, Alexey M and Pavlidis, Pavlos and Paraskevis, Dimitrios},
  year = {2020},
  month = aug,
  pages = {14},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YV9XSKS3\\2020.08.05.239046v1.full.pdf},
  journal = {bioRxiv preprint},
  language = {en}
}

@article{morinFrailtyModelsControl,
  title = {Frailty {{Models}} for the {{Control Time}} of {{Wildland Fires}} in the {{Former Intensive Fire Management Zone}} of {{Ontario}}, {{Canada}}},
  author = {Morin, Amy A and {Albert-Green}, Alisha and Woolford, Douglas G and Martell, David L},
  pages = {16},
  abstract = {Using the control time of a forest or wildland fire, defined as the time from the start of suppression action to the time that it is declared under control, we extend the analysis from Morin et al. (2015) to investigate spatial trends in forest fire survival probability across Ontario's Intensive Fire Management Zone for the period 1989 to 2004. The fire management compartments (FMCs) described in Woolford et al. (2009) form the spatial units of analysis. Spatial differences are explored in our study region by using proportional hazards shared frailty models which incorporate a random effect to modify the hazard for fires within each FMC. Estimates of this excess risk are used to visualize spatial patterns. We show that the frailty models achieve better fit, as compared to the models without frailty terms, and that the model assumptions are suitable for these data. Visualizing the estimated FMC-specific frailties suggest the following: lightning-caused fires in a region of northwestern Ontario have experienced shorter control times than comparable lightning fires that occur elsewhere; and, people-caused fires in that same region in northwestern Ontario as well as a region of southern Ontario may also have experienced shorter control times than comparable people-caused fires that have occurred elsewhere.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C86XR58J\\Morin et al. - Frailty Models for the Control Time of Wildland Fi.pdf},
  language = {en}
}

@article{moyaPopulationGeneticsEvolutionary2004,
  title = {The Population Genetics and Evolutionary Epidemiology of {{RNA}} Viruses},
  author = {Moya, Andr{\'e}s and Holmes, Edward C. and {Gonz{\'a}lez-Candelas}, Fernando},
  year = {2004},
  month = apr,
  volume = {2},
  pages = {279--288},
  issn = {1740-1526, 1740-1534},
  doi = {10.1038/nrmicro863},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9HYBKBPB\\Moya et al. - 2004 - The population genetics and evolutionary epidemiol.pdf},
  journal = {Nature Reviews Microbiology},
  language = {en},
  number = {4}
}

@article{musmeciSpacetimeClusteringModel1992,
  title = {A Space-Time Clustering Model for Historical Earthquakes},
  author = {Musmeci, F. and {Vere-Jones}, D.},
  year = {1992},
  month = mar,
  volume = {44},
  pages = {1--11},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/BF00048666},
  abstract = {This paper describes a generalization of Hawkes' self-exciting process in which each event creates a process of "offspring" with conditional intensity governed by a diffusion kernel. The process may be described as a space-time branching process with immigration, the immigration representing a background series of independent events. The model can be fitted by likelihood methods. As an illustration it is fitted to the catalogue of historical Italian earthquakes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FBZLEIUJ\\Musmeci and Vere-Jones - 1992 - A space-time clustering model for historical earth.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {1}
}

@article{myllymakiConditionallyHeteroscedasticIntensitydependent2009,
  title = {Conditionally Heteroscedastic Intensity-Dependent Marking of Log {{Gaussian Cox}} Processes},
  author = {Myllym{\"a}ki, Mari and Penttinen, Antti},
  year = {2009},
  month = nov,
  volume = {63},
  pages = {450--473},
  issn = {00390402, 14679574},
  doi = {10.1111/j.1467-9574.2009.00433.x},
  abstract = {Spatial point processes are models for systems of points randomly distributed in space. If the points are heterogeneous with respect to size, environment or neighbourhood, for example, this variation is described by measured quantities called marks. A general framework for statistical analysis of such systems of random points and marks is based on marked point processes. This study deals with marking, methods of constructing marked point processes from unmarked ones. Two special cases, independent and geostatistical markings, are among the known simple examples of marking strategies and are often used in practice. However, these markings are not able to model density-dependence of marks, the case where the local point intensity affects the mark distribution. This study develops new marking models of such a generality that not only the mean of the mark distribution but also its variance is affected by the local intensity. The new models are employed for marking of the log Gaussian Cox process and their theoretical mean, variance and mark correlation properties are presented. The most important part is statistical inference for such heteroscedastic marked point processes. The performance of the suggested estimation methods are studied in a simulation experiment. A tropical rainforest data is modelled using the developed models and statistical methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JD5K3UJN\\MyllymÃ¤ki and Penttinen - 2009 - Conditionally heteroscedastic intensity-dependent .pdf},
  journal = {Statistica Neerlandica},
  language = {en},
  number = {4}
}

@article{myllymakiConditionallyHeteroscedasticIntensitydependent2009a,
  title = {Conditionally Heteroscedastic Intensity-Dependent Marking of Log {{Gaussian Cox}} Processes},
  author = {Myllym{\"a}ki, Mari and Penttinen, Antti},
  year = {2009},
  month = nov,
  volume = {63},
  pages = {450--473},
  issn = {00390402, 14679574},
  doi = {10.1111/j.1467-9574.2009.00433.x},
  abstract = {Spatial point processes are models for systems of points randomly distributed in space. If the points are heterogeneous with respect to size, environment or neighbourhood, for example, this variation is described by measured quantities called marks. A general framework for statistical analysis of such systems of random points and marks is based on marked point processes. This study deals with marking, methods of constructing marked point processes from unmarked ones. Two special cases, independent and geostatistical markings, are among the known simple examples of marking strategies and are often used in practice. However, these markings are not able to model density-dependence of marks, the case where the local point intensity affects the mark distribution. This study develops new marking models of such a generality that not only the mean of the mark distribution but also its variance is affected by the local intensity. The new models are employed for marking of the log Gaussian Cox process and their theoretical mean, variance and mark correlation properties are presented. The most important part is statistical inference for such heteroscedastic marked point processes. The performance of the suggested estimation methods are studied in a simulation experiment. A tropical rainforest data is modelled using the developed models and statistical methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\W9CURR7G\\MyllymÃ¤ki and Penttinen - 2009 - Conditionally heteroscedastic intensity-dependent .pdf},
  journal = {Statistica Neerlandica},
  language = {en},
  number = {4}
}

@misc{myrlandHockeyProspectusCrease2017,
  title = {Hockey {{Prospectus}} | {{In The Crease}}: {{Shot Recording In The NHL}}, {{Part On}}\ldots},
  shorttitle = {Hockey {{Prospectus}} | {{In The Crease}}},
  author = {Myrland, Philip},
  year = {2017},
  month = aug,
  file = {C\:\\Users\\devan\\Zotero\\storage\\IBVFSB64\\11ovi.html},
  howpublished = {http://archive.is/11ovi},
  journal = {archive.is}
}

@article{naitoSemiparametricDensityEstimation2004,
  title = {Semiparametric Density Estimation by Local {{L}}\_2-Fitting},
  author = {Naito, Kanta},
  year = {2004},
  month = jun,
  volume = {32},
  pages = {1162--1191},
  issn = {0090-5364},
  doi = {10.1214/009053604000000319},
  abstract = {This article examines density estimation by combining a parametric approach with a nonparametric factor. The plug-in parametric estimator is seen as a crude estimator of the true density and is adjusted by a nonparametric factor. The nonparametric factor is derived by a criterion called local L\_2-fitting. A class of estimators that have multiplicative adjustment is provided, including estimators proposed by several authors as special cases, and the asymptotic theories are developed. Theoretical comparison reveals that the estimators in this class are better than, or at least competitive with, the traditional kernel estimator in a broad class of densities. The asymptotically best estimator in this class can be obtained from the elegant feature of the bias function.},
  archivePrefix = {arXiv},
  eprint = {math/0406522},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\79MNUS2D\\Naito - 2004 - Semiparametric density estimation by local L_2-fit.pdf},
  journal = {The Annals of Statistics},
  keywords = {62G07 (Primary) 62G20 (Secondary),Mathematics - Statistics Theory},
  language = {en},
  number = {3}
}

@article{najafabadiModelingForestFires2015,
  title = {Modeling Forest Fires in {{Mazandaran Province}}, {{Iran}}},
  author = {Najafabadi, Amir T. Payandeh and Gorgani, Fatemeh and Najafabadi, Maryam Omidi},
  year = {2015},
  month = dec,
  volume = {26},
  pages = {851--858},
  issn = {1007-662X, 1993-0607},
  doi = {10.1007/s11676-015-0107-z},
  abstract = {We used a spatio-temporal shot-noise Cox process to study the distribution of forest fires reported between 2006 and 2010 in the Mazandaran Province's forests. The fitted model shows that daily temperature, altitude, and slope-exposure impacted fire occurrence. Forest fire occurred in the region had an aggregated behavior, which increased in radius below 1-km away from fired areas; a periodic pattern of fire occurrence in the region was verified. The risk of forest fire is significantly higher for areas with southern exposure and slope between 30\textdegree{} and 50\textdegree, northern exposure and slope between 0\textdegree{} and 50\textdegree, and eastern exposure and slope between 0\textdegree{} and 30\textdegree. The risk of fire was also significantly higher at altitudes between 1350 and 3000 m asl. Human causes were the main ignition source for forest fires in the region. The fire occurrence rate stayed above average during the drought period from September 2008 to September 2009. Our findings could lead to the development of fire-response and fire-suppression strategies appropriate to specific regions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VU3NTEG4\\Najafabadi et al. - 2015 - Modeling forest fires in Mazandaran Province, Iran.pdf},
  journal = {Journal of Forestry Research},
  language = {en},
  number = {4}
}

@book{nationalacademiesforumNationalAcademiesForum2000,
  title = {National {{Academies Forum}}: Proceedings of the 1999 Seminar : {{Fire}}! {{The Australian Experience}}.},
  shorttitle = {National {{Academies Forum}}},
  editor = {{National Academies Forum}},
  year = {2000},
  publisher = {{National Academies Forum}},
  address = {{Canberra, A.C.T.}},
  annotation = {OCLC: 154315427},
  file = {C\:\\Users\\devan\\Zotero\\storage\\224F5IIA\\National Academies Forum - 2000 - National Academies Forum proceedings of the 1999 .pdf;C\:\\Users\\devan\\Zotero\\storage\\C37KKTUG\\National Academies Forum - 2000 - National Academies Forum proceedings of the 1999 .pdf;C\:\\Users\\devan\\Zotero\\storage\\NTKU2K82\\National Academies Forum - 2000 - National Academies Forum proceedings of the 1999 .pdf},
  isbn = {978-1-875618-57-6},
  keywords = {Fire Good},
  language = {en}
}

@article{navarroDevilDeepBlue2018,
  title = {Between the Devil and the Deep Blue Sea: {{Tensions}} between Scientific Judgement and Statistical Model Selection},
  shorttitle = {Between the Devil and the Deep Blue Sea},
  author = {Navarro, Danielle},
  year = {2018},
  month = oct,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/39q8y},
  abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leave-one-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might "toy problems" tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9PI5BTBC\\Navarro - 2018 - Between the devil and the deep blue sea Tensions .pdf;C\:\\Users\\devan\\Zotero\\storage\\JKJMBIVY\\39q8y.html}
}

@article{neal1994contribution,
  title = {Contribution to the Discussion of ``{{Approximate Bayesian}} Inference with the Weighted Likelihood Bootstrap'' by {{Newton MA}}, {{Raftery AE}}},
  author = {Neal, Radford M},
  year = {1994},
  volume = {56},
  pages = {41--42},
  journal = {JR Stat Soc Ser A (Methodological)}
}

@article{newton1994approximate,
  title = {Approximate {{Bayesian}} Inference with the Weighted Likelihood Bootstrap},
  author = {Newton, Michael A and Raftery, Adrian E},
  year = {1994},
  pages = {3--48},
  publisher = {{JSTOR}},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)}
}

@article{nguyenMappingCancerRisk2012,
  title = {Mapping {{Cancer Risk}} in {{Southwestern Ontario}} with {{Changing Census Boundaries}}},
  author = {Nguyen, P. and Brown, P. E. and Stafford, J.},
  year = {2012},
  month = dec,
  volume = {68},
  pages = {1228--1237},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2012.01792.x},
  abstract = {Mapping disease risk often involves working with data that have been spatially aggregated to census regions or postal regions, either for administrative reasons or confidentiality. When studying rare diseases, data must be collected over a long time period in order to accumulate a meaningful number of cases. These long time periods can result in spatial boundaries of the census regions changing over time, as is the case with the motivating example of exploring the spatial structure of mesothelioma lung cancer risk in Lambton County and Middlesex County of southwestern Ontario, Canada. This article presents a local-EM kernel smoothing algorithm that allows for the combining of data from different spatial maps, being capable of modeling risk for spatially aggregated data with time-varying boundaries. Inference and uncertainty estimates are carried out with parametric bootstrap procedures, and cross-validation is used for bandwidth selection. Results for the lung cancer study are shown and discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FBRS5H9Y\\Nguyen et al. - 2012 - Mapping Cancer Risk in Southwestern Ontario with C.pdf},
  journal = {Biometrics},
  language = {en},
  number = {4}
}

@article{nicholsApplicationPrototypePoint2011,
  title = {The Application of Prototype Point Processes for the Summary and Description of {{California}} Wildfires: {{PROTOTYPES OF CALIFORNIA WILDFIRES}}},
  shorttitle = {The Application of Prototype Point Processes for the Summary and Description of {{California}} Wildfires},
  author = {Nichols, Kevin and Schoenberg, Frederic Paik and Keeley, Jon E. and Bray, Andrew and Diez, David},
  year = {2011},
  month = jul,
  volume = {32},
  pages = {420--429},
  issn = {01439782},
  doi = {10.1111/j.1467-9892.2011.00734.x},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AZSI8BPL\\Nichols et al. - 2011 - The application of prototype point processes for t.pdf},
  journal = {Journal of Time Series Analysis},
  language = {en},
  number = {4}
}

@article{nielsenCountingProcessApproach1992,
  title = {A {{Counting Process Approach}} to {{Maximum Likelihood Estimation}} in {{Frailty Models}}},
  author = {Nielsen, Gert G. and Gill, Richard D. and Andersen, Per Kragh and S{\o}rensen, Thorkild I. A.},
  year = {1992},
  volume = {19},
  pages = {25--43},
  abstract = {We study counting process models for event history data where the intensities depend on unobservable quantities ("frailties"). Examples include models for dependent failure times and regression models with unobservable covariates. Estimation in both parametric and non- or semi-parametric models is performed by maximum likelihood methods using the EM algorithm. Simulations and practical examples are presented.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8BSCGG4W\\Nielsen et al. - 1992 - A Counting Process Approach to Maximum Likelihood .pdf;C\:\\Users\\devan\\Zotero\\storage\\SF2SVPTU\\Nielsen et al. - 1992 - A Counting Process Approach to Maximum Likelihood .pdf},
  journal = {Scandinavian Journal of Statistics},
  language = {en},
  number = {1}
}

@article{nightingalePairwiseInteractionPoint,
  title = {Pairwise {{Interaction Point Processes}} for {{Modelling Bivariate Spatial Point Patterns}} in the {{Presence}} of {{Interaction Uncertainty}}},
  author = {Nightingale, Glenna F and Illian, Janine B and King, Ruth},
  pages = {21},
  abstract = {Current ecological research seeks to understand the mechanisms that sustain biodiversity and allow a large number of species to coexist. Coexistence concerns inter-individual interactions. Consequently, there is an interest in identifying and quantifying interactions within and between species as reflected in the spatial pattern formed by the individuals. This study analyses the spatial pattern formed by the locations of plants in a community with high biodiversity from Western Australia. We fit a pairwise interaction Gibbs marked point process to the data using a Bayesian approach and quantify the inhibitory interactions within and between the two species. We quantitively discriminate between competing models corresponding to different inter-specific and intraspecific interactions via posterior model probabilities. The analysis provides evidence that the intraspecific interactions for the two species of the genus Banksia are generally similar to those between the two species providing some evidence for mechanisms that sustain biodiversity.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TWBUQD6D\\Nightingale et al. - Pairwise Interaction Point Processes for Modelling.pdf},
  language = {en}
}

@article{ogataLikelihoodAnalysisSpatial1988,
  title = {Likelihood Analysis of Spatial Inhomogeneity for Marked Point Patterns},
  author = {Ogata, Yosihiko and Katsura, Koichi},
  year = {1988},
  month = mar,
  volume = {40},
  pages = {29--39},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/BF00053953},
  abstract = {An objective method is developed for estimations of both spatial intensity of the point locations and spatial variation of a characteristic parameter of the distributions for the attached marks. Its utility is demonstrated by means of analyses of seismological and ecological data sets.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7TBK7KXG\\Ogata and Katsura - 1988 - Likelihood analysis of spatial inhomogeneity for m.pdf;C\:\\Users\\devan\\Zotero\\storage\\TSESCH9L\\Ogata and Katsura - 1988 - Likelihood analysis of spatial inhomogeneity for m.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {1}
}

@article{ogataStatisticalModelsEarthquake1988,
  title = {Statistical {{Models}} for {{Earthquake Occurrences}} and {{Residual Analysis}} for {{Point Processes}}},
  author = {Ogata, Yosihiko},
  year = {1988},
  month = mar,
  volume = {83},
  pages = {9--27},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1988.10478560},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QMAZINEK\\Ogata - 1988 - Statistical Models for Earthquake Occurrences and .pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {401}
}

@article{oharaReviewBayesianVariable2009,
  title = {A Review of {{Bayesian}} Variable Selection Methods: What, How and Which},
  shorttitle = {A Review of {{Bayesian}} Variable Selection Methods},
  author = {O'Hara, R. B. and Sillanp{\"a}{\"a}, M. J.},
  year = {2009},
  month = mar,
  volume = {4},
  pages = {85--117},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/09-BA403},
  abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo \& Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Jeffreys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their different properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the different methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Q6QQMCT7\\O'Hara and Sillanpää - 2009 - A review of Bayesian variable selection methods w.pdf;C\:\\Users\\devan\\Zotero\\storage\\CX6LB6SJ\\1340370391.html},
  journal = {Bayesian Analysis},
  keywords = {BUGS,MCMC,Variable Selection},
  language = {EN},
  mrnumber = {MR2486240},
  number = {1},
  zmnumber = {1330.62291}
}

@article{paikschoenbergTestingSeparabilitySpatialTemporal2004,
  title = {Testing {{Separability}} in {{Spatial}}-{{Temporal Marked Point Processes}}},
  author = {Schoenberg, Frederic Paik},
  year = {2004},
  month = jun,
  volume = {60},
  pages = {471--481},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2004.00192.x},
  abstract = {Nonparametric tests for investigating the separability of a spatial-temporal marked point process are described and compared. It is shown that a Cramer\textendash von Mises-type test is very powerful at detecting gradual departures from separability, and that a residual test based on randomly rescaling the process is powerful at detecting nonseparable clustering or inhibition of the marks. An application to Los Angeles County wildfire data is given, in which it is shown that the separability hypothesis is invalidated largely due to clustering of fires of similar sizes within periods of up to 3.9 years.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AZF6DVY6\\Paik Schoenberg - 2004 - Testing Separability in Spatial-Temporal Marked Po.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@techreport{parisienMappingWildfireSusceptibility2005,
  title = {Mapping Wildfire Susceptibility with the {{BURN}}-{{P3}} Simulation Model},
  author = {Parisien, M A and Kafka, V G and Hirsch, K G and Todd, J B and Lavoie, S G and Maczek, P D},
  year = {2005},
  pages = {36},
  address = {{Edmonton, Alberta}},
  institution = {{Natural Resources Canada, Canadian Forest Service, Northern Forestry Centre}},
  abstract = {To optimize strategic planning, resource management in fire-dominated ecosystems requires an understanding of the probability of wildfire occurring and spreading at different points on a landscape. This report describes an approach to evaluating wildfire susceptibility, or burn probability (BP), for fire-prone landscapes such as the boreal forest of North America. BURN-P3 (probability, prediction, and planning) is a landscape-level simulation model producing BP maps. The model combines deterministic fire growth based on the Canadian Fire Behavior Prediction System and spatial data for forest fuels and topography with probabilistic fire ignitions and spread events derived from historical fire and weather data. Model components include the location and frequency of ignitions, the rate at which fires escape initial attack and become large, the number of days on which each fire achieves significant spread, the fire weather conditions associated with these spread event days, and the deterministic fire spread. For a given landscape, BP is simulated for a single annual time step, or iteration, based on 500 to 1000 Monte Carlo simulations. A case study of the application of BURN-P3 was undertaken for a 15 \texttimes{} 106 ha boreal mixedwood area of central Saskatchewan. The BP values varied considerably within the study area. Regions with a high BP were highly localized (clustered distribution), largely because of the configuration and continuity of flammable forest fuels. These results highlight the importance of landscape features, such as lakes and recent burns, to wildfire susceptibility, and suggest that assessments based solely on stand-level characteristics may be inadequate.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\FCVBUAFJ\\Parisien et al. - MAPPING WILDFIRE SUSCEPTIBILITY WITH THE BURN-P3 S.pdf},
  language = {en},
  number = {NOR-X-405}
}

@article{parkinCookingBooksSimply2010,
  title = {Cooking the Books or Simply Getting the Best out of the Data? {{Assessing}} the Nature of the Relationship between Variables},
  shorttitle = {Cooking the Books or Simply Getting the Best out of the Data?},
  author = {Parkin, T. D. H. and Brown, P. E. and French, N. P. and Morgan, K. L.},
  year = {2010},
  month = jan,
  volume = {37},
  pages = {189--191},
  issn = {04251644, 20423306},
  doi = {10.2746/0425164054530614},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5KALKJXD\\Parkin et al. - 2010 - Cooking the books or simply getting the best out o.pdf;C\:\\Users\\devan\\Zotero\\storage\\8KWV8DDX\\Parkin et al. - 2010 - Cooking the books or simply getting the best out o.pdf},
  journal = {Equine Veterinary Journal},
  language = {en},
  number = {3}
}

@misc{parnassHowImportantPlaying2016,
  title = {How {{Important}} Is {{Playing}} the {{Off}}-{{Wing}} on the {{Power Play}}?},
  author = {Parnass, Arik},
  year = {2016},
  abstract = {Should players should play their strong or off-wings on the power play? I looked into the age-old question from a variety of different statistical perspectives.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UPQGNXZR\\how-important-is-playing-the-off-wing-on-the-power-play.html},
  howpublished = {http://www.nhlspecialteams.com/blog/2016/1/19/how-important-is-playing-the-off-wing-on-the-power-play},
  journal = {Special Teams Project},
  language = {en-US}
}

@misc{parnassStatisticallyAnalyzingHockey2016,
  title = {Statistically {{Analyzing Hockey}}'s {{One}}-{{Timer}}},
  author = {Parnass, Arik},
  year = {2016},
  abstract = {The one-timer is the most dangerous shot in hockey, but is it the most effective? I look into the numbers to uncover the answer.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7EHK7ATY\\statistically-analyzing-hockeys-one-timer.html},
  howpublished = {http://www.nhlspecialteams.com/blog/2016/2/1/statistically-analyzing-hockeys-one-timer},
  journal = {Special Teams Project},
  language = {en-US}
}

@misc{PDFUsingINLA,
  title = {({{PDF}}) {{Using INLA To Fit A Complex Point Process Model With Temporally Varying Effects}} \textendash{} {{A Case Study}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6FDR8DRY\\262005387_Using_INLA_To_Fit_A_Complex_Point_Process_Model_With_Temporally_Varying_Effects_-_A_C.html},
  howpublished = {https://www.researchgate.net/publication/262005387\_Using\_INLA\_To\_Fit\_A\_Complex\_Point\_Process\_Model\_With\_Temporally\_Varying\_Effects\_-\_A\_Case\_Study}
}

@article{pebesmaClassesMethodsSpatial2005,
  title = {Classes and Methods for Spatial Data in {{R}}},
  author = {Pebesma, Edzer J. and Bivand, Roger S.},
  year = {2005},
  month = nov,
  volume = {5},
  pages = {9--13},
  journal = {R News},
  number = {2}
}

@article{Peng2008,
  title = {A Method for Visualizing Multivariate Time Series Data},
  author = {Peng, Roger D.},
  year = {2008},
  volume = {25},
  pages = {1--17},
  issn = {15487660},
  doi = {http://dx.doi.org/10.18637/jss.v025.c01},
  abstract = {Visualization and exploratory analysis is an important part of any data analysis and is made more challenging when the data are voluminous and high-dimensional. One such example is environmental monitoring data, which are often collected over time and at multiple locations, resulting in a geographically indexedmultivariate time series. Financial data, although not necessarily containing a geographic component, present another source of high-volume multivariate time series data. We present the mvtsplot function which provides a method for visualizing multivariate time series data. We outline the basic design concepts and provide some examples of its usage by applying it to a database of ambient air pollution measurements in the United States and to a hypothetical portfolio of stocks.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6T2EUTF6\\Peng - 2008 - A method for visualizing multivariate time series .pdf},
  journal = {Journal of Statistical Software},
  keywords = {multivariate time series,vizualization},
  number = {March}
}

@article{pengMethodVisualizingMultivariate2008,
  title = {A {{Method}} for {{Visualizing Multivariate Time Series Data}}},
  author = {Peng, Roger D.},
  year = {2008},
  volume = {25},
  issn = {1548-7660},
  doi = {10.18637/jss.v025.c01},
  abstract = {Visualization and exploratory analysis is an important part of any data analysis and is made more challenging when the data are voluminous and high-dimensional. One such example is environmental monitoring data, which are often collected over time and at multiple locations, resulting in a geographically indexed multivariate time series. Financial data, although not necessarily containing a geographic component, present another source of high-volume multivariate time series data. We present the mvtsplot function which provides a method for visualizing multivariate time series data. We outline the basic design concepts and provide some examples of its usage by applying it to a database of ambient air pollution measurements in the United States and to a hypothetical portfolio of stocks.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3I2BA3DD\\Peng - 2008 - A Method for Visualizing Multivariate Time Series .pdf;C\:\\Users\\devan\\Zotero\\storage\\3Z5B2SFA\\Peng - 2008 - A Method for Visualizing Multivariate Time Series .pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {Code Snippet 1}
}

@article{pengSpaceTimeConditional2005,
  title = {A {{Space}}\textendash{{Time Conditional Intensity Model}} for {{Evaluating}} a {{Wildfire Hazard Index}}},
  author = {Peng, Roger D and Schoenberg, Frederic Paik and Woods, James A},
  year = {2005},
  month = mar,
  volume = {100},
  pages = {26--35},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214504000001763},
  abstract = {Numerical indices are commonly used as tools to aid wildfire management and hazard assessment. Although the use of such indices is widespread, assessment of these indices in their respective regions of application is rare. We evaluate the effectiveness of the burning index (BI) for predicting wildfire occurrences in Los Angeles County, California using space\textendash time point-process models. These models are based on an additive decomposition of the conditional intensity, with separate terms used to describe spatial and seasonal variability as well as contributions from the BI. We fit the models to wildfire and BI data from the years 1976\textendash 2000 using a combination of nonparametric kernel-smoothing methods and parametric maximum likelihood. In addition to using the Akaike information criterion (AIC) to compare competing models, we use new multidimensional residual methods based on approximate random thinning and rescaling to detect departures from the models and to ascertain the precise contribution of the BI to predicting wildfire occurrence. We find that although the BI appears to have a positive impact on wildfire prediction, the contribution is relatively small after taking into account natural seasonal and spatial variation. In particular, the BI does not appear to take into account increased activity during the years 1979\textendash 1981 and can overpredict during the early months of the year.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LIUQJD6T\\Peng et al. - 2005 - A Space–Time Conditional Intensity Model for Evalu.pdf},
  journal = {Journal of the American Statistical Association},
  keywords = {Covariates,Ignition Probability},
  language = {en},
  number = {469}
}

@article{petoussisStatisticalAnalysisHeaped,
  title = {Statistical {{Analysis}} of {{Heaped Duration Data}}},
  author = {Petoussis, K and Gill, R D and Zeelenberg, C},
  pages = {13},
  abstract = {This paper shows how heaping of duration data, e.g. caused by rounding due to memory e ects, can be analyzed. If the data are heaped Cox's partial likelihood approach, which is often used in survival analysis, is no longer appropriate. We show how this problem can be overcome by considering the problem as a missing data problem. A variant of Cox's Proportional Hazard Model is constructed that takes heaping into account, and is estimated by maximum likelihood using the EM algorithm. with many nuisance parameters, simultaneously for all parameters. Ingredients of our method are application of the EM algorithm, Cox regression and nonparametric maximum likelihood calculation with `predicted' data in each M step. An example from practice, where jackknife is used to estimate the variances, illustrates the power of the new methodology.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\28WKG39D\\Petoussis et al. - Statistical Analysis of Heaped Duration Data.pdf;C\:\\Users\\devan\\Zotero\\storage\\BQ2E788W\\Petoussis et al. - Statistical Analysis of Heaped Duration Data.pdf},
  language = {en}
}

@misc{petoussisStatisticalAnalysisHeaped1997,
  title = {Statistical Analysis of Heaped Duration Data},
  author = {Petoussis, Kos and Gill, Richard and Zeelenberg, Kees},
  year = {1997},
  month = jan,
  abstract = {This paper shows how heaping of duration data, e.g. caused by rounding due to memory effects, can be analyzed. If the data are heaped Cox's partial likelihood approach, which is often used in survival analysis, is no longer appropriate. We show how this problem can be overcome by considering the problem as a missing data problem. A variant of Cox's Proportional Hazard Model is constructed that takes heaping into account, and is estimated by maximum likelihood using the EM algorithm, with many nuisance parameters, simultaneously for all parameters.  Ingredients of our method are application of the EM algorithm, Cox regression and nonparametric maximum likelihood calculation with `predicted' data in each M step.  An example from practice, where jackknife is used to estimate the variances, illustrates the power of the new methodology.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KH2J72JT\\Petoussis et al. - 1997 - Statistical analysis of heaped duration data.pdf;C\:\\Users\\devan\\Zotero\\storage\\HGXYYZPC\\89263.html},
  howpublished = {https://mpra.ub.uni-muenchen.de/89263/},
  language = {en},
  type = {{{MPRA Paper}}}
}

@article{petrovRiskManagementForest2019,
  title = {Risk Management of Forest Fire Occurrence},
  author = {Petrov, V. N. and Katkova, T. E. and Vinogradova, E. V.},
  year = {2019},
  month = sep,
  volume = {316},
  pages = {012050},
  issn = {1755-1315},
  doi = {10.1088/1755-1315/316/1/012050},
  abstract = {This academic paper is devoted to the issue of how to reduce forest fire damage through affecting the fire causes. Forest fire causes are a subject of random probability. Adequate forest management is not possible when there is lack of precise information about when and where a forest fire may occur. When there are no ways and methods to influence the reasons of forest fires occurrence, the likelihood of a wrong decision is directly proportional to the negative consequences of forest fires. The research purpose was to develop mechanisms of risk management taking into account the cause and effect relation between human activities, natural phenomena and occurrence of forest fire. The risk as economic category is the target of the research. The scope of the research encompasses the process of decision-taking in forest management with regards to forest preservation from fires under the conditions of unpredictability. The research was based on a comprehensive approach to the problem and used contemporary regulations of sustainable forest management and global experience in forestry relationship. It was suggested to classify the causes of forest fires in groups. Probability of forest fires occurrence was estimated in accordance with various causes. There was introduced an approach of forest fire risk management based on strategic risk management. The results of the research can be used by scientific institutions, as well as by forestry companies while planning environmental protection measures. The research can proceed towards assessment and forecasting of fire risks in forestry at the national and international levels.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2ZBNTR5Q\\Petrov et al. - 2019 - Risk management of forest fire occurrence.pdf},
  journal = {IOP Conference Series: Earth and Environmental Science},
  language = {en}
}

@article{pettigrewAssessingOffensiveProductivity2015,
  title = {Assessing the Offensive Productivity of {{NHL}} Players Using In-Game Win Probabilities},
  author = {Pettigrew, Stephen},
  year = {2015},
  pages = {9},
  abstract = {Hockey journalists and statisticians currently lack many of the empirical tools available in other sports. In this paper I introduce a win probability metric for the NHL and use it to develop a new statistic, Added Goal Value, which evaluates player offensive productivity. The metric is the first of its kind to incorporate powerplay information and is the only NHL in-game win probability metric currently available. I show how win probabilities can enhance the narrative around an individual game and can also be used to evaluate playoff series win probabilities. I then introduce Added Goal Value which improves upon traditional offensive player statistics by accounting for game context. A player's AGV has a strong positive correlation between seasons, making it a useful statistic for predicting future offensive productivity. By accounting for the context in which goals are scored, AGV also allows for comparisons to be made between players who have identical goalscoring rates. The work in this paper provides several advances in hockey analytics and also provides a framework for unifying current and future work on Corsi, Fenwick, and other NHL analytics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DKJQFXJ4\\Pettigrew - Assessing the offensive productivity of NHL player.pdf},
  language = {en}
}

@article{pickeringDigitPreferenceEstimated1992,
  title = {Digit Preference in Estimated Gestational Age},
  author = {Pickering, R. M.},
  year = {1992},
  volume = {11},
  pages = {1225--1238},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.4780110908},
  abstract = {A digit preference model is developed to describe the preference for estimating gestational age at birth as an even week during the 20th to 36th week of gestation. The model incorporates a probability of misclassification to adjacent even weeks at odd gestational ages, while even gestational ages are assumed correctly classified.The model is extended to allow the misclassification probabilities to decrease linearly with week during the period. A piecewise exponential model is used to model relative risks of delivery associated with a previous spontaneous abortion and a model incorporating digit preference is fitted as a generalized bilinear model in GLIM. The estimates of relative risk in the underlying survival model are virtually the same whether the misclassification is incorporated in the model or ignored.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ABIY923Z\\Pickering - 1992 - Digit preference in estimated gestational age.pdf;C\:\\Users\\devan\\Zotero\\storage\\IXNEJPRS\\Pickering - 1992 - Digit preference in estimated gestational age.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {9}
}

@article{plucinskiPredictingNumberDaily2014,
  title = {Predicting the Number of Daily Human-Caused Bushfires to Assist Suppression Planning in South-West {{Western Australia}}.},
  author = {Plucinski, M. and McCaw, W. L. and Gould, J. S. and Wotton, B. M.},
  year = {2014},
  doi = {10.1071/WFI3090},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\N5ZMQ24I\\publications.html},
  language = {English}
}

@article{plummerJAGSProgramAnalysis2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  shorttitle = {{{JAGS}}},
  author = {Plummer, Martyn},
  year = {2003},
  volume = {124},
  abstract = {JAGS is a program for Bayesian Graphical modelling which aims for compatibility with Classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, briefly describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GBXINJNB\\Plummer - 2003 - JAGS A program for analysis of Bayesian graphical.pdf;C\:\\Users\\devan\\Zotero\\storage\\85RM9SL7\\summary.html},
  journal = {3rd International Workshop on Distributed Statistical Computing (DSC 2003); Vienna, Austria},
  number = {4}
}

@article{podurCompoundPoissonModel2009,
  title = {A Compound Poisson Model for the Annual Area Burned by Forest Fires in the Province of {{Ontario}}},
  author = {Podur, Justin J. and Martell, David L. and Stanford, David},
  year = {2009},
  pages = {n/a-n/a},
  issn = {11804009, 1099095X},
  doi = {10.1002/env.996},
  abstract = {We use the compound Poisson probability distribution to model the annual area burned by forest fires in the Canadian province of Ontario. Models for sums-of-random variables, relevant for modeling aggregate insurance claims and assessing insurance risk are also relevant in modeling aggregate area burned based on sums of sizes of individual fires. Researchers have fit the distribution of fire sizes to the truncated power-law (or Pareto) distribution (Ward et al., 2001) and a four-parameter Weibull distribution (Reed and McKelvey, 2002). Armstrong (1999) fitted a lognormal distribution to annual proportion of area burned by forest fires in a region of Alberta. We derive expressions and moments for aggregate area burned in Ontario using fire data from the Ontario Ministry of Natural Resources (OMNR). We derive expressions for the distribution of area burned for ``severe'' and ``mild'' fire weather scenarios and for ``intensive suppression'' and ``no suppression'' scenarios (represented by the intensive and extensive fire protection zones of the province). These distributions can be used to perform risk analysis of annual area burned. Copyright \# 2009 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6DPLULVF\\Podur et al. - 2009 - A compound poisson model for the annual area burne.pdf;C\:\\Users\\devan\\Zotero\\storage\\PSNDU4RG\\Podur et al. - 2009 - A compound poisson model for the annual area burne.pdf},
  journal = {Environmetrics},
  keywords = {Compound Poisson,Count,Size},
  language = {en}
}

@article{podurSpatialPatternsLightningcaused2003,
  title = {Spatial Patterns of Lightning-Caused Forest Fires in {{Ontario}}, 1976\textendash 1998},
  author = {Podur, Justin and Martell, David L and Csillag, Ferenc},
  year = {2003},
  month = jun,
  volume = {164},
  pages = {1--20},
  issn = {03043800},
  doi = {10.1016/S0304-3800(02)00386-1},
  abstract = {The spatial pattern of forest fire locations is of interest for fire occurrence prediction and for understanding the role of fire in landscape processes. A spatial statistical analysis of lightning-caused fires in the province of Ontario, between 1976 and 1998, was carried out to investigate the spatial pattern of fires, the way they depart from randomness, and the scales at which spatial correlation occurs. Fire locations were found to be spatially clustered. Kernel estimation of the spatial pattern of lightning strikes on days when the dryness of the forest floor exceeded a designated threshold yielded clusters in the same areas as the lightning fire clusters. \textcopyright{} 2002 Elsevier Science B.V. All rights reserved.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JNNK2MEW\\Podur et al. - 2003 - Spatial patterns of lightning-caused forest fires .pdf},
  journal = {Ecological Modelling},
  keywords = {Ignition Probability},
  language = {en},
  number = {1}
}

@article{podurStatisticalQualityControl2002,
  title = {Statistical Quality Control Analysis of Forest Fire Activity in {{Canada}}},
  author = {Podur, Justin and Martell, David L and Knight, Keith},
  year = {2002},
  month = feb,
  volume = {32},
  pages = {195--205},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/x01-183},
  abstract = {Statistical quality-control methods were used to detect significant changes in the mean and variance of the annual fire occurrence and area burned in Canada (1918\textendash 2000), Ontario (1917\textendash 2000), and northwestern Ontario (1917\textendash 2000). The quality-control chart method employed uses the first half of the record of a process as a baseline to test for significant changes in the mean or variance of the process in the second half of the record. Significant increases were detected in annual area burned and in fire occurrence in Canada, Ontario, and northwestern Ontario.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\V4ZUHK89\\Podur et al. - 2002 - Statistical quality control analysis of forest fir.pdf},
  journal = {Canadian Journal of Forest Research},
  language = {en},
  number = {2}
}

@misc{PointProcessbasedModeling,
  title = {Point Process-Based Modeling of Multiple Debris Flow Landslides Using {{INLA}}: An Application to the 2009 {{Messina}} Disaster | {{SpringerLink}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZCJD7J88\\Point process-based modeling of multiple debris fl.pdf;C\:\\Users\\devan\\Zotero\\storage\\3HBRZP36\\10.html},
  howpublished = {https://link.springer.com/article/10.1007\%2Fs00477-018-1518-0}
}

@article{poonImpactsShortcomingsGenetic2016,
  title = {Impacts and Shortcomings of Genetic Clustering Methods for Infectious Disease Outbreaks},
  author = {Poon, Art F. Y.},
  year = {2016},
  month = jul,
  volume = {2},
  pages = {vew031},
  issn = {2057-1577},
  doi = {10.1093/ve/vew031},
  abstract = {For infectious diseases, a genetic cluster is a group of closely related infections that is usually interpreted as representing a recent outbreak of transmission. Genetic clustering methods are becoming increasingly popular for molecular epidemiology, especially in the context of HIV where there is now considerable interest in applying these methods to prioritize groups for public health resources such as pre-exposure prophylaxis. To date, genetic clustering has generally been performed with ad hoc algorithms, only some of which have since been encoded and distributed as free software. These algorithms have seldom been validated on simulated data where clusters are known, and their interpretation and similarities are not transparent to users outside of the field. Here, I provide a brief overview on the development and inter-relationships of genetic clustering methods, and an evaluation of six methods on data simulated under an epidemic model in a risk-structured population. The simulation analysis demonstrates that the majority of clustering methods are systematically biased to detect variation in sampling rates among subpopulations, not variation in transmission rates. I discuss these results in the context of previous work and the implications for public health applications of genetic clustering.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D5VNQQGG\\Poon - 2016 - Impacts and shortcomings of genetic clustering met.pdf},
  journal = {Virus Evolution},
  language = {en},
  number = {2}
}

@article{powellFatigueTwoPilotOperations2008,
  title = {Fatigue in {{Two}}-{{Pilot Operations}}: {{Implications}} for {{Flight}} and {{Duty Time Limitations}}},
  shorttitle = {Fatigue in {{Two}}-{{Pilot Operations}}},
  author = {Powell, David and Spencer, Mick B. and Holland, David and Petrie, Keith J.},
  year = {2008},
  month = nov,
  volume = {79},
  pages = {1047--1050},
  issn = {00956562},
  doi = {10.3357/ASEM.2362.2008},
  file = {C\:\\Users\\devan\\Zotero\\storage\\67KZP9IQ\\Powell et al. - 2008 - Fatigue in Two-Pilot Operations Implications for .pdf;C\:\\Users\\devan\\Zotero\\storage\\9PWPGQGA\\Powell et al. - 2008 - Fatigue in Two-Pilot Operations Implications for .pdf},
  journal = {Aviation, Space, and Environmental Medicine},
  language = {en},
  number = {11}
}

@article{powellPilotFatigueShortHaul2007,
  title = {Pilot {{Fatigue}} in {{Short}}-{{Haul Operations}}: {{Effects}} of {{Number}} of {{Sectors}}, {{Duty Length}}, and {{Time}} of {{Day}}},
  author = {Powell, David M C and Spencer, Mick B and Holland, David and Broadbent, Elizabeth and Petrie, Keith J},
  year = {2007},
  volume = {78},
  pages = {4},
  file = {C\:\\Users\\devan\\Zotero\\storage\\L4XKUJZY\\Powell et al. - 2007 - Pilot Fatigue in Short-Haul Operations Effects of.pdf;C\:\\Users\\devan\\Zotero\\storage\\TSTCVLZQ\\Powell et al. - 2007 - Pilot Fatigue in Short-Haul Operations Effects of.pdf},
  language = {en},
  number = {7}
}

@incollection{preislerForestFireModels2013,
  title = {Forest-{{Fire Models}}},
  booktitle = {Encyclopedia of {{Environmetrics}}},
  author = {Preisler, Haiganoush K. and Ager, Alan A.},
  editor = {{El-Shaarawi}, Abdel H. and Piegorsch, Walter W.},
  year = {2013},
  month = jan,
  pages = {vaf010.pub2},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9780470057339.vaf010.pub2},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SKULFVSJ\\Preisler and Ager - 2013 - Forest-Fire ModelsBased in part on the article “Fo.pdf},
  isbn = {978-0-471-89997-6 978-0-470-05733-9},
  language = {en}
}

@incollection{preislerForestFireModels2014,
  title = {Forest-{{Fire Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Preisler, Haiganoush K. and Ager, Alan A.},
  year = {2014},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat07705},
  abstract = {For applied mathematicians forest-fire models refer mainly to a nonlinear dynamic system often used to simulate spread of fire. For forest managers forest-fire models may pertain to any of the three phases of fire management: prefire planning (fire risk models), fire suppression (fire behavior models), and postfire evaluation (fire effects models). In this context forest-fire models may be statistical, rule-based, process-based, or a combination of all three.},
  copyright = {This Article is a US Government Work and is in the Public Domain in the United States of America.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RAJFLX4V\\9781118445112.html},
  isbn = {978-1-118-44511-2},
  keywords = {burn probability,extreme events,farsite,Huygen's principle,prometheus,spatial–temporal point process},
  language = {en}
}

@article{preislerProbabilityBasedModels2004,
  title = {Probability Based Models for Estimation of Wildfire Risk},
  author = {Preisler, Haiganoush K. and Brillinger, David R. and Burgan, Robert E. and Benoit, J. W.},
  year = {2004},
  volume = {13},
  pages = {133},
  issn = {1049-8001},
  doi = {10.1071/WF02061},
  abstract = {We present a probability-based model for estimating fire risk. Risk is defined using three probabilities: the probability of fire occurrence; the conditional probability of a large fire given ignition; and the unconditional probability of a large fire. The model is based on grouped data at the 1 km2-day cell level. We fit a spatially and temporally explicit non-parametric logistic regression to the grouped data. The probability framework is particularly useful for assessing the utility of explanatory variables, such as fire weather and danger indices for predicting fire risk. The model may also be used to produce maps of predicted probabilities and to estimate the total number of expected fires, or large fires, in a given region and time period. As an example we use historic data from the State of Oregon to study the significance and the forms of relationships between some of the commonly used weather and danger variables on the probabilities of fire. We also produce maps of predicted probabilities for the State of Oregon. Graphs of monthly total numbers of fires are also produced for a small region in Oregon, as an example, and expected numbers are compared to actual numbers of fires for the period 1989\textendash 1996. The fits appear to be reasonable; however, the standard errors are large indicating the need for additional weather or topographic variables.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XIF63T86\\Preisler et al. - 2004 - Probability based models for estimation of wildfir.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {2}
}

@article{preislerSpatiallyExplicitForecasts2011,
  title = {Spatially Explicit Forecasts of Large Wildland Fire Probability and Suppression Costs for {{California}}},
  author = {Preisler, Haiganoush K. and Westerling, Anthony L. and Gebert, Krista M. and {Munoz-Arriola}, Francisco and Holmes, Thomas P.},
  year = {2011},
  volume = {20},
  pages = {508},
  issn = {1049-8001},
  doi = {10.1071/WF09087},
  abstract = {In the last decade, increases in fire activity and suppression expenditures have caused budgetary problems for federal land management agencies. Spatial forecasts of upcoming fire activity and costs have the potential to help reduce expenditures, and increase the efficiency of suppression efforts, by enabling them to focus resources where they have the greatest effect. In this paper, we present statistical models for estimating 1\textendash 6 months ahead spatially explicit forecasts of expected numbers, locations and costs of large fires on a 0.1258 grid with vegetation, topography and hydroclimate data used as predictors. As an example, forecasts for California Federal and State protection responsibility are produced for historic dates and compared with recorded fire occurrence and cost data. The results seem promising in that the spatially explicit forecasts of large fire probabilities seem to match the actual occurrence of large fires, with the exception of years with widespread lightning events, which remain elusive. Forecasts of suppression expenditures did seem to differentiate between low- and high-cost fire years. Maps of forecast levels of expenditures provide managers with a spatial representation of where costly fires are most likely to occur. Additionally, the statistical models provide scientists with a tool for evaluating the skill of spatially explicit fire risk products.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PPVFZ3F9\\Preisler et al. - 2011 - Spatially explicit forecasts of large wildland fir.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {4}
}

@article{preislerStatisticalModelForecasting2007,
  title = {Statistical {{Model}} for {{Forecasting Monthly Large Wildfire Events}} in {{Western United States}}},
  author = {Preisler, Haiganoush K. and Westerling, Anthony L.},
  year = {2007},
  month = jul,
  volume = {46},
  pages = {1020--1030},
  issn = {1558-8424},
  doi = {10.1175/JAM2513.1},
  abstract = {The ability to forecast the number and location of large wildfire events (with specified confidence bounds) is important to fire managers attempting to allocate and distribute suppression efforts during severe fire seasons. This paper describes the development of a statistical model for assessing the forecasting skills of fire-danger predictors and producing 1-month-ahead wildfire-danger probabilities in the western United States. The method is based on logistic regression techniques with spline functions to accommodate nonlinear relationships between fire-danger predictors and probability of large fire events. Estimates were based on 25 yr of historic fire occurrence data (1980\textendash 2004). The model using the predictors monthly average temperature, and lagged Palmer drought severity index demonstrated significant improvement in forecasting skill over historic frequencies (persistence forecasts) of large fire events. The statistical models were particularly amenable to model evaluation and production of probability-based fire-danger maps with prespecified precisions. For example, during the 25 yr of the study for the month of July, an area greater than 400 ha burned in 3\% of locations where the model forecast was low; 11\% of locations where the forecast was moderate; and 76\% of locations where the forecast was extreme. The statistical techniques may be used to assess the skill of forecast fire-danger indices developed at other temporal or spatial scales.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MXMENGV4\\Preisler and Westerling - 2007 - Statistical Model for Forecasting Monthly Large Wi.pdf;C\:\\Users\\devan\\Zotero\\storage\\IGC7X3S6\\jam2513.html},
  journal = {Journal of Applied Meteorology and Climatology},
  number = {7}
}

@article{preislerWildlandFireProbabilities2008,
  title = {Wildland Fire Probabilities Estimated from Weather Model-Deduced Monthly Mean Fire Danger Indices},
  author = {Preisler, Haiganoush K. and Chen, Shyh-Chin and Fujioka, Francis and Benoit, John W. and Westerling, Anthony L.},
  year = {2008},
  volume = {17},
  pages = {305},
  issn = {1049-8001},
  doi = {10.1071/WF06162},
  abstract = {The National Fire Danger Rating System indices deduced from a regional simulation weather model were used to estimate probabilities and numbers of large fire events on monthly and 1-degree grid scales. The weather model simulations and forecasts are ongoing experimental products from the Experimental Climate Prediction Center at the Scripps Institution of Oceanography. The monthly average Fosberg Fire Weather Index, deduced from the weather simulation, along with the monthly average Keetch\textendash Byram Drought Index and Energy Release Component, were found to be more strongly associated with large fire events on a monthly scale than any of the other stand-alone fire weather or danger indices. These selected indices were used in the spatially explicit probability model to estimate the number of large fire events. Historic probabilities were also estimated using spatially smoothed historic frequencies of large fire events. It was shown that the probability model using four fire danger indices outperformed the historic model, an indication that these indices have some skill. Geographical maps of the estimated monthly wildland fire probabilities, developed using a combination of four indices, were produced for each year and were found to give reasonable matches to actual fire events. This method paves a feasible way to assess the skill of climate forecast outputs, from a dynamical meteorological model, in forecasting the probability of wildland fire severity with known precision.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IAVYNPUG\\Preisler et al. - 2008 - Wildland fire probabilities estimated from weather.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {3}
}

@article{proust-limaJointLatentClass2014,
  title = {Joint Latent Class Models for Longitudinal and Time-to-Event Data: {{A}} Review},
  shorttitle = {Joint Latent Class Models for Longitudinal and Time-to-Event Data},
  author = {{Proust-Lima}, C{\'e}cile and S{\'e}ne, Mb{\'e}ry and Taylor, Jeremy MG and {Jacqmin-Gadda}, H{\'e}l{\`e}ne},
  year = {2014},
  month = feb,
  volume = {23},
  pages = {74--90},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280212445839},
  abstract = {Most statistical developments in the joint modelling area have focused on the shared random-effect models that include characteristics of the longitudinal marker as predictors in the model for the timeto-event. A less well-known approach is the joint latent class model which consists in assuming that a latent class structure entirely captures the correlation between the longitudinal marker trajectory and the risk of the event. Owing to its flexibility in modelling the dependency between the longitudinal marker and the event time, as well as its ability to include covariates, the joint latent class model may be particularly suited for prediction problems. This article aims at giving an overview of joint latent class modelling, especially in the prediction context. The authors introduce the model, discuss estimation and goodness-of-fit, and compare it with the shared random-effect model. Then, dynamic predictive tools derived from joint latent class models, as well as measures to evaluate their dynamic predictive accuracy, are presented. A detailed illustration of the methods is given in the context of the prediction of prostate cancer recurrence after radiation therapy based on repeated measures of Prostate Specific Antigen.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9SBZ2N3M\\Proust-Lima et al. - 2014 - Joint latent class models for longitudinal and tim.pdf;C\:\\Users\\devan\\Zotero\\storage\\GGLVXWXX\\Proust-Lima et al. - 2014 - Joint latent class models for longitudinal and tim.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {1}
}

@article{Psarakis2007,
  title = {{{SPC Procedures}} for {{Monitoring Autocorrelated Processes}}},
  author = {Psarakis, S. and Papaleonida, G. E. A.},
  year = {2007},
  volume = {4},
  pages = {501--540},
  issn = {1684-3703},
  doi = {10.1080/16843703.2007.11673168},
  abstract = {The inference about the statistical properties of quality control methodologies is based on the assumptions of normality and independence. In real industrial environments though process data is often correlated or exhibits some serial dependence affecting the efficiency of Statistical Process Control (SPC) methodologies. New technology gives managers the option of using more sophisticated SPC models which more accurately reflect the process being monitored, by relaxing some of the assumptions. The aim of this paper is to present, to apply and to evaluate control charts that are designed to account for autocorrelation.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IIMB23T4\\Psarakis and Papaleonida - 2007 - SPC Procedures for Monitoring Autocorrelated Proce.pdf},
  journal = {Quality Technology {{\&}} Quantitative Management},
  number = {4}
}

@article{psarakisSPCProceduresMonitoring2007,
  title = {{{SPC Procedures}} for {{Monitoring Autocorrelated Processes}}},
  author = {Psarakis, S. and Papaleonida, G. E. A.},
  year = {2007},
  month = jan,
  volume = {4},
  pages = {501--540},
  issn = {1684-3703},
  doi = {10.1080/16843703.2007.11673168},
  abstract = {The inference about the statistical properties of quality control methodologies is based on the assumptions of normality and independence. In real industrial environments though process data is often correlated or exhibits some serial dependence affecting the efficiency of Statistical Process Control (SPC) methodologies. New technology gives managers the option of using more sophisticated SPC models which more accurately reflect the process being monitored, by relaxing some of the assumptions. The aim of this paper is to present, to apply and to evaluate control charts that are designed to account for autocorrelation.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HJNWCQEZ\\Psarakis and Papaleonida - 2007 - SPC Procedures for Monitoring Autocorrelated Proce.pdf;C\:\\Users\\devan\\Zotero\\storage\\RNSDWDLS\\Psarakis and Papaleonida - 2007 - SPC Procedures for Monitoring Autocorrelated Proce.pdf},
  journal = {Quality Technology \& Quantitative Management},
  language = {en},
  number = {4}
}

@article{pythonBayesianApproachModelling2016,
  title = {A {{Bayesian Approach}} to {{Modelling Fine}}-{{Scale Spatial Dynamics}} of {{Non}}-{{State Terrorism}}: {{World Study}}, 2002-2013},
  shorttitle = {A {{Bayesian Approach}} to {{Modelling Fine}}-{{Scale Spatial Dynamics}} of {{Non}}-{{State Terrorism}}},
  author = {Python, Andr{\'e} and Illian, Janine and {Jones-Todd}, Charlotte and Blangiardo, Marta},
  year = {2016},
  month = oct,
  abstract = {To this day, terrorism persists as a worldwide threat, as exemplified by the ongoing lethal attacks perpetrated by ISIS in Iraq, Syria, Al Qaeda in Yemen, and Boko Haram in Nigeria. In response, states deploy various counterterrorism policies, the costs of which could be reduced through efficient preventive measures. Statistical models able to account for complex spatio-temporal dependencies have not yet been applied, despite their potential for providing guidance to explain and prevent terrorism. In an effort to address this shortcoming, we employ hierarchical models in a Bayesian context, where the spatial random field is represented by a stochastic partial differential equation. Our results confirm the contagious nature of the lethality of terrorism and the number of lethal terrorist attacks in both space and time. Moreover, the frequency of lethal attacks tends to be higher in more economically developed areas, close to large cities, and within democratic countries. In contrast, attacks are more likely to be lethal far away from large cities, at higher altitudes, in less economically developed areas, and in locations with higher ethnic diversity. We argue that, on a local scale, the lethality of terrorism and the frequency of lethal attacks are driven by antagonistic mechanisms.},
  archivePrefix = {arXiv},
  eprint = {1610.01215},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C9H5JCZF\\Python et al. - 2016 - A Bayesian Approach to Modelling Fine-Scale Spatia.pdf;C\:\\Users\\devan\\Zotero\\storage\\HPV3CBED\\Python et al. - 2016 - A Bayesian Approach to Modelling Fine-Scale Spatia.pdf},
  journal = {arXiv:1610.01215 [stat]},
  keywords = {62P25,Statistics - Applications},
  language = {en},
  primaryClass = {stat}
}

@article{pythonBayesianApproachModelling2016a,
  title = {A {{Bayesian Approach}} to {{Modelling Fine}}-{{Scale Spatial Dynamics}} of {{Non}}-{{State Terrorism}}: {{World Study}}, 2002-2013},
  shorttitle = {A {{Bayesian Approach}} to {{Modelling Fine}}-{{Scale Spatial Dynamics}} of {{Non}}-{{State Terrorism}}},
  author = {Python, Andr{\'e} and Illian, Janine and {Jones-Todd}, Charlotte and Blangiardo, Marta},
  year = {2016},
  month = oct,
  abstract = {To this day, terrorism persists as a worldwide threat, as exemplified by the ongoing lethal attacks perpetrated by ISIS in Iraq, Syria, Al Qaeda in Yemen, and Boko Haram in Nigeria. In response, states deploy various counterterrorism policies, the costs of which could be reduced through efficient preventive measures. Statistical models able to account for complex spatio-temporal dependencies have not yet been applied, despite their potential for providing guidance to explain and prevent terrorism. In an effort to address this shortcoming, we employ hierarchical models in a Bayesian context, where the spatial random field is represented by a stochastic partial differential equation. Our results confirm the contagious nature of the lethality of terrorism and the number of lethal terrorist attacks in both space and time. Moreover, the frequency of lethal attacks tends to be higher in more economically developed areas, close to large cities, and within democratic countries. In contrast, attacks are more likely to be lethal far away from large cities, at higher altitudes, in less economically developed areas, and in locations with higher ethnic diversity. We argue that, on a local scale, the lethality of terrorism and the frequency of lethal attacks are driven by antagonistic mechanisms.},
  archivePrefix = {arXiv},
  eprint = {1610.01215},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\V87B3UIA\\Python et al. - 2016 - A Bayesian Approach to Modelling Fine-Scale Spatia.pdf},
  journal = {arXiv:1610.01215 [stat]},
  keywords = {62P25,Statistics - Applications},
  language = {en},
  primaryClass = {stat}
}

@article{quRandomEffectsModels1996,
  title = {Random {{Effects Models}} in {{Latent Class Analysis}} for {{Evaluating Accuracy}} of {{Diagnostic Tests}}},
  author = {Qu, Yinsheng and Tan, Ming and Kutner, Michael H.},
  year = {1996},
  month = sep,
  volume = {52},
  pages = {797},
  issn = {0006341X},
  doi = {10.2307/2533043},
  abstract = {When the results of a reference (or gold standard) test are missing or not error-free, the accuracy of diagnostic tests is often assessed through latent class models with two latent classes, representing diseased or nondiseased status. Such models, however, require that conditional on the true disease status, the tests are statistically independent, an assumption often violated in practice. Consequently, the model generally fits the data poorly. In this paper, we develop a general latent class model with random effects to model the conditional dependence among multiple diagnostic tests (or readers). We also develop a graphical method for checking whether or not the conditional dependence is of concern and for identifying the pattern of the correlation. Using the randomeffects model and the graphical method, a simple adequate model that is easy to interpret can be obtained. The methods are illustrated with three examples from the biometric literature. The proposed methodology is also applicable when the true disease status is indeed known and conditional dependence could well be present.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\A88UNEM9\\Qu et al. - 1996 - Random Effects Models in Latent Class Analysis for.pdf;C\:\\Users\\devan\\Zotero\\storage\\NRWPENXE\\Qu et al. - 1996 - Random Effects Models in Latent Class Analysis for.pdf;C\:\\Users\\devan\\Zotero\\storage\\WYX2ZEYR\\Qu et al. - 1996 - Random Effects Models in Latent Class Analysis for.pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{racinePRIMERREGRESSIONSPLINES,
  title = {A {{PRIMER ON REGRESSION SPLINES}}},
  author = {Racine, Jeffrey S},
  pages = {11},
  file = {C\:\\Users\\devan\\Zotero\\storage\\68T7T752\\Racine - A PRIMER ON REGRESSION SPLINES.pdf},
  language = {en}
}

@article{raftery2006estimating,
  title = {Estimating the Integrated Likelihood via Posterior Simulation Using the Harmonic Mean Identity},
  author = {Raftery, Adrian E and Newton, Michael A and Satagopan, Jaya M and Krivitsky, Pavel N},
  year = {2006},
  publisher = {{bepress}}
}

@article{rajalaFamilySpatialBiodiversity2012,
  title = {A Family of Spatial Biodiversity Measures Based on Graphs},
  author = {Rajala, Tuomas and Illian, Janine},
  year = {2012},
  month = dec,
  volume = {19},
  pages = {545--572},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-012-0200-9},
  abstract = {While much research in ecology has focused on spatially explicit modelling as well as on measures of biodiversity, the concept of spatial (or local) biodiversity has been discussed very little. This paper generalises existing measures of spatial biodiversity and introduces a family of spatial biodiversity measures by flexibly defining the notion of the individuals' neighbourhood within the framework of graphs associated to a spatial point pattern. We consider two non-independent aspects of spatial biodiversity, scattering, i.e. the spatial arrangement of the individuals in the study area and exposure, the local diversity in an individual's neighbourhood. A simulation study reveals that measures based on the most commonly used neighbourhood defined by the geometric graph do not distinguish well between scattering and exposure. This problem is much less pronounced when other graphs are used. In an analysis of the spatial diversity in a rainforest, the results based on the geometric graph have been shown to spuriously indicate a decrease in spatial biodiversity when no such trend was detected by the other types of neighbourhoods. We also show that the choice of neighbourhood markedly impacts on the classification of species according to how strongly and in what way different species spatially structure species diversity. Clearly, in an analysis of spatial or local diversity an appropriate choice of local neighbourhood is crucial in particular in terms of the biological interpretation of the results. Due to its general definition, the approach discussed here offers the necessary flexibility that allows suitable and varying neighbourhood structures to be chosen.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IENKKWAL\\Rajala and Illian - 2012 - A family of spatial biodiversity measures based on.pdf;C\:\\Users\\devan\\Zotero\\storage\\T7D3G9FM\\Rajala and Illian - 2012 - A family of spatial biodiversity measures based on.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {4}
}

@article{rambautCausesConsequencesHIV2004,
  title = {The Causes and Consequences of {{HIV}} Evolution},
  author = {Rambaut, Andrew and Posada, David and Crandall, Keith A. and Holmes, Edward C.},
  year = {2004},
  month = jan,
  volume = {5},
  pages = {52--61},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg1246},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5AKY9QI3\\Rambaut et al. - 2004 - The causes and consequences of HIV evolution.pdf},
  journal = {Nature Reviews Genetics},
  language = {en},
  number = {1}
}

@article{rambautDynamicNomenclatureProposal2020,
  title = {A Dynamic Nomenclature Proposal for {{SARS}}-{{CoV}}-2 Lineages to Assist Genomic Epidemiology},
  author = {Rambaut, Andrew and Holmes, Edward C. and O'Toole, {\'A}ine and Hill, Verity and McCrone, John T. and Ruis, Christopher and {du Plessis}, Louis and Pybus, Oliver G.},
  year = {2020},
  month = jul,
  issn = {2058-5276},
  doi = {10.1038/s41564-020-0770-5},
  file = {C\:\\Users\\devan\\Zotero\\storage\\N3FHIXSP\\Rambaut et al. - 2020 - A dynamic nomenclature proposal for SARS-CoV-2 lin.pdf},
  journal = {Nature Microbiology},
  language = {en}
}

@article{ramsayDistributionCompoundSums2009,
  title = {The Distribution of Compound Sums of {{Pareto}} Distributed Losses},
  author = {Ramsay, Colin M.},
  year = {2009},
  month = mar,
  volume = {2009},
  pages = {27--37},
  issn = {0346-1238, 1651-2030},
  doi = {10.1080/03461230802627835},
  file = {C\:\\Users\\devan\\Zotero\\storage\\R54ZQ68P\\Ramsay - 2009 - The distribution of compound sums of Pareto distri.pdf},
  journal = {Scandinavian Actuarial Journal},
  language = {en},
  number = {1}
}

@book{ramsayFunctionalDataAnalysis2006,
  title = {Functional {{Data Analysis}}},
  author = {Ramsay, James and Silverman, B. W.},
  year = {2006},
  month = jun,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Scientists today collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modelling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drwan from growth analysis, meterology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. Much of the material is based on the authors' own work, some of which appears here for the first time. Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He draws on his collaboration with researchers in speech articulation, motor control, meteorology, psychology, and human physiology to illustrate his technical contributions to functional data analysis in a wide range of statistical and application journals. Bernard Silverman, author of the highly regarded "Density Estimation for Statistics and Data Analysis," and coauthor of "Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach," is Professor of Statistics at Bristol University. His published work on smoothing methods and other aspects of applied, computational, and theoretical statistics has been recognized by the Presidents' Award of the Committee of Presidents of Statistical Societies, and the award of two Guy Medals by the Royal Statistical Society.},
  googlebooks = {REzuyz\_V6OQC},
  isbn = {978-0-387-22751-1},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  language = {en}
}

@article{rasmussenTemporalPointProcesses,
  title = {Temporal Point Processes: The Conditional Intensity Function},
  author = {Rasmussen, Jakob Gulddahl},
  pages = {19},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DD32L2X4\\Rasmussen - Temporal point processes the conditional intensit.pdf},
  language = {en}
}

@article{rathbunAsymptoticPropertiesEstimators1994,
  title = {Asymptotic {{Properties}} of {{Estimators}} for the {{Parameters}} of {{Spatial Inhomogeneous Poisson Point Processes}}},
  author = {Rathbun, Stephen L. and Cressie, Noel},
  year = {1994},
  month = mar,
  volume = {26},
  pages = {122--154},
  issn = {0001-8678, 1475-6064},
  doi = {10.2307/1427583},
  abstract = {Consider a spatial point pattern realized from an inhomogeneous Poisson process on a bounded Borel set A c Rd, with intensity function A(s; 0), where 0 E E c -k. In this article, we show that the maximum likelihood estimator 0A and the Bayes estimator 0a are consistent, asymptotically normal, and asymptotically efficient as the sample region A T Rd. These results extend asymptotic results of Kutoyants (1984), proved for an inhomogeneous Poisson process on [0, T] c=R, where T - coo. They also formalize (and extend to the multiparameter case) results announced by Krickeberg (1982), for the spatial domain Rd. Furthermore, a Cram6r-Rao lower bound is found for any estimator OA* of 0. The asymptotic properties of 0A and 0A are considered for modulated (Cox (1972)), and linear Poisson processes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8GMS8GMT\\Rathbun and Cressie - 1994 - Asymptotic Properties of Estimators for the Parame.pdf;C\:\\Users\\devan\\Zotero\\storage\\CE2KMQH8\\Rathbun and Cressie - 1994 - Asymptotic Properties of Estimators for the Parame.pdf},
  journal = {Advances in Applied Probability},
  language = {en},
  number = {1}
}

@article{rathbunAsymptoticPropertiesMaximum1996,
  title = {Asymptotic Properties of the Maximum Likelihood Estimator for Spatio-Temporal Point Processes},
  author = {Rathbun, Stephen L.},
  year = {1996},
  month = apr,
  volume = {51},
  pages = {55--74},
  issn = {03783758},
  doi = {10.1016/0378-3758(95)00070-4},
  abstract = {Consider a spatio-temporal point process whose events occur at times in the interval I0, T] and at corresponding locations in a region X. Such processes can be modeled through their conditional intensity function A(s, t;0); 0 \textasciitilde},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WE69RI4E\\Rathbun - 1996 - Asymptotic properties of the maximum likelihood es.pdf},
  journal = {Journal of Statistical Planning and Inference},
  language = {en},
  number = {1}
}

@misc{RCoreTeam2018,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2018},
  publisher = {{R Foundation for Statistical Computing}}
}

@misc{rcoreteamLanguageEnvironmentStatistical2019,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2019},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@article{reedPowerlawBehaviourParametric2002,
  title = {Power-Law Behaviour and Parametric Models for the Size-Distribution of Forest Fires},
  author = {Reed, William J. and McKelvey, Kevin S.},
  year = {2002},
  month = may,
  volume = {150},
  pages = {239--254},
  issn = {03043800},
  doi = {10.1016/S0304-3800(01)00483-5},
  abstract = {This paper examines the distribution of areas burned in forest fires. Empirical size distributions, derived from extensive fire records, for six regions in North America are presented. While they show some commonalities, it appears that a simple power-law distribution of sizes, as has been suggested by some authors, is too simple to describe the distributions over their full range. A stochastic model for the spread and extinguishment of fires is used to examine conditions for power-law behaviour and deviations from it. The concept of the extinguishment growth rate ratio (EGRR) is developed. A null model with constant EGRR leads to a power-law distribution, but this does not appear to hold empirically for the data sets examined. Some alternative parametric forms for the size distribution are presented, with a four-parameter `competing hazards' model providing the overall best fit.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Y4CCDIQ6\\Reed and McKelvey - 2002 - Power-law behaviour and parametric models for the .pdf},
  journal = {Ecological Modelling},
  language = {en},
  number = {3}
}

@article{rennerPointProcessModels2015,
  title = {Point Process Models for Presence-Only Analysis},
  author = {Renner, Ian W. and Elith, Jane and Baddeley, Adrian and Fithian, William and Hastie, Trevor and Phillips, Steven J. and Popovic, Gordana and Warton, David I.},
  year = {2015},
  volume = {6},
  pages = {366--379},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12352},
  abstract = {Presence-only data are widely used for species distribution modelling, and point process regression models are a flexible tool that has considerable potential for this problem, when data arise as point events. In this paper, we review point process models, some of their advantages and some common methods of fitting them to presence-only data. Advantages include (and are not limited to) clarification of what the response variable is that is modelled; a framework for choosing the number and location of quadrature points (commonly referred to as pseudo-absences or `background points') objectively; clarity of model assumptions and tools for checking them; models to handle spatial dependence between points when it is present; and ways forward regarding difficult issues such as accounting for sampling bias. Point process models are related to some common approaches to presence-only species distribution modelling, which means that a variety of different software tools can be used to fit these models, including maxent or generalised linear modelling software.},
  copyright = {\textcopyright{} 2015 The Authors. Methods in Ecology and Evolution \textcopyright{} 2015 British Ecological Society},
  file = {C\:\\Users\\devan\\Zotero\\storage\\V8DPRLWT\\Renner et al. - 2015 - Point process models for presence-only analysis.pdf;C\:\\Users\\devan\\Zotero\\storage\\EYEIFI77\\2041-210X.html},
  journal = {Methods in Ecology and Evolution},
  keywords = {Cox processes,Gibbs processes,maxent,pseudo-absences,species distribution modelling},
  language = {en},
  number = {4}
}

@article{renoufJointModellingLiver,
  title = {Joint {{Modelling}} in {{Liver Transplantation}}},
  author = {Renouf, Elizabeth M},
  pages = {116},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5TRK3X9B\\Renouf - Joint Modelling in Liver Transplantation.pdf},
  language = {en}
}

@article{renshawDisentanglingMarkPoint2007,
  title = {Disentangling Mark/Point Interaction in Marked-Point Processes},
  author = {Renshaw, Eric and Mateu, Jorge and Saura, Fuensanta},
  year = {2007},
  month = mar,
  volume = {51},
  pages = {3123--3144},
  issn = {01679473},
  doi = {10.1016/j.csda.2006.07.035},
  abstract = {In many spatial situations, not only do the point locations of mark variables (e.g. tree heights) play a key role in the underlying process generating mechanism, but there can be interdependence between the marks and points themselves. Although Monte Carlo frequency-domain analyses can separate mark and point structure, theoretical advances for marks have so far related to the conditional mark spectrum based on a given point structure. A `discrepancy function' is therefore developed which isolates the spatial structure of the marks alone, and involves a harmonic decomposition of the mark frequencies. The concept is introduced via various simulated examples based on mark cosine waves and thinned point processes, with particular attention given to the construction of sequential and simultaneous search procedures for developing parameter estimates. The procedure is then applied to Spanish daily ozone data with missing values, a spatial growth-interaction process, and a classic longleaf pine data set from the Wade Tract in Georgia, USA.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\E77CGKCR\\Renshaw et al. - 2007 - Disentangling markpoint interaction in marked-poi.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {6}
}

@article{ridoutModellingDigitPreference1991,
  title = {Modelling {{Digit Preference}} in {{Fecundability Studies}}},
  author = {Ridout, Martin S. and Morgan, Byron J. T.},
  year = {1991},
  month = dec,
  volume = {47},
  pages = {1423},
  issn = {0006341X},
  doi = {10.2307/2532396},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EELBH6DJ\\Ridout and Morgan - 1991 - Modelling Digit Preference in Fecundability Studie.pdf;C\:\\Users\\devan\\Zotero\\storage\\UZYB2MUW\\Ridout and Morgan - 1991 - Modelling Digit Preference in Fecundability Studie.pdf},
  journal = {Biometrics},
  number = {4}
}

@article{ripleySecondorderAnalysisStationary1976,
  title = {The Second-Order Analysis of Stationary Point Processes},
  author = {Ripley, B. D.},
  year = {1976},
  month = jun,
  volume = {13},
  pages = {255--266},
  issn = {0021-9002, 1475-6072},
  doi = {10.2307/3212829},
  abstract = {This paper provides a rigorous foundation for the secoid-order analysis of stationary point processes on general spaces. It illuminates the results of Bartlett on spatial point processes, and covers the point processes of stochastic geometry, including the line and hyperplane processes of Davidson and Krickeberg. The main tool is the decomposition of moment measures pioneered by Krickeberg and Vere-Jones. Finally some practical aspects of the analysis of point processes are discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9G9MZZ8J\\Ripley - 1976 - The second-order analysis of stationary point proc.pdf;C\:\\Users\\devan\\Zotero\\storage\\GK7J7YWE\\Ripley - 1976 - The second-order analysis of stationary point proc.pdf},
  journal = {Journal of Applied Probability},
  language = {en},
  number = {2}
}

@article{ripleyTestsRandomnessSpatial1979,
  title = {Tests of `{{Randomness}}' for {{Spatial Point Patterns}}},
  author = {Ripley, B. D.},
  year = {1979},
  volume = {41},
  pages = {368--374},
  abstract = {Tests of "randomness" and methods of edge-correction for spatial point patterns are surveyed. The asymptotic distribution theory and power of tests based on the nearest-neighbour distances and estimates of the variance function are investigated.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HGTYI7X5\\Ripley - 1979 - Tests of `Randomness' for Spatial Point Patterns.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {3}
}

@article{rizopoulosSharedParameterModels2008,
  title = {Shared Parameter Models under Random Effects Misspecification},
  author = {Rizopoulos, D. and Verbeke, G. and Molenberghs, G.},
  year = {2008},
  month = jan,
  volume = {95},
  pages = {63--74},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asm087},
  abstract = {A common objective in longitudinal studies is the investigation of the association structure between a longitudinal response process and the time to an event of interest. An attractive paradigm for the joint modelling of longitudinal and survival processes is the shared parameter framework, where a set of random effects is assumed to induce their interdependence. In this work, we propose an alternative parameterization for shared parameter models and investigate the effect of misspecifying the random effects distribution in the parameter estimates and their standard errors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CPA6DPRH\\Rizopoulos et al. - 2008 - Shared parameter models under random effects missp.pdf;C\:\\Users\\devan\\Zotero\\storage\\P9VPJJYY\\Rizopoulos et al. - 2008 - Shared parameter models under random effects missp.pdf},
  journal = {Biometrika},
  language = {en},
  number = {1}
}

@article{rizopoulosSharedParameterModels2008a,
  title = {Shared {{Parameter Models}} under {{Random Effects Misspecification}}},
  author = {Rizopoulos, Dimitris and Verbeke, Geert and Molenberghs, Geert},
  year = {2008},
  volume = {95},
  pages = {63--74},
  issn = {0006-3444},
  abstract = {A common objective in longitudinal studies is the investigation of the association structure between a longitudinal response process and the time to an event of interest. An attractive paradigm for the joint modelling of longitudinal and survival processes is the shared parameter framework, where a set of random effects is assumed to induce their interdependence. In this work, we propose an alternative parameterization for shared parameter models and investigate the effect of misspecifying the random effects distribution in the parameter estimates and their standard errors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DWSNXJI6\\Rizopoulos et al. - 2008 - Shared Parameter Models under Random Effects Missp.pdf},
  journal = {Biometrika},
  number = {1}
}

@article{robertsMeasuresTestsHeaping2001,
  title = {Measures and Tests of Heaping in Discrete Quantitative Distributions},
  author = {Roberts, John M. and Brewer, Devon D.},
  year = {2001},
  month = sep,
  volume = {28},
  pages = {887--896},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/02664760120074960},
  abstract = {Heaping is often found in discrete quantitative data based on subject responses to open-ended interview questions or observer assessments. Heaping occurs when subjects or observers prefer some set of numbers as responses (e.g. multiples of 5) simply because of the features of this set. Although heaping represents a common type of measurement error, apparently no prior general measure of heaping exists. We present simple measures and tests of heaping in discrete quantitative data, illustrate them with data from an epidemiologic study, and evaluate the bias of these statistics. These techniques permit formal measurement of heaping and facilitate comparisons of the degree of heaping in data from di\th{} erent samples, substantive domains, and data collection methods.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SXUZX83B\\Roberts and Brewer - 2001 - Measures and tests of heaping in discrete quantita.pdf;C\:\\Users\\devan\\Zotero\\storage\\YURNQXPX\\Roberts and Brewer - 2001 - Measures and tests of heaping in discrete quantita.pdf},
  journal = {Journal of Applied Statistics},
  language = {en},
  number = {7}
}

@article{robinsonThatBLUPGood1991,
  title = {That {{BLUP}} Is a {{Good Thing}}: {{The Estimation}} of {{Random Effects}}},
  author = {Robinson, G. K.},
  year = {1991},
  volume = {6},
  pages = {15--32},
  abstract = {In animal breeding, Best Linear Unbiased Prediction, or BLUP, is a technique for estimating genetic merits. In general, it is a method of estimating random effects. It can be used to derive the Kalman filter, the method of Kriging used for ore reserve estimation, credibility theory used to work out insurance premiums, and Hoadley's quality measurement plan used to estimate a quality index. It can be used for removing noise from images and for small-area estimation. This paper presents the theory of BLUP, some examples of its application and its relevance to the foundations of statistics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ARDI2U5U\\Robinson - 1991 - That BLUP is a Good Thing The Estimation of Rando.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZLUHIB83\\Robinson - 1991 - That BLUP is a Good Thing The Estimation of Rando.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {1}
}

@article{rodrigues-mottaMultivariateModelsCorrelated2013,
  title = {Multivariate Models for Correlated Count Data},
  author = {{Rodrigues-Motta}, Mariana and Pinheiro, Hildete P. and Martins, Eduardo G. and Ara{\'u}jo, M{\'a}rcio S. and {dos Reis}, S{\'e}rgio F.},
  year = {2013},
  month = jul,
  volume = {40},
  pages = {1586--1596},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/02664763.2013.789098},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5MARRGLB\\Rodrigues-Motta et al. - 2013 - Multivariate models for correlated count data.pdf;C\:\\Users\\devan\\Zotero\\storage\\WG9IY8QU\\Rodrigues-Motta et al. - 2013 - Multivariate models for correlated count data.pdf;C\:\\Users\\devan\\Zotero\\storage\\X5ZXNWXJ\\Rodrigues-Motta et al. - 2013 - Multivariate models for correlated count data.pdf},
  journal = {Journal of Applied Statistics},
  language = {en},
  number = {7}
}

@article{romanowiczRecursiveEstimationApproach2006,
  title = {A Recursive Estimation Approach to the Spatio-Temporal Analysis and Modelling of Air Quality Data},
  author = {Romanowicz, R. and Young, P. and Brown, P. and Diggle, P.},
  year = {2006},
  month = jun,
  volume = {21},
  pages = {759--769},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2005.02.004},
  abstract = {This paper presents the methodology for the spatial and temporal interpolation of air quality data. As a practical example, the methodology is applied to the daily nitric oxide NO concentrations measured at 23 stations around Paris. Analysis of the temporal and spatial variability of observations of NO in the Paris area is divided into: (i) time series analysis of AirParif data; and (ii) development of combined spatial and temporal analysis techniques using NO observations from 19 stations. The first part of the paper shows how advanced methods of nonstationary time series analysis can be used to interpolate the data sets of NO concentrations over periods where measurements are missing and to decompose the time series into trend and harmonic components. The results of this analysis applied to 19 stations around Paris are then used in further spatio-temporal analysis of the data. This consists of two steps: (i) preliminary analysis of spatial relations within the data sets; and (ii) the development of a spatiotemporal model for log-transformed NO measurements. The results of the analysis indicate that the simple spatio-temporal model consisting of trend and noise efficiently represents the spatio-temporal variations in the data and it can be applied to predict air pollution variations in time and space at un-sampled locations.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\K6KCZNXH\\Romanowicz et al. - 2006 - A recursive estimation approach to the spatio-temp.pdf;C\:\\Users\\devan\\Zotero\\storage\\NCPC3UNM\\Romanowicz et al. - 2006 - A recursive estimation approach to the spatio-temp.pdf},
  journal = {Environmental Modelling \& Software},
  language = {en},
  number = {6}
}

@book{rueAdvancedSpatialModeling,
  title = {Advanced {{Spatial Modeling}} with {{Stochastic Partial Differential Equations Using R}} and {{INLA}}},
  author = {Rue, Virgilio G{\'o}mez-Rubio, Haakon Bakka, Amanda Lenzi, Daniela Castro-Camilo, Daniel Simpson, Finn Lindgren {and} H{\aa}vard, Elias T. Krainski},
  abstract = {Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WWLJZPQX\\ch-lcox.html}
}

@article{rueApproximateBayesianInference2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  volume = {71},
  pages = {319--392},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2008.00700.x},
  abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
  copyright = {\textcopyright{} 2009 Royal Statistical Society},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3IHPEBCQ\\Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;C\:\\Users\\devan\\Zotero\\storage\\YZVEZMZG\\j.1467-9868.2008.00700.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
  language = {en},
  number = {2}
}

@article{rueBayesianComputingINLA2016,
  title = {Bayesian {{Computing}} with {{INLA}}: {{A Review}}},
  shorttitle = {Bayesian {{Computing}} with {{INLA}}},
  author = {Rue, H{\aa}vard and Riebler, Andrea Ingeborg and S{\o}rbye, Sigrunn Holbek and Illian, Janine B. and Simpson, Daniel Peter and Lindgren, Finn Kristian},
  year = {2016},
  month = dec,
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-060116-054045},
  abstract = {The key operation in Bayesian inference is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre-Simon Laplace (1774). This simple idea approximates the integrand with a second-order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of integrated nested Laplace approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we discuss the reasons for the success of the INLA approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute, and why LGMs make such a useful concept for Bayesian computing.},
  copyright = {openAccess},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9HWKDQZN\\Rue et al. - 2016 - Bayesian Computing with INLA A Review.pdf;C\:\\Users\\devan\\Zotero\\storage\\M63DC7BP\\13371.html},
  language = {eng}
}

@article{rueBayesianComputingINLA2016a,
  title = {Bayesian {{Computing}} with {{INLA}}: {{A Review}}},
  shorttitle = {Bayesian {{Computing}} with {{INLA}}},
  author = {Rue, H{\aa}vard and Riebler, Andrea and S{\o}rbye, Sigrunn H. and Illian, Janine B. and Simpson, Daniel P. and Lindgren, Finn K.},
  year = {2016},
  month = apr,
  abstract = {The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to PierreSimon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.},
  archivePrefix = {arXiv},
  eprint = {1604.00860},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5HSKX74V\\Rue et al. - 2016 - Bayesian Computing with INLA A Review.pdf;C\:\\Users\\devan\\Zotero\\storage\\D2I47INK\\Rue et al. - 2016 - Bayesian Computing with INLA A Review.pdf},
  journal = {arXiv:1604.00860 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@misc{ryderGoalPrevention2004,
  title = {Goal {{Prevention}}},
  author = {Ryder, Alan},
  year = {2004},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IN55NIVT\\Goal_Prevention_2004.pdf},
  howpublished = {http://hockeyanalytics.com/Research\_files/ Goal\_Prevention\_2004.pdf},
  journal = {Hockey Analytics}
}

@misc{ryderPoissonToolbox2004,
  title = {Poisson {{Toolbox}}},
  author = {Ryder, Alan},
  year = {2004},
  file = {C\:\\Users\\devan\\Zotero\\storage\\S8JMERB8\\Poisson_Toolbox.pdf},
  howpublished = {http://www.hockeyanalytics.com/Research\_files/Poisson\_Toolbox.pdf},
  journal = {Hockey Analytics}
}

@misc{ryderShotQuality2004,
  title = {Shot {{Quality}}},
  author = {Ryder, Alan},
  year = {2004},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MNZBNCFJ\\Shot_Quality.pdf},
  howpublished = {http://hockeyanalytics.com/Research\_files/Shot\_Quality.pdf},
  journal = {Hockey Analytics}
}

@article{sadykovaBayesianJointModels2017,
  title = {Bayesian Joint Models with {{INLA}} Exploring Marine Mobile Predator\textendash Prey and Competitor Species Habitat Overlap},
  author = {Sadykova, Dinara and Scott, Beth E. and De Dominicis, Michela and Wakelin, Sarah L. and Sadykov, Alexander and Wolf, Judith},
  year = {2017},
  month = jun,
  volume = {7},
  pages = {5212--5226},
  issn = {2045-7758},
  doi = {10.1002/ece3.3081},
  abstract = {Understanding spatial physical habitat selection driven by competition and/or predator\textendash prey interactions of mobile marine species is a fundamental goal of spatial ecology. However, spatial counts or density data for highly mobile animals often (1) include excess zeros, (2) have spatial correlation, and (3) have highly nonlinear relationships with physical habitat variables, which results in the need for complex joint spatial models. In this paper, we test the use of Bayesian hierarchical hurdle and zero-inflated joint models with integrated nested Laplace approximation (INLA), to fit complex joint models to spatial patterns of eight mobile marine species (grey seal, harbor seal, harbor porpoise, common guillemot, black-legged kittiwake, northern gannet, herring, and sandeels). For each joint model, we specified nonlinear smoothed effect of physical habitat covariates and selected either competing species or predator\textendash prey interactions. Out of a range of six ecologically important physical and biologic variables that are predicted to change with climate change and large-scale energy extraction, we identified the most important habitat variables for each species and present the relationships between these bio/physical variables and species distributions. In particular, we found that net primary production played a significant role in determining habitat preferences of all the selected mobile marine species. We have shown that the INLA method is well-suited for modeling spatially correlated data with excessive zeros and is an efficient approach to fit complex joint spatial models with nonlinear effects of covariates. Our approach has demonstrated its ability to define joint habitat selection for both competing and prey\textendash predator species that can be relevant to numerous issues in the management and conservation of mobile marine species.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D5R88YRN\\ECE3-7-5212-s002.txt;C\:\\Users\\devan\\Zotero\\storage\\RQ9JRSCT\\Sadykova et al. - 2017 - Bayesian joint models with INLA exploring marine m.pdf},
  journal = {Ecology and Evolution},
  number = {14},
  pmcid = {PMC5528225},
  pmid = {29242741}
}

@article{saitouNeighborjoiningMethodNew1987,
  title = {The Neighbor-Joining Method: A New Method for Reconstructing Phylogenetic Trees.},
  shorttitle = {The Neighbor-Joining Method},
  author = {Saitou, Naruya and Nei, Nasatoshi},
  year = {1987},
  month = jul,
  volume = {4},
  pages = {406--425},
  issn = {1537-1719},
  doi = {10.1093/oxfordjournals.molbev.a040454},
  file = {C\:\\Users\\devan\\Zotero\\storage\\25PKWXZ5\\1987 - The neighbor-joining method a new method for reco.pdf},
  journal = {Molecular Biology and Evolution},
  language = {en},
  number = {4}
}

@article{sandholtzChuckersMeasuringLineup2019,
  title = {Chuckers: {{Measuring Lineup Shot Distribution Optimality Using Spatial Allocative Efficiency Models}}},
  author = {Sandholtz, Nathan and Mortensen, Jacob and Bornn, Luke},
  year = {2019},
  pages = {13},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZC5NRFWJ\\Sandholtz et al. - 2019 - Chuckers Measuring Lineup Shot Distribution Optim.pdf},
  language = {en}
}

@article{sangNonignorableMissingness,
  title = {Non-Ignorable {{Missingness}}},
  author = {Sang, Qiuguang},
  pages = {105},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TS698WX7\\Sang - Non-ignorable Missingness.pdf;C\:\\Users\\devan\\Zotero\\storage\\UX4R3ZRV\\Sang - Non-ignorable Missingness.pdf},
  language = {en}
}

@article{savilleTestingRandomEffects2009,
  title = {Testing {{Random Effects}} in the {{Linear Mixed Model Using Approximate Bayes Factors}}},
  author = {Saville, Benjamin R. and Herring, Amy H.},
  year = {2009},
  month = jun,
  volume = {65},
  pages = {369--376},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2008.01107.x},
  abstract = {Deciding which predictor effects may vary across subjects is a difficult issue. Standard model selection criteria and test procedures are often inappropriate for comparing models with different numbers of random effects due to constraints on the parameter space of the variance components. Testing on the boundary of the parameter space changes the asymptotic distribution of some classical test statistics and causes problems in approximating Bayes factors. We propose a simple approach for testing random effects in the linear mixed model using Bayes factors. We scale each random effect to the residual variance and introduce a parameter that controls the relative contribution of each random effect free of the scale of the data. We integrate out the random effects and the variance components using closed-form solutions. The resulting integrals needed to calculate the Bayes factor are low-dimensional integrals lacking variance components and can be efficiently approximated with Laplace's method. We propose a default prior distribution on the parameter controlling the contribution of each random effect and conduct simulations to show that our method has good properties for model selection problems. Finally, we illustrate our methods on data from a clinical trial of patients with bipolar disorder and on data from an environmental study of water disinfection by-products and male reproductive outcomes.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H3BBQG7K\\Saville and Herring - 2009 - Testing Random Effects in the Linear Mixed Model U.pdf;C\:\\Users\\devan\\Zotero\\storage\\YAN2LMK8\\Saville and Herring - 2009 - Testing Random Effects in the Linear Mixed Model U.pdf},
  journal = {Biometrics},
  language = {en},
  number = {2}
}

@article{SC2003,
  title = {On the Distribution of Wildfire Sizes},
  author = {Schoenberg, Frederic Paik and Peng, Roger and Woods, James},
  year = {2003},
  volume = {14},
  pages = {583--592},
  doi = {10.1002/env.605},
  abstract = {Abstract A variety of models for the wildfire size distribution are examined using data on Los Angeles County wildfires greater than 100 acres between 1950 and 2000. In addition to graphs and likelihood criteria, Kolmogorov\textendash Smirnoff and Cramer\textendash von Mises statistics are used to compare the models. The tapered Pareto distribution appears to fit the data quite well and offers some advantages over the untapered Pareto distribution, while alternatives including the lognormal, half-normal, exponential and extremal distributions fit poorly. The size distribution appears to be quite stable over the examination period, though inspection of the transformed wildfire sizes for the tapered Pareto reveals some limited trend in the residuals, indicating a very slight gradual decrease in the average fire size in Los Angeles County over these 50 years. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.605},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ITZTRZJ5\\Schoenberg et al. - 2003 - On the distribution of wildfire sizes.pdf},
  journal = {Environmetrics},
  keywords = {Pareto,tapered Pareto,wildfires},
  number = {6}
}

@article{schlatherDetectingDependenceMarks2004,
  title = {Detecting Dependence between Marks and Locations of Marked Point Processes},
  author = {Schlather, Martin and Ribeiro, Paulo J. and Diggle, Peter J.},
  year = {2004},
  month = feb,
  volume = {66},
  pages = {79--93},
  issn = {1369-7412, 1467-9868},
  doi = {10.1046/j.1369-7412.2003.05343.x},
  abstract = {We introduce two characteristics for stationary and isotropic marked point processes, E.h/ and V .h/, and describe their use in investigating mark\textendash point interactions. These quantities are functions of the interpoint distance h and denote the conditional expectation and the conditional variance of a mark respectively, given that there is a further point of the process a distance h away. We present tests based on E and V for the hypothesis that the values of the marks can be modelled by a random field which is independent of the unmarked point process. We apply the methods to two data sets in forestry.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KEP5WYQQ\\Schlather et al. - 2004 - Detecting dependence between marks and locations o.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {1}
}

@article{schlatherDetectingDependenceMarks2004a,
  title = {Detecting {{Dependence}} between {{Marks}} and {{Locations}} of {{Marked Point Processes}}},
  author = {Schlather, Martin and Ribeiro, Paulo J. and Diggle, Peter J.},
  year = {2004},
  volume = {66},
  pages = {79--93},
  issn = {1369-7412},
  abstract = {We introduce two characteristics for stationary and isotropic marked point processes, E(h) and V(h), and describe their use in investigating mark-point interactions. These quantities are functions of the interpoint distance h and denote the conditional expectation and the conditional variance of a mark respectively, given that there is a further point of the process a distance h away. We present tests based on E and V for the hypothesis that the values of the marks can be modelled by a random field which is independent of the unmarked point process. We apply the methods to two data sets in forestry.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9GX8CJRW\\Schlather et al. - 2004 - Detecting Dependence between Marks and Locations o.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  number = {1}
}

@article{schlatherSecondOrderCharacteristicsMarked2001,
  title = {On the {{Second}}-{{Order Characteristics}} of {{Marked Point Processes}}},
  author = {Schlather, Martin},
  year = {2001},
  month = feb,
  volume = {7},
  pages = {99},
  issn = {13507265},
  doi = {10.2307/3318604},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TIUEFIZR\\Schlather - 2001 - On the Second-Order Characteristics of Marked Poin.pdf},
  journal = {Bernoulli},
  language = {en},
  number = {1}
}

@incollection{schmidtBayesianNonnegativeMatrix2009,
  title = {Bayesian {{Non}}-Negative {{Matrix Factorization}}},
  booktitle = {Independent {{Component Analysis}} and {{Signal Separation}}},
  author = {Schmidt, Mikkel N. and Winther, Ole and Hansen, Lars Kai},
  editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
  year = {2009},
  volume = {5441},
  pages = {540--547},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-00599-2_68},
  abstract = {We present a Bayesian treatment of non-negative matrix factorization (NMF), based on a normal likelihood and exponential priors, and derive an efficient Gibbs sampler to approximate the posterior density of the NMF factors. On a chemical brain imaging data set, we show that this improves interpretability by providing uncertainty estimates. We discuss how the Gibbs sampler can be used for model order selection by estimating the marginal likelihood, and compare with the Bayesian information criterion. For computing the maximum a posteriori estimate we present an iterated conditional modes algorithm that rivals existing state-of-the-art NMF algorithms on an image feature extraction problem.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\D765NDBI\\Schmidt et al. - 2009 - Bayesian Non-negative Matrix Factorization.pdf},
  isbn = {978-3-642-00598-5 978-3-642-00599-2},
  language = {en}
}

@article{schoenbergConsistentParametricEstimation2005,
  title = {Consistent Parametric Estimation of the Intensity of a Spatial\textendash Temporal Point Process},
  author = {Schoenberg, Frederic Paik},
  year = {2005},
  month = jan,
  volume = {128},
  pages = {79--93},
  issn = {03783758},
  doi = {10.1016/j.jspi.2003.09.027},
  abstract = {We consider conditions under which parametric estimates of the intensity of a spatial\textendash temporal point process are consistent. Although the actual point process being estimated may not be Poisson, an estimate involving maximizing a function that corresponds exactly to the log-likelihood if the process is Poisson is consistent under certain simple conditions. A second estimate based on weighted least squares is also shown to be consistent under quite similar assumptions. The conditions for consistency are simple and easily veri\"yed, and examples are provided to illustrate the extent to which consistent estimation may be achieved. An important special case is when the point processes being estimated are in fact Poisson, though other important examples are explored as well.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HA877WC8\\Schoenberg - 2005 - Consistent parametric estimation of the intensity .pdf},
  journal = {Journal of Statistical Planning and Inference},
  language = {en},
  number = {1}
}

@article{schoenbergConsistentParametricEstimation2005a,
  title = {Consistent Parametric Estimation of the Intensity of a Spatial\textendash Temporal Point Process},
  author = {Schoenberg, Frederic Paik},
  year = {2005},
  month = jan,
  volume = {128},
  pages = {79--93},
  issn = {03783758},
  doi = {10.1016/j.jspi.2003.09.027},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X9H52TF4\\Schoenberg - 2005 - Consistent parametric estimation of the intensity .pdf},
  journal = {Journal of Statistical Planning and Inference},
  language = {en},
  number = {1}
}

@article{schoenbergCriticalAssessmentBurning2007,
  title = {A Critical Assessment of the {{Burning Index}} in {{Los Angeles County}}, {{California}}},
  author = {Schoenberg, Frederic P. and Chang, Chien-Hsun and Keeley, Jon E. and Pompa, Jamie and Woods, James and Xu, Haiyong},
  year = {2007},
  volume = {16},
  pages = {473},
  issn = {1049-8001},
  doi = {10.1071/WF05089},
  abstract = {The Burning Index (BI) is commonly used as a predictor of wildfire activity. An examination of data on the BI and wildfires in Los Angeles County, California from January 1976 to December 2000 reveals that although the BI is positively associated with wildfire occurrence, its predictive value is quite limited. Wind speed alone has a higher correlation with burn area than BI, for instance, and a simple alternative point process model using wind speed, relative humidity, precipitation and temperature well outperforms the BI in terms of predictive power. The BI is generally far too high in Winter and too low in Fall, and may exaggerate the impact of individual variables such as wind speed or temperature during times when other variables, such as precipitation or relative humidity, render the environment ill-suited for wildfires.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7DB7DJIT\\Schoenberg et al. - 2007 - A critical assessment of the Burning Index in Los .pdf},
  journal = {International Journal of Wildland Fire},
  keywords = {Count,Covariates,Ignition Probability},
  language = {en},
  number = {4}
}

@article{schoenbergCriticalAssessmentBurning2007a,
  title = {A Critical Assessment of the {{Burning Index}} in {{Los Angeles County}}, {{California}}},
  author = {Schoenberg, F.P. and Chang, H.-C. and Keeley, J.E. and Pompa, J. and Woods, J. and Xu, H.},
  year = {2007},
  volume = {16},
  pages = {11},
  doi = {10.1071/WF05089},
  abstract = {The Burning Index (BI) is commonly used as a predictor of wildfire activity. An examination of data on the BI and wildfires in Los Angeles County, California, from January 1976 to December 2000 reveals that although the BI is positively associated with wildfire occurrence, its predictive value is quite limited. Wind speed alone has a higher correlation with burn area than BI, for instance, and a simple alternative point process model using wind speed, relative humidity, precipitation and temperature well outperforms the BI in terms of predictive power. The BI is generally far too high in winter and too low in fall, and may exaggerate the impact of individual variables such as wind speed or temperature during times when other variables, such as precipitation or relative humidity, render the environment ill suited for wildfires. ?? IAWF 2007.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TJAI9HMQ\\Schoenberg et al. - 2007 - A critical assessment of the Burning Index in Los .pdf},
  journal = {International Journal of Wildland Fire},
  number = {4}
}

@article{schoenbergDescriptionEarthquakeAftershock2008,
  title = {Description of Earthquake Aftershock Sequences Using Prototype Point Patterns},
  author = {Schoenberg, Frederic Paik and Tranbarger, Katherine E.},
  year = {2008},
  month = may,
  volume = {19},
  pages = {271--286},
  issn = {11804009, 1099095X},
  doi = {10.1002/env.867},
  abstract = {We introduce the use of prototype point patterns to characterize the behavior of a typical aftershock sequence from the global Harvard earthquake catalog. These prototypes are used not only for data description and summary but also to identify outliers and to classify sequences into groups exhibiting similar aftershock behavior. We find that a typical shallow earthquake of magnitude between 7.5 and 8.0 has approximately five aftershocks of magnitude at least 5.5, and that within an observation window of 0.113 days to 2.0 years after the mainshock, these aftershocks are roughly evenly distributed in log-time. The relative magnitudes and distances from the mainshock for the typical aftershock sequence are characterized as well. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\QY6TCDD9\\Schoenberg and Tranbarger - 2008 - Description of earthquake aftershock sequences usi.pdf},
  journal = {Environmetrics},
  language = {en},
  number = {3}
}

@article{schoenbergDistributionWildfireSizes2003,
  title = {On the Distribution of Wildfire Sizes},
  author = {Schoenberg, Frederic Paik and Peng, Roger and Woods, James},
  year = {2003},
  month = sep,
  volume = {14},
  pages = {583--592},
  issn = {1180-4009, 1099-095X},
  doi = {10.1002/env.605},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HJMG37M8\\Schoenberg et al. - 2003 - On the distribution of wildfire sizes.pdf;C\:\\Users\\devan\\Zotero\\storage\\UYA5CWHT\\Schoenberg et al. - 2003 - On the distribution of wildfire sizes.pdf},
  journal = {Environmetrics},
  keywords = {Size},
  language = {en},
  number = {6}
}

@article{schoenbergNonsimpleMarkedPoint2006,
  title = {On {{Non}}-Simple {{Marked Point Processes}}},
  author = {Schoenberg, Frederic Paik},
  year = {2006},
  month = jun,
  volume = {58},
  pages = {223--233},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-005-0003-y},
  abstract = {Simple point processes are often characterized by their associated compensators or conditional intensities. Non-simple point processes are not uniquely determined by their conditional intensity and compensator, so instead one may identify with the point process its associated simple point process and corresponding conditional intensity, on an expanded mark space. Some relations between the conditional intensity on the expanded mark space and the ordinary conditional intensity are investigated here, and some classes of separable non-simple processes are presented. Transformations into simple point processes, involving thinning and rescaling, are presented.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MDU5U6KH\\Schoenberg - 2006 - On Non-simple Marked Point Processes.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {2}
}

@article{schoenbergNoteConsistentEstimation2016,
  title = {A Note on the Consistent Estimation of Spatial-Temporal Point Process Parameters},
  author = {Schoenberg, Frederic Paik},
  year = {2016},
  issn = {10170405},
  doi = {10.5705/ss.2014.150},
  file = {C\:\\Users\\devan\\Zotero\\storage\\W4X7L7BN\\Schoenberg - 2016 - A note on the consistent estimation of spatial-tem.pdf},
  journal = {Statistica Sinica},
  language = {en}
}

@article{schoenbergNoteNonparametricSemiparametric2009,
  title = {A Note on Non-Parametric and Semi-Parametric Modeling of Wildfire Hazard in {{Los Angeles County}}, {{California}}},
  author = {Schoenberg, Frederic Paik and Pompa, Jamie and Chang, Chien-Hsun},
  year = {2009},
  month = jun,
  volume = {16},
  pages = {251--269},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-007-0087-z},
  abstract = {This paper explores the use of, and problems that arise in, kernel smoothing and parametric estimation of the relationships between wildfire incidence and various meteorological variables. Such relationships may be treated as components in separable point process models for wildfire activity. The resulting models can be used for comparative purposes in order to assess the predictive performance of the Burning Index.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VKZNKGIE\\Schoenberg et al. - 2009 - A note on non-parametric and semi-parametric model.pdf},
  journal = {Environmental and Ecological Statistics},
  keywords = {Count,Ignition Probability,Nonparametric},
  language = {en},
  number = {2}
}

@article{schoenbergNoteSeparabilityMultidimensional2006,
  title = {A {{Note}} on the {{Separability}} of {{Multidimensional Point Processes}} with {{Covariates}}},
  author = {Schoenberg, Frederic P},
  year = {2006},
  pages = {20},
  abstract = {For models used to describe multi-dimensional marked point processes with covariates, the high number of parameters typically involved and the high dimensionality of the process can make model evaluation, construction, and estimation using maximum likelihood quite difficult. Conditions are explored here under which parameters governing one set of coordinates or covariates affecting a multi-dimensional marked point process may be estimated separately. The resulting estimates are, under the given conditions, similar to maximum likelihood estimates.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UL9JEWIY\\Schoenberg - A Note on the Separability of Multidimensional Poi.pdf},
  journal = {University of California - Los Angeles},
  language = {en}
}

@incollection{schoenbergPointProcessesSpatialTemporalBased2013,
  title = {Point {{Processes}}, {{Spatial}}-{{TemporalBased}} in Part on the Article ``{{Point}} Processes, Spatial-Temporal'' by {{Frederic Paik Schoenberg}}, {{David R}}. {{Brillinger}}, and {{Peter Guttorp}}, Which Appeared in the {{{\emph{Encyclopedia}}}}{\emph{ of }}{{{\emph{Environmetrics}}}} .},
  booktitle = {Encyclopedia of {{Environmetrics}}},
  author = {Schoenberg, Frederic Paik and Brillinger, David R. and Guttorp, Peter},
  editor = {{El-Shaarawi}, Abdel H. and Piegorsch, Walter W.},
  year = {2013},
  month = jan,
  pages = {vap020.pub2},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9780470057339.vap020.pub2},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PLZGTALM\\Schoenberg et al. - 2013 - Point Processes, Spatial-TemporalBased in part on .pdf},
  isbn = {978-0-471-89997-6 978-0-470-05733-9},
  language = {en}
}

@article{schoenbergTestingSeparabilitySpatialTemporal2004,
  title = {Testing {{Separability}} in {{Spatial}}-{{Temporal Marked Point Processes}}},
  author = {Schoenberg, Frederic Paik},
  year = {2004},
  volume = {60},
  pages = {471--481},
  issn = {0006-341X},
  abstract = {Nonparametric tests for investigating the separability of a spatial-temporal marked point process are described and compared. It is shown that a Cramer-von Mises-type test is very powerful at detecting gradual departures from separability, and that a residual test based on randomly rescaling the process is powerful at detecting nonseparable clustering or inhibition of the marks. An application to Los Angeles County wildfire data is given, in which it is shown that the separability hypothesis is invalidated largely due to clustering of fires of similar sizes within periods of up to 3.9 years.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\T2JSNST9\\Schoenberg - 2004 - Testing Separability in Spatial-Temporal Marked Po.pdf},
  journal = {Biometrics},
  number = {2}
}

@article{schuckersAccountingRinkEffects2014,
  title = {Accounting for {{Rink Effects}} in the {{National Hockey League}}'s {{Real Time Scoring System}}},
  author = {Schuckers, Michael and Macdonald, Brian},
  year = {2014},
  month = dec,
  abstract = {Recording of events in National Hockey League rinks is done through the Real Time Scoring System. This system records events such as hits, shots, faceoffs, etc., as part of the play-by-play files that are made publicly available. Several previous studies have found that there are inconsistencies in the recording of these events from rink to rink. In this paper, we propose a methodology for estimation of the rink effects for each of the rinks in the National Hockey League. Our aim is to build a model which accounts for the relative differences between rinks. We use log-linear regression to model counts of events per game with several predictors including team factors and average score differential. The estimated rink effects can be used to reweight recorded events so that can have comparable counts of events across rinks. Applying our methodology to data from six regular seasons, we find that there are some rinks with rink effects that are significant and consistent across these seasons for multiple events.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\B8XL7K28\\Schuckers_Macdonald_RinkEffects_Final.pdf;C\:\\Users\\devan\\Zotero\\storage\\JK39XC3V\\Schuckers and Macdonald - 2014 - Accounting for Rink Effects in the National Hockey.pdf}
}

@article{schuckersDIGRDefenseIndependent2011,
  title = {{{DIGR}}: {{A Defense Independent Rating}} of {{NHL Goaltenders}} Using {{Spatially Smoothed Save Percentage Maps}}},
  author = {Schuckers, Michael E},
  year = {2011},
  pages = {8},
  abstract = {Evaluation of NHL goalies is often done by comparing their save percentage. These save percentages depend highly upon the defense in front of each goalie and the difficulty of shots that each goalie faces. In this paper we introduce a new methodology for evaluating NHL goalies that does not depend upon the distribution of shots that any individual goalie faced. To achieve this new metric we create smoothed nonlinear spatial maps of goalie performance based upon the shots they did face and then evaluate these goalies on the league average distribution of shots. These maps show the probability of a goalie giving up a goal from across the playing surface. We derive a general mathematical framework for the evaluation of a goalie's save percentage. Using data from the 2009-10 NHL regular season, we apply this new methodology and calculate our new defense independent goalie rating (DIGR) for each goalie that face more than 600 shots. Results of this evaluation are given and possible extensions of the methodology are discussed.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6VWV9EDZ\\Schuckers - 2011 - DIGR A Defense Independent Rating of NHL Goaltend.pdf},
  language = {en}
}

@inproceedings{schuckersTotalHockeyRating2013,
  title = {Total {{Hockey Rating}} ( {{THoR}} ) : {{A}} Comprehensive Statistical Rating of {{National Hockey League}} Forwards and Defensemen Based upon All on-Ice Events},
  shorttitle = {Total {{Hockey Rating}} ( {{THoR}} )},
  author = {Schuckers, Michael E. and Curro, James},
  year = {2013},
  abstract = {Hockey is a fluid sport with players frequently coming on and off the ice without the stoppage of play. It is also a relatively low scoring sport compared to other sports such as basketball. Both of these features make evaluation of players difficult. Recently, there have been some attempts to get at the value of National Hockey League (NHL) players including Macdonald [1], Ferrari [2], and Awad [3]. Here we present a new comprehensive rating that accounts for other players on the ice will a give player as well as the impact of where a shift starts, often called zone starts [4], and of every non-shooting events such as turnovers and hits that occur when a player is on the ice. The impact of each play is determined by the probability that it leads to a goal for a player\quotedblbase s team (or their opponent) in the subsequent 20 seconds. The primary outcome of this work is a reliable methodology that can quantify the impact of players in creating and preventing goals for both forwards and defenseman. We present results based on all events from the 2010-11 and 2011-12 NHL regular seasons.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XXXEXZLN\\Schuckers and Curro - 2013 - Total Hockey Rating ( THoR )  A comprehensive sta.pdf},
  keywords = {Personnel Turnover,Sports}
}

@article{scottAnalyticalFrameworkQuantifying2006,
  title = {An Analytical Framework for Quantifying Wildland Fire Risk and Fuel Treatment Benefit},
  author = {Scott, Joe H.},
  year = {2006},
  volume = {041},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3CY7DD7B\\Scott - 2006 - An analytical framework for quantifying wildland f.pdf;C\:\\Users\\devan\\Zotero\\storage\\L9NATQ78\\25944.html},
  journal = {In: Andrews, Patricia L.; Butler, Bret W., comps. 2006. Fuels Management-How to Measure Success: Conference Proceedings. 28-30 March 2006; Portland, OR. Proceedings RMRS-P-41. Fort Collins, CO: U.S. Department of Agriculture, Forest Service, Rocky Mountain Research Station. p. 169-184},
  language = {en}
}

@article{seneSharedRandomeffectModels,
  title = {Shared Random-Effect Models for the Joint Analysis of Longitudinal and Time-to-Event Data: Application to the Prediction of Prostate Cancer Recurrence},
  author = {Sene, Mbery and Bellera, Carine A and {Proust-Lima}, Cecile},
  pages = {41},
  abstract = {In the last decade, joint modeling research has expanded very rapidly in biostatistics and medical research. This type of models enables the simultaneous study of a longitudinal marker and a correlated time-to-event. Among them, the shared randomeffect models that define a mixed model for the longitudinal marker and a survival model for the time-to-event including characteristics of the mixed model as covariates received the main interest. Indeed, they extend naturally the survival model with time-dependent covariates and offer a flexible framework to explore the link between a longitudinal biomarker and a risk of event.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\U6NAHMHT\\Sene et al. - Shared random-eﬀect models for the joint analysis .pdf},
  language = {en}
}

@article{serrasaurinaSpatiotemporalLogGaussianCox2014,
  title = {Spatio-Temporal Log-{{Gaussian Cox}} Processes for Modelling Wildfire Occurrence: The Case of {{Catalonia}}, 1994-2008},
  shorttitle = {Spatio-Temporal Log-{{Gaussian Cox}} Processes for Modelling Wildfire Occurrence},
  author = {Serra Saurina, Laura and S{\'a}ez Zafra, Marc and Mateu, Jorge and Varga, Diego and Juan Verdoy, Pablo and {D{\'i}az-{\'A}valos}, Carlos and Rue, H{\aa}vard},
  year = {2014},
  month = sep,
  issn = {1352-8505 (versi\'o paper)},
  doi = {http://dx.doi.org/10.1007/s10651-013-0267-y},
  abstract = {Wildfires have become one of the principal environmental problems in the Mediterranean basin. While fire plays an important role in most terrestrial plant ecosystems, the potential hazard that it represents for human lives and property has led to the application of fire exclusion policies that, in the long term, have caused severe damage, mainly due to the increase of fuel loadings in forested areas, in some forest systems. The lack of an easy solution to forest fire management highlights the importance of preventive tasks. The observed spatio-temporal pattern of wildfire occurrences may be idealized as a realization of some stochastic process. In particular, we may use a space\textendash time point pattern approach for the analysis and inference process. We studied wildfires in Catalonia, a region in the north-east of the Iberian Peninsula, and we analyzed the spatio-temporal patterns produced by those wildfire incidences by considering the influence of covariates on trends in the intensity of wildfire locations. A total of 3,166 wildfires from 1994\textendash 2008 have been recorded. We specified spatio-temporal log-Gaussian Cox process models. Models were estimated using Bayesian inference for Gaussian Markov Random Field through the integrated nested Laplace approximation algorithm. The results of our analysis have provided statistical evidence that areas closer to humans have more human induced wildfires, areas farther have more naturally occurring wildfires. We believe the methods presented in this paper may contribute to the prevention and management of those wildfires which are not random in space or time},
  copyright = {Tots els drets reservats},
  file = {C\:\\Users\\devan\\Zotero\\storage\\74U62T7R\\Serra Saurina et al. - 2014 - Spatio-temporal log-Gaussian Cox processes for mod.pdf;C\:\\Users\\devan\\Zotero\\storage\\2ZQ4DKII\\9343.html},
  language = {eng}
}

@article{serraSpatiotemporalLogGaussianCox2014,
  title = {Spatio-Temporal Log-{{Gaussian Cox}} Processes for Modelling Wildfire Occurrence: The Case of {{Catalonia}}, 1994\textendash 2008},
  shorttitle = {Spatio-Temporal Log-{{Gaussian Cox}} Processes for Modelling Wildfire Occurrence},
  author = {Serra, Laura and Saez, Marc and Mateu, Jorge and Varga, Diego and Juan, Pablo and {D{\'i}az-{\'A}valos}, Carlos and Rue, H{\aa}vard},
  year = {2014},
  month = sep,
  volume = {21},
  pages = {531--563},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-013-0267-y},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8SVVHLLE\\Serra et al. - 2014 - Spatio-temporal log-Gaussian Cox processes for mod.pdf;C\:\\Users\\devan\\Zotero\\storage\\DKJSSAI4\\Serra et al. - 2014 - Spatio-temporal log-Gaussian Cox processes for mod.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {3}
}

@article{serraSpatiotemporalPoissonHurdle2014,
  title = {A Spatio-Temporal {{Poisson}} Hurdle Point Process to Model Wildfires},
  author = {Serra, Laura and Saez, Marc and Juan, Pablo and Varga, Diego and Mateu, Jorge},
  year = {2014},
  month = oct,
  volume = {28},
  doi = {10.1007/s00477-013-0823-x},
  abstract = {Wildfires have been studied in many ways, for instance as a spatial point pattern or through modeling the size of fires or the relative risk of big fires. Lately a large variety of complex statistical models can be fitted routinely to complex data sets, in particular wildfires, as a result of widely accessible high-level statistical software, such as R. The objective in this paper is to model the occurrence of big wildfires (greater than a given extension of hectares) using an adapted two-part econometric model, specifically a hurdle model. The methodology used in this paper is useful to determine those factors that help any fire to become a big wildfire. Our proposal and methodology can be routinely used to contribute to the management of big wildfires.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NYQJ87IY\\Serra et al. - 2014 - A spatio-temporal Poisson hurdle point process to .pdf},
  journal = {Stochastic Environmental Research and Risk Assessment}
}

@article{Shang2016,
  title = {Rainbow: {{Rainbow Plots}}, {{Bagplots}} and {{Boxplots}} for {{Functional Data}}},
  author = {Shang, Han Lin and Hyndman, Rob J.},
  year = {2016},
  journal = {R Package Version 3.6}
}

@article{shangRainbowPackageVisualizing,
  title = {Rainbow: {{An R}} Package for {{Visualizing Functional Time Series}}},
  author = {Shang, Han Lin},
  year = {2019},
  journal = {R Package Version 3.6}
}

@article{sheatherDensityEstimation2004,
  title = {Density {{Estimation}}},
  author = {Sheather, Simon J.},
  year = {2004},
  month = nov,
  volume = {19},
  pages = {588--597},
  issn = {0883-4237},
  doi = {10.1214/088342304000000297},
  abstract = {This paper provides a practical description of density estimation based on kernel methods. An important aim is to encourage practicing statisticians to apply these methods to data. As such, reference is made to implementations of these methods in R, S-PLUS and SAS.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MVJDLL2D\\Sheather - 2004 - Density Estimation.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {4}
}

@article{sheppardCalculationMostProbable1897,
  title = {On the {{Calculation}} of the Most {{Probable Values}} of {{Frequency}}-{{Constants}}, for {{Data}} Arranged According to {{Equidistant Division}} of a {{Scale}}},
  author = {Sheppard, W. F.},
  year = {1897},
  month = nov,
  volume = {s1-29},
  pages = {353--380},
  issn = {0024-6115},
  doi = {10.1112/plms/s1-29.1.353},
  abstract = {W. F. Sheppard, M.A., LL.M.;  On the Calculation of the most Probable Values of Frequency-Constants, for Data arranged according to Equidistant Division of a Sc},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5967DR5F\\Sheppard - 1897 - On the Calculation of the most Probable Values of .pdf;C\:\\Users\\devan\\Zotero\\storage\\J8NPGBV5\\1541840.html},
  journal = {Proceedings of the London Mathematical Society},
  language = {en},
  number = {1}
}

@article{shewhart1925application,
  title = {The Application of Statistics as an Aid in Maintaining Quality of a Manufactured Product},
  author = {Shewhart, Walter A},
  year = {1925},
  volume = {20},
  pages = {546--548},
  publisher = {{Taylor {{\&}} Francis Group}},
  journal = {Journal of the American Statistical Association},
  number = {152}
}

@article{shiauMonitoringNonlinearProfiles2009,
  title = {Monitoring {{Nonlinear Profiles}} with {{Random Effects}} by {{Nonparametric Regression}}},
  author = {Shiau, Jyh-Jen Horng and Huang, Hsiang-Ling and Lin, Shuo-Hui and Tsai, Ming-Ye},
  year = {2009},
  month = may,
  volume = {38},
  pages = {1664--1679},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610920802702535},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3L79WCP7\\Shiau et al. - 2009 - Monitoring Nonlinear Profiles with Random Effects .pdf;C\:\\Users\\devan\\Zotero\\storage\\XZZSIV2B\\Shiau et al. - 2009 - Monitoring Nonlinear Profiles with Random Effects .pdf},
  journal = {Communications in Statistics - Theory and Methods},
  language = {en},
  number = {10}
}

@article{shirotaInferenceLogGaussian2016,
  title = {Inference for Log {{Gaussian Cox}} Processes Using an Approximate Marginal Posterior},
  author = {Shirota, Shinichiro and Gelfand, Alan E.},
  year = {2016},
  month = nov,
  abstract = {The log Gaussian Cox process is a flexible class of point pattern models for capturing spatial and spatio-temporal dependence for point patterns. Model fitting requires approximation of stochastic integrals which is implemented through discretization of the domain of interest. With fine scale discretization, inference based on Markov chain Monte Carlo is computationally heavy because of the cost of repeated iteration or inversion or Cholesky decomposition (cubic order) of high dimensional covariance matrices associated with latent Gaussian variables. Furthermore, hyperparameters for latent Gaussian variables have strong dependence with sampled latent Gaussian variables. Altogether, standard Markov chain Monte Carlo strategies are inefficient and not well behaved.},
  archivePrefix = {arXiv},
  eprint = {1611.10359},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\HS5NSUDD\\Shirota and Gelfand - 2016 - Inference for log Gaussian Cox processes using an .pdf;C\:\\Users\\devan\\Zotero\\storage\\MLIALREM\\Shirota and Gelfand - 2016 - Inference for log Gaussian Cox processes using an .pdf},
  journal = {arXiv:1611.10359 [stat]},
  keywords = {Statistics - Computation,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@misc{shomerFooledGrittinessEstimating2016,
  title = {Fooled {{By Grittiness}}: {{Estimating Shooting Talent}}},
  shorttitle = {Fooled {{By Grittiness}}},
  author = {Shomer, Harry},
  year = {2016},
  month = apr,
  file = {C\:\\Users\\devan\\Zotero\\storage\\YTCT5A97\\estimating-shooting-talent_13.html},
  journal = {Fooled By Grittiness}
}

@misc{ShotLocationData2014,
  title = {Shot {{Location Data}} and {{Strategy II}}: {{Evaluating Individual Defensive Play}}},
  shorttitle = {Shot {{Location Data}} and {{Strategy II}}},
  year = {2014},
  month = nov,
  abstract = {This is the second post in a (likely) 3 part series going through the data/methods/results which I presented at the Pittsburgh Hockey Analytics Workshop. Part I, which covers whether defencemen pla\ldots},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UF2CJJYI\\shot-location-data-and-strategy-ii-evaluating-individual-defensive-play.html},
  journal = {puck++},
  language = {en}
}

@misc{sidwarUndocumentedNHLStats,
  title = {The {{Undocumented NHL Stats API}}},
  author = {Sidwar, Kevin},
  abstract = {Update 3/2/2018  - I really appreciate everyone that has contributed with other links down in the comments. As an additional resource if you are looking for certain endpoints but don't see them here I have a YouTube video where people are also leaving great comments. One day, when I have time I hope},
  file = {C\:\\Users\\devan\\Zotero\\storage\\T6SCA564\\the-undocumented-nhl-stats-api.html},
  howpublished = {https://www.kevinsidwar.com/iot/2017/7/1/the-undocumented-nhl-stats-api},
  journal = {KevinSidwar.com},
  language = {en-US}
}

@article{simpsonGoingGridComputationally2011,
  title = {Going off Grid: {{Computationally}} Efficient Inference for Log-{{Gaussian Cox}} Processes},
  shorttitle = {Going off Grid},
  author = {Simpson, Daniel and Illian, Janine and Lindgren, Finn and S{\o}rbye, Sigrunn and Rue, H{\aa}vard},
  year = {2011},
  month = nov,
  abstract = {This paper introduces a new method for performing computational inference on log-Gaussian Cox processes. The likelihood is approximated directly by making novel use of a continuously specified Gaussian random field. We show that for sufficiently smooth Gaussian random field prior distributions, the approximation can converge with arbitrarily high order, while an approximation based on a counting process on a partition of the domain only achieves first-order convergence. The given results improve on the general theory of convergence of the stochastic partial differential equation models, introduced by Lindgren et al. (2011). The new method is demonstrated on a standard point pattern data set and two interesting extensions to the classical log-Gaussian Cox process framework are discussed. The first extension considers variable sampling effort throughout the observation window and implements the method of Chakraborty et al. (2011). The second extension constructs a log-Gaussian Cox process on the world's oceans. The analysis is performed using integrated nested Laplace approximation for fast approximate inference.},
  archivePrefix = {arXiv},
  eprint = {1111.0641},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\N9MYMEC2\\Simpson et al. - 2011 - Going off grid Computationally efficient inferenc.pdf},
  journal = {arXiv:1111.0641 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Computation},
  language = {en},
  primaryClass = {math, stat}
}

@article{simpsonGoingGridComputationally2016,
  title = {Going off Grid: Computationally Efficient Inference for Log-{{Gaussian Cox}} Processes},
  shorttitle = {Going off Grid},
  author = {Simpson, D. and Illian, J. B. and Lindgren, F. and S{\o}rbye, S. H. and Rue, H.},
  year = {2016},
  month = mar,
  volume = {103},
  pages = {49--70},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asv064},
  abstract = {This paper introduces a new method for performing computational inference on log-Gaussian Cox processes. The likelihood is approximated directly by making use of a continuously specified Gaussian random field. We show that for sufficiently smooth Gaussian random field prior distributions, the approximation can converge with arbitrarily high order, whereas an approximation based on a counting process on a partition of the domain achieves only first-order convergence. The results improve upon the general theory of convergence for stochastic partial differential equation models introduced by Lindgren et al. (2011). The new method is demonstrated on a standard point pattern dataset, and two interesting extensions to the classical log-Gaussian Cox process framework are discussed. The first extension considers variable sampling effort throughout the observation window and implements the method of Chakraborty et al. (2011). The second extension constructs a log-Gaussian Cox process on the world's oceans. The analysis is performed using integrated nested Laplace approximation for fast approximate inference.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F3EZ78EL\\Simpson et al. - 2016 - Going off grid computationally efficient inferenc.pdf;C\:\\Users\\devan\\Zotero\\storage\\LB6LSXL8\\Simpson et al. - 2016 - Going off grid computationally efficient inferenc.pdf;C\:\\Users\\devan\\Zotero\\storage\\RW858AMV\\Simpson et al. - 2016 - Going off grid computationally efficient inferenc.pdf},
  journal = {Biometrika},
  language = {en},
  number = {1}
}

@article{simpsonOrderMakeSpatial2012,
  title = {In Order to Make Spatial Statistics Computationally Feasible, We Need to Forget about the Covariance Function: {{SPDES}}, {{GMRFS}}, {{AND KERNEL METHODS}}},
  shorttitle = {In Order to Make Spatial Statistics Computationally Feasible, We Need to Forget about the Covariance Function},
  author = {Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2012},
  month = feb,
  volume = {23},
  pages = {65--74},
  issn = {11804009},
  doi = {10.1002/env.1137},
  abstract = {Gaussian random fields (GRFs) are the most common way of modelling structured spatial random effects in spatial statistics. Unfortunately, their high computational cost renders the direct use of GRFs impractical for large problems and approximations are commonly used. In this paper we compare two approximations to GRFs with Mat\textasciiacute ern covariance functions: the kernel convolution approximation and the Gaussian Markov random field representation of an associated stochastic partial differential equation. We show that the second approach is a natural way to tackle the problem and is better than methods based on approximating the kernel convolution.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LML3U433\\Simpson et al. - 2012 - In order to make spatial statistics computationall.pdf;C\:\\Users\\devan\\Zotero\\storage\\TUV5XQYW\\Simpson et al. - 2012 - In order to make spatial statistics computationall.pdf},
  journal = {Environmetrics},
  language = {en},
  number = {1}
}

@article{simpsonPenalisingModelComponent2017,
  title = {Penalising {{Model Component Complexity}}: {{A Principled}}, {{Practical Approach}} to {{Constructing Priors}}},
  shorttitle = {Penalising {{Model Component Complexity}}},
  author = {Simpson, Daniel and Rue, H{\aa}vard and Riebler, Andrea and Martins, Thiago G. and S{\o}rbye, Sigrunn H.},
  year = {2017},
  month = feb,
  volume = {32},
  pages = {1--28},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/16-STS576},
  abstract = {In this paper, we introduce a new concept for constructing prior distributions. We exploit the natural nested structure inherent to many model components, which defines the model component to be a flexible extension of a base model. Proper priors are defined to penalise the complexity induced by deviating from the simpler base model and are formulated after the input of a user-defined scaling parameter for that model component, both in the univariate and the multivariate case. These priors are invariant to reparameterisations, have a natural connection to Jeffreys' priors, are designed to support Occam's razor and seem to have excellent robustness properties, all which are highly desirable and allow us to use this approach to define default prior distributions. Through examples and theoretical results, we demonstrate the appropriateness of this approach and how it can be applied in various situations.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZXI9FKLM\\Simpson et al. - 2017 - Penalising Model Component Complexity A Principle.pdf;C\:\\Users\\devan\\Zotero\\storage\\J3VBDN8L\\1491465621.html},
  journal = {Statistical Science},
  keywords = {Bayesian theory,disease mapping,hierarchical models,information geometry,interpretable prior distributions,prior on correlation matrices},
  language = {EN},
  mrnumber = {MR3634300},
  number = {1},
  zmnumber = {06946257}
}

@article{simpsonThinkContinuousMarkovian2012,
  title = {Think Continuous: {{Markovian Gaussian}} Models in Spatial Statistics},
  shorttitle = {Think Continuous},
  author = {Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2012},
  month = may,
  volume = {1},
  pages = {16--29},
  issn = {22116753},
  doi = {10.1016/j.spasta.2012.02.003},
  abstract = {Gaussian Markov random fields (GMRFs) are frequently used as computationally efficient models in spatial statistics. Unfortunately, it has traditionally been difficult to link GMRFs with the more traditional Gaussian random field models, as the Markov property is difficult to deploy in continuous space. Following the pioneering work of Lindgren et al. (2011), we expound on the link between Markovian Gaussian random fields and GMRFs. In particular, we discuss the theoretical and practical aspects of fast computation with continuously specified Markovian Gaussian random fields, as well as the clear advantages they offer in terms of clear, parsimonious, and interpretable models of anisotropy and nonstationarity.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AW2RI8VC\\Simpson et al. - 2012 - Think continuous Markovian Gaussian models in spa.pdf;C\:\\Users\\devan\\Zotero\\storage\\IXVQM22Y\\Simpson et al. - 2012 - Think continuous Markovian Gaussian models in spa.pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@phdthesis{sinharayBayesFactorsVariance2001,
  title = {Bayes Factors for Variance Component Testing in Generalized Linear Mixed Models},
  author = {Sinharay, Sandip},
  year = {2001},
  address = {{Ames}},
  doi = {10.31274/rtd-180813-14241},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DYT3XXMA\\Sinharay - 2001 - Bayes factors for variance component testing in ge.pdf;C\:\\Users\\devan\\Zotero\\storage\\UK7JXRY7\\Sinharay - 2001 - Bayes factors for variance component testing in ge.pdf;C\:\\Users\\devan\\Zotero\\storage\\VL75P3MT\\Sinharay - 2001 - Bayes factors for variance component testing in ge.pdf},
  language = {en},
  school = {Iowa State University, Digital Repository},
  type = {Doctor of {{Philosophy}}}
}

@article{skinnerMethodUsingPlayer2015,
  title = {A {{Method}} for {{Using Player Tracking Data}} in {{Basketball}} to {{Learn Player Skills}} and {{Predict Team Performance}}},
  author = {Skinner, Brian and Guy, Stephen J.},
  editor = {{Emmert-Streib}, Frank},
  year = {2015},
  month = sep,
  volume = {10},
  pages = {e0136393},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136393},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CZJ3IJDL\\Skinner and Guy - 2015 - A Method for Using Player Tracking Data in Basketb.PDF;C\:\\Users\\devan\\Zotero\\storage\\S7NAXU5F\\Skinner and Guy - 2015 - A Method for Using Player Tracking Data in Basketb.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {9}
}

@article{sorbyeCarefulPriorSpecification2019,
  title = {Careful Prior Specification Avoids Incautious Inference for Log-{{Gaussian Cox}} Point Processes},
  author = {S{\o}rbye, Sigrunn H and Illian, Janine B. and Simpson, Daniel P. and Burslem, David and Rue, H{\aa}vard},
  year = {2019},
  volume = {68},
  pages = {543--564},
  issn = {1467-9876},
  doi = {10.1111/rssc.12321},
  abstract = {Hyperprior specifications for random fields in spatial point process modelling can have a major influence on the results. In fitting log-Gaussian Cox processes to rainforest tree species, we consider a reparameterized model combining a spatially structured and an unstructured random field into a single component. This component has one hyperparameter accounting for marginal variance, whereas an additional hyperparameter governs the fraction of the variance that is explained by the spatially structured effect. This facilitates interpretation of the hyperparameters, and significance of covariates is studied for a range of hyperprior specifications. Appropriate scaling makes the analysis invariant to grid resolution.},
  copyright = {\textcopyright{} 2018 Royal Statistical Society},
  file = {C\:\\Users\\devan\\Zotero\\storage\\B97QIV3Z\\S⊘rbye et al. - 2019 - Careful prior specification avoids incautious infe.pdf;C\:\\Users\\devan\\Zotero\\storage\\WRHIDY65\\rssc.html},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  keywords = {Bayesian analysis,Penalized complexity prior,R-INLA,Spatial modelling,Spatial point process},
  language = {en},
  number = {3}
}

@article{sorbyeTutorialScalingIGMRFmodels,
  title = {Tutorial: {{Scaling IGMRF}}-Models in {{R}}-{{INLA}}},
  author = {S{\o}rbye, Sigrunn Holbek},
  pages = {7},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2ZVVUCQR\\Sørbye - Tutorial Scaling IGMRF-models in R-INLA.pdf;C\:\\Users\\devan\\Zotero\\storage\\92M2DWWY\\Sørbye - Tutorial Scaling IGMRF-models in R-INLA.pdf;C\:\\Users\\devan\\Zotero\\storage\\EFCMHIV5\\Sørbye - Tutorial Scaling IGMRF-models in R-INLA.pdf},
  language = {en}
}

@article{soriano-redondoUnderstandingSpeciesDistribution2019,
  title = {Understanding Species Distribution in Dynamic Populations: A New Approach Using Spatio-Temporal Point Process Models},
  shorttitle = {Understanding Species Distribution in Dynamic Populations},
  author = {Soriano-Redondo, Andrea and Jones-Todd, Charlotte M. and Bearhop, Stuart and Hilton, Geoff M. and Lock, Leigh and Stanbury, Andrew and Votier, Stephen C. and Illian, Janine B.},
  year = {2019},
  volume = {42},
  pages = {1092--1102},
  issn = {1600-0587},
  doi = {10.1111/ecog.03771},
  abstract = {Understanding and predicting a species' distribution across a landscape is of central importance in ecology, biogeography and conservation biology. However, it presents daunting challenges when populations are highly dynamic (i.e. increasing or decreasing their ranges), particularly for small populations where information about ecology and life history traits is lacking. Currently, many modelling approaches fail to distinguish whether a site is unoccupied because the available habitat is unsuitable or because a species expanding its range has not arrived at the site yet. As a result, habitat that is indeed suitable may appear unsuitable. To overcome some of these limitations, we use a statistical modelling approach based on spatio-temporal log-Gaussian Cox processes. These model the spatial distribution of the species across available habitat and how this distribution changes over time, relative to covariates. In addition, the model explicitly accounts for spatio-temporal dynamics that are unaccounted for by covariates through a spatio-temporal stochastic process. We illustrate the approach by predicting the distribution of a recently established population of Eurasian cranes Grus grus in England, UK, and estimate the effect of a reintroduction in the range expansion of the population. Our models show that wetland extent and perimeter-to-area ratio have a positive and negative effect, respectively, in crane colonisation probability. Moreover, we find that cranes are more likely to colonise areas near already occupied wetlands and that the colonisation process is progressing at a low rate. Finally, the reintroduction of cranes in SW England can be considered a human-assisted long-distance dispersal event that has increased the dispersal potential of the species along a longitudinal axis in S England. Spatio-temporal log-Gaussian Cox process models offer an excellent opportunity for the study of species where information on life history traits is lacking, since these are represented through the spatio-temporal dynamics reflected in the model.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecog.03771},
  copyright = {\textcopyright{} 2019 The Authors},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EHLUW8CV\\Soriano‐Redondo et al. - 2019 - Understanding species distribution in dynamic popu.pdf;C\:\\Users\\devan\\Zotero\\storage\\97DNH5QI\\ecog.html},
  journal = {Ecography},
  keywords = {point process models,spatio-temporal log-Gaussian Cox process,species distribution models},
  language = {en},
  number = {6}
}

@misc{speakAdjustedShotLocations2019,
  title = {Adjusted {{Shot Locations}}},
  author = {Speak, Hayden},
  year = {2019},
  month = apr,
  howpublished = {http://archive.is/PWAWM},
  journal = {archive.is}
}

@article{spiegelhalterBayesianMeasuresModel2002,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and {van der Linde}, Angelika},
  year = {2002},
  month = oct,
  volume = {64},
  pages = {583--639},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/1467-9868.00353},
  abstract = {We consider the problem of comparing complex hierarchical models in which the number of parametersis not clearly defined. Using an informationtheoretic argument we derive a measure p\textasciitilde{} for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general p\textasciitilde{} approximately corresponds to the trace of the product of Fisher's informationand the posterior covariance,which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding p\textasciitilde{} to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\445LVVL2\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;C\:\\Users\\devan\\Zotero\\storage\\6CMP8QQV\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;C\:\\Users\\devan\\Zotero\\storage\\FJ7SBGZM\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;C\:\\Users\\devan\\Zotero\\storage\\LJ87C4A7\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {4}
}

@article{spiegelhalterBayesianMeasuresModel2002a,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Linde, Angelika Van Der},
  year = {2002},
  volume = {64},
  pages = {583--639},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00353},
  abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8VSRIIZN\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;C\:\\Users\\devan\\Zotero\\storage\\U396JJ8W\\1467-9868.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
  language = {en},
  number = {4}
}

@article{starkConstraintsPriors2015,
  title = {Constraints versus {{Priors}}},
  author = {Stark, Philip B.},
  year = {2015},
  month = jan,
  volume = {3},
  pages = {586--598},
  issn = {2166-2525},
  doi = {10.1137/130920721},
  abstract = {Many scientific problems have unknown parameters that are thought to lie in some known set. For instance, the amount of energy absorbed by an x-ray specimen must be between 0 and 100\% of the incident energy. Similar constraints arise in expressing ``epistemic'' uncertainty. Such prior information can be handled directly by frequentist methods. In contrast, Bayesian methods involve supplementing the constraint with a prior probability distribution for the parameter. This can cause frequentist and Bayesian estimates and the nominal uncertainties of those estimates to differ substantially. Moreover, Bayesian and frequentist definitions of uncertainty may sound similar, but they measure quite different things. For instance, Bayesian uncertainties generally involve expectations with respect to the posterior distribution of the parameter, holding the data fixed, while frequentist uncertainties generally involve expectations with respect to the distribution of the data, holding the parameter fixed. This paper gives simple examples where ``uninformative'' priors are in fact extremely informative, and sketches how to measure how much information the prior adds to the constraint.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DFIXWIQ6\\Stark - 2015 - Constraints versus Priors.pdf},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  language = {en},
  number = {1}
}

@article{steelFireFrequencyseverityRelationship2015,
  title = {The Fire Frequency-Severity Relationship and the Legacy of Fire Suppression in {{California}} Forests},
  author = {Steel, Zachary L. and Safford, Hugh D. and Viers, Joshua H.},
  year = {2015},
  month = jan,
  volume = {6},
  pages = {art8},
  issn = {2150-8925},
  doi = {10.1890/ES14-00224.1},
  abstract = {Fire is one of the most important natural disturbance processes in the western United States and ecosystems differ markedly with respect to their ecological and evolutionary relationships with fire. Reference fire regimes in forested ecosystems can be categorized along a gradient ranging from ``fuellimited'' to ``climate-limited'' where the former types are often characterized by frequent, lower-severity wildfires and the latter by infrequent, more severe wildfires. Using spatial data on fire severity from 1984\textendash 2011 and metrics related to fire frequency, we tested how divergence from historic (pre-Euroamerican settlement) fire frequencies due to a century of fire suppression influences rates of high-severity fire in five forest types in California. With some variation among bioregions, our results suggest that fires in forest types characterized by fuel-limited fire regimes (e.g., yellow pine and mixed conifer forest) tend to burn with greater proportions of high-severity fire as either time since last fire or the mean modern fire return interval (FRI) increases. Two intermediate fire regime types (mixed evergreen and bigcone Douglas-fir) showed a similar relationship between fire frequency and fire severity. However, red fir and redwood forests, which are characterized by more climate-limited fire regimes, did not show significant positive relationships between FRI and fire severity. This analysis provides strong evidence that for fuel-limited fire regimes, lack of fire leads to increasing rates of high-severity burning. Our study also substantiates the general validity of ``fuel-limited'' vs. ``climate-limited'' explanations of differing patterns of fire effects and response in forest types of the western US.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PU9FWV73\\Steel et al. - 2015 - The fire frequency-severity relationship and the l.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZH576BQF\\Steel et al. - 2015 - The fire frequency-severity relationship and the l.pdf},
  journal = {Ecosphere},
  keywords = {Fire Good},
  language = {en},
  number = {1}
}

@article{stephensManagingForestsFire2013,
  title = {Managing {{Forests}} and {{Fire}} in {{Changing Climates}}},
  author = {Stephens, S. L. and Agee, J. K. and Ful{\'e}, P. Z. and North, M. P. and Romme, W. H. and Swetnam, T. W. and Turner, M. G.},
  year = {2013},
  month = oct,
  volume = {342},
  pages = {41--42},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1240294},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2QDEBXRD\\Stephens et al. - 2013 - Managing Forests and Fire in Changing Climates.pdf;C\:\\Users\\devan\\Zotero\\storage\\5I7R9X2U\\Stephens et al. - 2013 - Managing Forests and Fire in Changing Climates.pdf;C\:\\Users\\devan\\Zotero\\storage\\TZWA48L2\\Stephens et al. - 2013 - Managing Forests and Fire in Changing Climates.pdf},
  journal = {Science},
  keywords = {Fire Good},
  language = {en},
  number = {6154}
}

@article{stocksLargeForestFires2002,
  title = {Large Forest Fires in {{Canada}}, 1959\textendash 1997},
  author = {Stocks, B. J. and Mason, J. A. and Todd, J. B. and Bosch, E. M. and Wotton, B. M. and Amiro, B. D. and Flannigan, M. D. and Hirsch, K. G. and Logan, K. A. and Martell, D. L. and Skinner, W. R.},
  year = {2002},
  month = dec,
  volume = {108},
  pages = {8149},
  issn = {0148-0227},
  doi = {10.1029/2001JD000484},
  abstract = {Large Fire Database (LFDB), which includes information on fire location, start date, final size, cause, and suppression action, has been developed for all fires larger than 200 ha in area for Canada for the 1959\textendash 1997 period. The LFDB represents only 3.1\% of the total number of Canadian fires during this period, the remaining 96.9\% of fires being suppressed while {$<$}200 ha in size, yet accounts for  97\% of the total area burned, allowing a spatial and temporal analysis of recent Canadian landscape-scale fire impacts. On average  2 million ha burned annually in these large fires, although more than 7 million ha burned in some years. Ecozones in the boreal and taiga regions experienced the greatest areas burned, with an average of 0.7\% of the forested land burning annually. Lightning fires predominate in northern Canada, accounting for 80\% of the total LFDB area burned. Large fires, although small in number, contribute substantially to area burned, most particularly in the boreal and taiga regions. The Canadian fire season runs from late April through August, with most of the area burned occurring in June and July due primarily to lightning fire activity in northern Canada. Close to 50\% of the area burned in Canada is the result of fires that are not actioned due to their remote location, low values-at-risk, and efforts to accommodate the natural role of fire in these ecosystems. The LFDB is updated annually and is being expanded back in time to permit a more thorough analysis of long-term trends in Canadian fire activity.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4XUFLK2U\\Stocks et al. - 2002 - Large forest fires in Canada, 1959–1997.pdf;C\:\\Users\\devan\\Zotero\\storage\\IJUEVL59\\Stocks et al. - 2002 - Large forest fires in Canada, 1959–1997.pdf;C\:\\Users\\devan\\Zotero\\storage\\NKMZ5G64\\Stocks et al. - 2002 - Large forest fires in Canada, 1959–1997.pdf},
  journal = {Journal of Geophysical Research},
  keywords = {EDA,Fire Good,Size,Tail Behaviour},
  language = {en},
  number = {D1}
}

@misc{stoyanSecondorderCharacteristicsStochastic,
  title = {Second-Order {{Characteristics}} for {{Stochastic Structures Connected}} with {{Gibbs Point Process}}},
  author = {Stoyan, Dietrich and Grabarnik, Pavel},
  file = {C\:\\Users\\devan\\Zotero\\storage\\58EE72L6\\Second-order Characteristics for Stochastic Structures Connected with Gibbs Point Process.pdf}
}

@article{stoyanVariogramsPointProcess2000,
  title = {On {{Variograms}} in {{Point Process Statistics}}, {{II}}: {{Models}} of {{Markings}} and {{Ecological Interpretation}}},
  author = {Stoyan, Dietrich},
  year = {2000},
  pages = {17},
  abstract = {This paper presents new models for marked point processes for describing forestry data. In these models two factors play a role: Long-range variability is modeled by a random field, which may describe environmental variability, while short-range variability is caused by the interaction of points of two classes. These models may help in the interpretation of empirical mark variograms as is shown by two examples from forestry statistics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NITU3HLA\\Stoyan - 2000 - On Variograms in Point Process Statistics, II Mod.pdf},
  journal = {Biometrical Journal},
  language = {en}
}

@article{stramVarianceComponentsTesting1994,
  title = {Variance {{Components Testing}} in the {{Longitudinal Mixed Effects Model}}},
  author = {Stram, Daniel O. and Lee, Jae Won},
  year = {1994},
  volume = {50},
  pages = {1171--1177},
  issn = {0006-341X},
  doi = {10.2307/2533455},
  abstract = {This article discusses the asymptotic behavior of likelihood ratio tests for nonzero variance components in the longitudinal mixed effects linear model described by Laird and Ware (1982, Biometrics 38, 963-974). Our discussion of the large-sample behavior of likelihood ratio tests for nonzero variance components is based on the results for nonstandard testing situations by Self and Liang (1987, Journal of the American Statistical Association 82, 605-610).},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WADFQY8G\\Stram and Lee - 1994 - Variance Components Testing in the Longitudinal Mi.pdf},
  journal = {Biometrics},
  number = {4}
}

@article{Strauss2010,
  title = {Pilot {{Fatigue}}},
  author = {Strauss, Samuel},
  year = {2010},
  pages = {28--32},
  journal = {Aerospace Medicine NASA/Johnson Space Center, Houston, Texas. http://aeromedical. org/Articles/Pilot{{\_}}Fatigue. html}
}

@article{SurveyRecentAdvances2013,
  title = {Survey of Some Recent Advances in Spatial-Temporal Point Processes},
  year = {2013},
  pages = {41},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IYHGCZC3\\2013 - Survey of some recent advances in spatial-temporal.pdf},
  language = {en}
}

@article{swartzNewInsightsInvolving2014,
  title = {New {{Insights Involving}} the {{Home Team Advantage}}},
  author = {Swartz, Tim B. and Arce, Adriano},
  year = {2014},
  month = sep,
  volume = {9},
  pages = {681--692},
  issn = {1747-9541, 2048-397X},
  doi = {10.1260/1747-9541.9.4.681},
  abstract = {Although the home team advantage is known to exist in many sports, there are nuances of the advantage that are less well understood. In this paper, we investigate various aspects of the home team advantage including changes in the advantage over time, the relationship of the advantage to the overall scoring rate and differential advantages within leagues. The analysis is mainly based on descriptive statistics and is confined to the home team advantage pertaining to the National Hockey League and the National Basketball Association.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BGZ8C4B7\\Swartz and Arce - 2014 - New Insights Involving the Home Team Advantage.pdf},
  journal = {International Journal of Sports Science \& Coaching},
  language = {en},
  number = {4}
}

@article{swartzWhereShouldPublish2018,
  title = {Where {{Should I Publish My Sports Paper}}?},
  author = {Swartz, Tim B.},
  year = {2018},
  month = apr,
  pages = {1--6},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1459842},
  abstract = {With the increasing fascination of sport in society and the increasing availability of sport-related data, there are great opportunities to carry out sports analytics research. In this article, we discuss some of the issues that are relevant to publishing in the field of sports analytics. Potential publication outlets are identified, some summary statistics are given, and some experiences and opinions are provided.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H89BVMAV\\Swartz - 2018 - Where Should I Publish My Sports Paper.pdf;C\:\\Users\\devan\\Zotero\\storage\\C43PWZA8\\00031305.2018.html},
  journal = {The American Statistician}
}

@article{swartzWhereShouldPublish2018a,
  title = {Where {{Should I Publish My Sports Paper}}?},
  author = {Swartz, Tim B.},
  year = {2018},
  month = apr,
  pages = {1--6},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1459842},
  abstract = {With the increasing fascination of sport in society and the increasing availability of sport related data, there are great opportunities to carry out sports analytics research. In this paper, we discuss some of the issues that are relevant to publishing in the field of sports analytics. Potential publication outlets are identified, some summary statistics are given, and some experiences and opinions are provided.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KZLN2XXY\\Swartz - 2018 - Where Should I Publish My Sports Paper.pdf;C\:\\Users\\devan\\Zotero\\storage\\UBC4SW4E\\Swartz - 2018 - Where Should I Publish My Sports Paper.pdf},
  journal = {The American Statistician},
  language = {en}
}

@article{tamuraProspectsInferringVery2004,
  title = {Prospects for Inferring Very Large Phylogenies by Using the Neighbor-Joining Method},
  author = {Tamura, K. and Nei, M. and Kumar, S.},
  year = {2004},
  month = jul,
  volume = {101},
  pages = {11030--11035},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0404206101},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TWLMWYWY\\Tamura et al. - 2004 - Prospects for inferring very large phylogenies by .pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {30}
}

@article{tanakaParameterEstimationModel2008,
  title = {Parameter {{Estimation}} and {{Model Selection}} for {{Neyman}}-{{Scott Point Processes}}},
  author = {Tanaka, Ushio and Ogata, Yosihiko and Stoyan, Dietrich},
  year = {2008},
  month = feb,
  volume = {50},
  pages = {43--57},
  issn = {03233847, 15214036},
  doi = {10.1002/bimj.200610339},
  abstract = {This paper proposes an approximative method for maximum likelihood estimation of parameters of Neyman-Scott and similar point processes. It is based on the point pattern resulting from forming all difference points of pairs of points in the window of observation. The intensity function of this constructed point process can be expressed in terms of second-order characteristics of the original process. This opens the way to parameter estimation, if the difference pattern is treated as a non-homogeneous Poisson process. The computational feasibility and accuracy of this approach is examined by means of simulated data. Furthermore, the method is applied to two biological data sets. For these data, various cluster process models are considered and compared with respect to their goodness-of-fit.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\25ALNHMZ\\Tanaka et al. - 2008 - Parameter Estimation and Model Selection for Neyma.pdf;C\:\\Users\\devan\\Zotero\\storage\\3RGDG5HE\\Tanaka et al. - 2008 - Parameter Estimation and Model Selection for Neyma.pdf},
  journal = {Biometrical Journal},
  language = {en},
  number = {1}
}

@article{taylorBayesianInferenceData2015,
  title = {Bayesian {{Inference}} and {{Data Augmentation Schemes}} for {{Spatial}}, {{Spatiotemporal}} and {{Multivariate Log}}-{{Gaussian Cox Processes}} in {{{\emph{R}}}}},
  author = {Taylor, Benjamin M. and Davies, Tilman M. and Rowlingson, Barry S. and Diggle, Peter J.},
  year = {2015},
  volume = {63},
  issn = {1548-7660},
  doi = {10.18637/jss.v063.i07},
  abstract = {Log-Gaussian Cox processes are an important class of models for spatial and spatiotemporal point-pattern data. Delivering robust Bayesian inference for this class of models presents a substantial challenge, since Markov chain Monte Carlo (MCMC) algorithms require careful tuning in order to work well. To address this issue, we describe recent advances in MCMC methods for these models and their implementation in the R package lgcp. Our suite of R functions provides an extensible framework for inferring covariate effects as well as the parameters of the latent field.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2ZVE8CTF\\Taylor et al. - 2015 - Bayesian Inference and Data Augmentation Schemes f.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {7}
}

@article{taylorCorrigendumSpatiotemporalPrediction2013,
  title = {Corrigendum: {{Spatiotemporal}} Prediction for Log-{{Gaussian Cox}} Processes},
  shorttitle = {Corrigendum},
  author = {Taylor, Benjamin M. and Diggle, Peter J.},
  year = {2013},
  month = jun,
  volume = {75},
  pages = {601--602},
  issn = {13697412},
  doi = {10.1111/rssb.12008},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9R6S9NC3\\Taylor and Diggle - 2013 - Corrigendum Spatiotemporal prediction for log-Gau.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {3}
}

@article{taylorINLAMCMCTutorial2012,
  title = {{{INLA}} or {{MCMC}}? {{A Tutorial}} and {{Comparative Evaluation}} for {{Spatial Prediction}} in Log-{{Gaussian Cox Processes}}},
  shorttitle = {{{INLA}} or {{MCMC}}?},
  author = {Taylor, Benjamin M. and Diggle, Peter J.},
  year = {2012},
  month = feb,
  abstract = {We investigate two options for performing Bayesian inference on spatial log-Gaussian Cox processes assuming a spatially continuous latent field: Markov chain Monte Carlo (MCMC) and the integrated nested Laplace approximation (INLA). We first describe the device of approximating a spatially continuous Gaussian field by a Gaussian Markov random field on a discrete lattice, and present a simulation study showing that, with careful choice of parameter values, small neighbourhood sizes can give excellent approximations. We then introduce the spatial log-Gaussian Cox process and describe MCMC and INLA methods for spatial prediction within this model class. We report the results of a simulation study in which we compare MALA and the technique of approximating the continuous latent field by a discrete one, followed by approximate Bayesian inference via INLA over a selection of 18 simulated scenarios. The results question the notion that the latter technique is both significantly faster and more robust than MCMC in this setting; 100,000 iterations of the MALA algorithm running in 20 minutes on a desktop PC delivered greater predictive accuracy than the default INLA strategy, which ran in 4 minutes and gave comparative performance to the full Laplace approximation which ran in 39 minutes.},
  archivePrefix = {arXiv},
  eprint = {1202.1738},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H54IUCAG\\Taylor and Diggle - 2012 - INLA or MCMC A Tutorial and Comparative Evaluatio.pdf},
  journal = {arXiv:1202.1738 [stat]},
  keywords = {Statistics - Computation},
  language = {en},
  primaryClass = {stat}
}

@article{taylorINLAMCMCTutorial2014,
  title = {{{INLA}} or {{MCMC}}? {{A}} Tutorial and Comparative Evaluation for Spatial Prediction in Log-{{Gaussian Cox}} Processes},
  shorttitle = {{{INLA}} or {{MCMC}}?},
  author = {Taylor, Benjamin M. and Diggle, Peter J.},
  year = {2014},
  month = oct,
  volume = {84},
  pages = {2266--2284},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2013.788653},
  file = {C\:\\Users\\devan\\Zotero\\storage\\8BZXFKIT\\Taylor and Diggle - 2014 - INLA or MCMC A tutorial and comparative evaluatio.pdf;C\:\\Users\\devan\\Zotero\\storage\\W6RY3XQR\\Taylor and Diggle - 2014 - INLA or MCMC A tutorial and comparative evaluatio.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {10}
}

@article{taylorLgcpPackageInference2013,
  title = {Lgcp : {{An R Package}} for {{Inference}} with {{Spatial}} and {{Spatio}}-{{Temporal Log}}-{{Gaussian Cox Processes}}},
  shorttitle = {{\textbf{Lgcp}}},
  author = {Taylor, Benjamin M. and Davies, Tilman M. and Rowlingson, Barry S. and Diggle, Peter J.},
  year = {2013},
  volume = {52},
  issn = {1548-7660},
  doi = {10.18637/jss.v052.i04},
  abstract = {This paper introduces an R package for spatial and spatio-temporal prediction and forecasting for log-Gaussian Cox processes. The main computational tool for these models is Markov chain Monte Carlo (MCMC) and the new package, lgcp, therefore also provides an extensible suite of functions for implementing MCMC algorithms for processes of this type. The modelling framework and details of inferential procedures are first presented before a tour of lgcp functionality is given via a walk-through data-analysis. Topics covered include reading in and converting data, estimation of the key components and parameters of the model, specifying output and simulation quantities, computation of Monte Carlo expectations, post-processing and simulation of data sets.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\77HG6DNB\\Taylor et al. - 2013 - blgcpb  An iRi Package for Inference wi.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {4}
}

@article{taylorWildfirePredictionInform2013,
  title = {Wildfire {{Prediction}} to {{Inform Management}}: {{Statistical Science Challenges}}},
  author = {Taylor, S. W. and Woolford, Douglas G. and Dean, C. B. and Martell, David L.},
  year = {2013},
  volume = {28},
  pages = {586--615},
  abstract = {Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UP8JMDZ5\\Taylor et al. - 2013 - Wildfire Prediction to Inform Management Statisti.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {4,}
}

@article{taylorWildfirePredictionInform2013a,
  title = {Wildfire {{Prediction}} to {{Inform Management}}: {{Statistical Science Challenges}}},
  shorttitle = {Wildfire {{Prediction}} to {{Inform Management}}},
  author = {Taylor, S. W. and Woolford, Douglas G. and Dean, C. B. and Martell, David L.},
  year = {2013},
  volume = {28},
  pages = {586--615},
  issn = {0883-4237},
  abstract = {Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UX2Z7J25\\Taylor et al. - 2013 - Wildfire Prediction to Inform Management Statisti.pdf},
  journal = {Statistical Science},
  number = {4}
}

@article{taylorWildfirePredictionInform2013b,
  title = {Wildfire {{Prediction}} to {{Inform Fire Management}}: {{Statistical Science Challenges}}},
  shorttitle = {Wildfire {{Prediction}} to {{Inform Fire Management}}},
  author = {Taylor, S. W. and Woolford, Douglas G. and Dean, C. B. and Martell, David L.},
  year = {2013},
  month = nov,
  volume = {28},
  pages = {586--615},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/13-STS451},
  abstract = {Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UIVPDYR6\\Taylor et al. - 2013 - Wildfire Prediction to Inform Fire Management Sta.pdf;C\:\\Users\\devan\\Zotero\\storage\\JZJ5XMDR\\1386078880.html},
  journal = {Statistical Science},
  keywords = {Environmetrics,forest fire,prediction,review,wildland fire},
  language = {EN},
  mrnumber = {MR3161589},
  number = {4},
  zmnumber = {1331.86029}
}

@article{tengBayesianComputationLogGaussian2017,
  title = {Bayesian Computation for {{Log}}-{{Gaussian Cox}} Processes: A Comparative Analysis of Methods},
  shorttitle = {Bayesian Computation for {{Log}}-{{Gaussian Cox}} Processes},
  author = {Teng, Ming and Nathoo, Farouk and Johnson, Timothy D.},
  year = {2017},
  month = jul,
  volume = {87},
  pages = {2227--2252},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2017.1326117},
  abstract = {The Log-Gaussian Cox process is a commonly used model for the analysis of spatial point pattern data. Fitting this model is difficult because of its doubly stochastic property, that is, it is a hierarchical combination of a Poisson process at the first level and a Gaussian process at the second level. Various methods have been proposed to estimate such a process, including traditional likelihood-based approaches as well as Bayesian methods. We focus here on Bayesian methods and several approaches that have been considered for model fitting within this framework, including Hamiltonian Monte Carlo, the Integrated nested Laplace approximation, and Variational Bayes. We consider these approaches and make comparisons with respect to statistical and computational efficiency. These comparisons are made through several simulation studies as well as through two applications, the first examining ecological data and the second involving neuroimaging data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5T7QWKXE\\Teng et al. - 2017 - Bayesian computation for Log-Gaussian Cox processe.pdf;C\:\\Users\\devan\\Zotero\\storage\\H6RT258P\\Teng et al. - 2017 - Bayesian computation for Log-Gaussian Cox processe.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {11}
}

@article{tengBayesianComputationLogGaussian2017a,
  title = {Bayesian {{Computation}} for {{Log}}-{{Gaussian Cox Processes}}--{{A Comparative Analysis}} of {{Methods}}},
  author = {Teng, Ming and Nathoo, Farouk S. and Johnson, Timothy D.},
  year = {2017},
  month = jan,
  abstract = {The Log-Gaussian Cox Process is a commonly used model for the analysis of spatial point patterns. Fitting this model is difficult because of its doubly-stochastic property, i.e., it is an hierarchical combination of a Poisson process at the first level and a Gaussian Process at the second level. Different methods have been proposed to estimate such a process, including traditional likelihood-based approaches as well as Bayesian methods. We focus here on Bayesian methods and several approaches that have been considered for model fitting within this framework, including Hamiltonian Monte Carlo, the Integrated nested Laplace approximation, and Variational Bayes. We consider these approaches and make comparisons with respect to statistical and computational efficiency. These comparisons are made through several simulations studies as well as through applications examining both ecological data and neuroimaging data.},
  archivePrefix = {arXiv},
  eprint = {1701.00857},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VQUJHWIR\\Teng et al. - 2017 - Bayesian Computation for Log-Gaussian Cox Processe.pdf},
  journal = {arXiv:1701.00857 [stat]},
  keywords = {Statistics - Computation},
  language = {en},
  primaryClass = {stat}
}

@article{thomasInterarrivalTimesGoals2007,
  title = {Inter-Arrival {{Times}} of {{Goals}} in {{Ice Hockey}}},
  author = {Thomas, Andrew C},
  year = {2007},
  volume = {3},
  issn = {1559-0410},
  doi = {10.2202/1559-0410.1064},
  abstract = {Previous studies have attempted to model goal scoring in sports such as ice hockey as simple Poisson processes. Others (Thomas, 2006) have shown that events within the game of ice hockey are better modelled as a Semi-Markov process determined by puck possession and location. I demonstrate that a similarly defined Semi-Markov process model is well-suited to describe the times between goals scored in NHL hockey, and use this to demonstrate that the scoring of a goal has the effect of shortening the remainder of the game by roughly 20 seconds. This is used to improve previous estimates of the value of a goal scored, calculated as a difference in win probabilities at specific time points during the game.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Q4FXH4J3\\Thomas - 2007 - Inter-arrival Times of Goals in Ice Hockey.pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  keywords = {hazard function,hockey,semi-Markov process,survival analysis},
  number = {3}
}

@article{thompsonAnisotropicKernelSmoothing2018,
  title = {Anisotropic Kernel Smoothing for Change-Point Data with an Analysis of Fire Spread Rate Variability},
  author = {Thompson, John},
  year = {2018},
  month = nov,
  file = {C\:\\Users\\devan\\Zotero\\storage\\FKLMRUVY\\Thompson - 2018 - Anisotropic kernel smoothing for change-point data.pdf;C\:\\Users\\devan\\Zotero\\storage\\Y6E4MGQS\\5889.html},
  journal = {Electronic Thesis and Dissertation Repository}
}

@article{thorntonLightningEnhancementMajor2017,
  title = {Lightning Enhancement over Major Oceanic Shipping Lanes},
  author = {Thornton, Joel A. and Virts, Katrina S. and Holzworth, Robert H. and Mitchell, Todd P.},
  year = {2017},
  volume = {44},
  pages = {9102--9111},
  issn = {1944-8007},
  doi = {10.1002/2017GL074982},
  abstract = {Using 12 years of high-resolution global lightning stroke data from the World Wide Lightning Location Network (WWLLN), we show that lightning density is enhanced by up to a factor of 2 directly over shipping lanes in the northeastern Indian Ocean and the South China Sea as compared to adjacent areas with similar climatological characteristics. The lightning enhancement is most prominent during the convectively active season, November\textendash April for the Indian Ocean and April\textendash December in the South China Sea, and has been detectable from at least 2005 to the present. We hypothesize that emissions of aerosol particles and precursors by maritime vessel traffic lead to a microphysical enhancement of convection and storm electrification in the region of the shipping lanes. These persistent localized anthropogenic perturbations to otherwise clean regions are a unique opportunity to more thoroughly understand the sensitivity of maritime deep convection and lightning to aerosol particles.},
  copyright = {\textcopyright 2017. The Authors.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GQVZ66XR\\Thornton et al. - 2017 - Lightning enhancement over major oceanic shipping .pdf;C\:\\Users\\devan\\Zotero\\storage\\YAMGQP7N\\2017GL074982.html},
  journal = {Geophysical Research Letters},
  keywords = {aerosols,CCN,convection,lightning,shipping,WWLLN},
  language = {en},
  number = {17}
}

@misc{tHowTellDifference2011,
  title = {How to Tell the Difference between a Worn-out Penny and a Punk Student},
  author = {T, Eric},
  year = {2011},
  month = feb,
  abstract = {The stat guys talk a lot about variance, luck, and regression to the mean. A lot of it doesn't mesh up with our intuition, and I wanted to meander through its implications a little bit. I'll start...},
  file = {C\:\\Users\\devan\\Zotero\\storage\\77AR8FI3\\how-to-tell-the-difference-between-a-worn-out-penny-and-a-punk-student.html},
  howpublished = {https://www.broadstreethockey.com/2011/2/23/2009446/how-to-tell-the-difference-between-a-worn-out-penny-and-a-punk-student},
  journal = {Broad Street Hockey}
}

@article{tibshiraniRegressionShrinkageSelection1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  volume = {58},
  pages = {267--288},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {1}
}

@article{TimeSeriesModelingStatistical,
  title = {Time-{{Series Modeling}} for {{Statistical Process Control}}},
  pages = {10},
  file = {C\:\\Users\\devan\\Zotero\\storage\\665JZRXH\\Time-Series Modeling for Statistical Process Contr.pdf;C\:\\Users\\devan\\Zotero\\storage\\8WFRGE3I\\Time-Series Modeling for Statistical Process Contr.pdf},
  language = {en}
}

@article{tsiatisJointModelingLongitudinal,
  title = {Joint {{Modeling}} of {{Longitudinal}} and {{Time}}-to-{{Event Data}}: {{An Overview}}},
  author = {Tsiatis, Anastasios A and Davidian, Marie},
  pages = {36},
  abstract = {A common objective in longitudinal studies is to characterize the relationship between a longitudinal response process and a time-to-event. Considerable recent interest has focused on so-called joint models, where models for the event time distribution and longitudinal data are taken to depend on a common set of latent random effects. In the literature, precise statement of the underlying assumptions typically made for these models has been rare. We review the rationale for and development of joint models, offer insight into the structure of the likelihood for model parameters that clarifies the nature of common assumptions, and describe and contrast some of our recent proposals for implementation and inference.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\39BEV25K\\Tsiatis and Davidian - Joint Modeling of Longitudinal and Time-to-Event D.pdf;C\:\\Users\\devan\\Zotero\\storage\\9EVG3DM3\\Tsiatis and Davidian - Joint Modeling of Longitudinal and Time-to-Event D.pdf},
  language = {en}
}

@article{tsiatisJointModellingLongitudinal2004,
  title = {Joint {{Modelling}} of {{Longitudinal}} and {{Time}}-to-{{Event Data}}: {{An Overview}}},
  shorttitle = {{{JOINT MODELING OF LONGITUDINAL AND TIME}}-{{TO}}-{{EVENT DATA}}},
  author = {Tsiatis, Anastasios A. and Davidian, Marie},
  year = {2004},
  volume = {14},
  pages = {809--834},
  issn = {1017-0405},
  abstract = {A common objective in longitudinal studies is to characterize the relationship between a longitudinal response process and a time-to-event. Considerable recent interest has focused on so-called joint models, where models for the event time distribution and longitudinal data are taken to depend on a common set of latent random effects. In the literature, precise statement of the underlying assumptions typically made for these models has been rare. We review the rationale for and development of joint models, offer insight into the structure of the likelihood for model parameters that clarifies the nature of common assumptions, and describe and contrast some of our recent proposals for implementation and inference.},
  journal = {Statistica Sinica},
  number = {3}
}

@book{Tukey1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {{Addison-Wesley Publishing Company}},
  abstract = {Une introduction historique aux diff\'erentes mani\`eres de compter et repr\'esenter manuellement les donn\'ees pour faire de l'analyse explorative de donn\'ees.}
}

@article{turkmanGeneratingAnnualFire2014,
  title = {Generating {{Annual Fire Risk Maps Using Bayesian Hierarchical Models}}},
  author = {Turkman, K. F. and Turkman, M. A. Amaral and Pereira, P. and S{\'a}, A. and Pereira, J. M. C.},
  year = {2014},
  month = jul,
  volume = {8},
  pages = {509--533},
  issn = {1559-8608, 1559-8616},
  doi = {10.1080/15598608.2013.820158},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YL3D6NQK\\Turkman et al. - 2014 - Generating Annual Fire Risk Maps Using Bayesian Hi.pdf},
  journal = {Journal of Statistical Theory and Practice},
  language = {en},
  number = {3}
}

@article{turnerPointPatternsForest2009,
  title = {Point Patterns of Forest Fire Locations},
  author = {Turner, Rolf},
  year = {2009},
  month = jun,
  volume = {16},
  pages = {197--223},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-007-0085-1},
  abstract = {In this paper I demonstrate some of the techniques for the analysis of spatial point patterns that have become available due to recent developments in point process modelling software. These developments permit convenient exploratory data analysis, model fitting, and model assessment. Efficient model fitting, in particular, makes possible the testing of statistical hypotheses of genuine interest, even when interaction between points is present, via Monte Carlo methods. The discussion of these techniques is conducted jointly with and in the context of some preliminary analyses of a collection of data sets which are of considerable interest in their own right. These data sets (which were kindly provided to me by the New Brunswick Department of Natural Resources) consist of the complete records of wildfires which occurred in New Brunswick during the years 1987 through 2003. In treating these data sets I deal with data-cleaning problems, methods of exploratory data analysis, means of detecting interaction, fitting of statistical models, and residual analysis and diagnostics. In addition to demonstrating modelling techniques, I include a discussion on the nature of statistical models for point patterns. This is given with a view to providing an understanding of why, in particular, the Strauss model fails as a model for interpoint attraction and how it has been modified to overcome this difficulty. All actual modelling of the New Brunswick fire data is done only with the intent of illustrating techniques. No substantive conclusions are or can be drawn at this stage. Realistic modelling of these data sets would require incorporation of covariate information which I do not so far have available.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9LBHXQ9K\\Turner - 2009 - Point patterns of forest fire locations.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {2}
}

@article{tymstraStatisticalSurveillanceThresholds2019,
  title = {Statistical {{Surveillance Thresholds}} for {{Enhanced Situational Awareness}} of {{Spring Wildland Fire Activity}} in {{Alberta}}, {{Canada}}},
  author = {Tymstra, Cordy and Woolford, Douglas G and Flannigan, Mike D},
  year = {2019},
  volume = {9},
  pages = {26},
  abstract = {Wildland fire disasters across Canada and globally are increasing in frequency. Alberta's spring wildfire season is a particularly challenging period. Situational awareness of the wildfire environment is critical for wildfire management agencies to be prepared when extreme events occur. We propose the use of simple initial attack (IA) and being held (BH) escape surveillance charts in near-real time with thresholds as tools for enhancing and tracking situational awareness. Since the discrete data sets we used are zero-inflated and over-dispersed we chose to model the exceedances over a threshold. We also used preceding December sea surface temperatures (SST) of the Pacific Ocean as an indicator of persistent spring wildfire activity. Our analysis indicates the tracking of IA and BH escapes and SST can provide additional decision support as part of an early warning system of spring wildfire risk.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\NLKD2LHG\\Tymstra et al. - Statistical Surveillance Thresholds for Enhanced S.pdf},
  journal = {Journal of Environmental Statistics},
  language = {en},
  number = {4}
}

@article{tzalaBayesianLatentVariable2008,
  title = {Bayesian Latent Variable Modelling of Multivariate Spatio-Temporal Variation in Cancer Mortality},
  author = {Tzala, Evangelia and Best, Nicky},
  year = {2008},
  month = feb,
  volume = {17},
  pages = {97--118},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280207081243},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CBAYKRZK\\Tzala and Best - 2008 - Bayesian latent variable modelling of multivariate.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {1}
}

@article{vanderlaanImputationRoundedData2011,
  title = {{Imputation of rounded data}},
  author = {{van der Laan}, Jan and Kuijvenhoven, L{\'e}ander},
  year = {2011},
  abstract = {In surveys persons have a tendency to round their answers. For example, in the Labour Force Survey people are asked about the period they have been unemployed. There is clearly a tendency to give answers that are rounded to years of half years. Because of this rounding statistics based on this data tend to be biased. In this paper we introduce a method with which the rounding mechanism is modelled together with the `true' underlying distribution. These are then used to select samples which are likely to be rounded an impute new values for these. This method is applied to the Labour Force Survey data. An investigation of robustness shows that the method is robust against misspecification of the model of the underlying distribution and to misspecification of the rounding mechanism.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\RLRMH679\\Statistiek - Imputation of rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\72BYE4XB\\imputation-of-rounded-data.html},
  journal = {Statistics Netherlands},
  language = {nl-NL}
}

@book{vanmontfortLongitudinalResearchLatent2010,
  title = {Longitudinal {{Research}} with {{Latent Variables}}},
  editor = {{van Montfort}, Kees and Oud, Johan H.L. and Satorra, Albert},
  year = {2010},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-11760-2},
  abstract = {Mixed models have become very popular for the analysis of longitudinal data, partly because they are flexible and widely applicable, partly also because many commercially available software packages offer procedures to fit them. They assume that measurements from a single subject share a set of latent, unobserved, random effects which are used to generate an association structure between the repeated measurements. In this chapter, we give an overview of frequently used mixed models for continuous as well as discrete longitudinal data, with emphasis on model formulation and parameter interpretation. The fact that the latent structures generate associations implies that mixed models are also extremely convenient for the joint analysis of longitudinal data with other outcomes such as dropout time or some time-to-event outcome, or for the analysis of multiple longitudinally measured outcomes. All models will be extensively illustrated with the analysis of real data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3RTDE3QT\\van Montfort et al. - 2010 - Longitudinal Research with Latent Variables.pdf},
  isbn = {978-3-642-11759-6 978-3-642-11760-2},
  language = {en}
}

@article{vanniekerkCompetingRisksJoint2019,
  title = {Competing Risks Joint Models Using {{R}}-{{INLA}}},
  author = {{van Niekerk}, Janet and Bakka, Haakon and Rue, Haavard},
  year = {2019},
  month = sep,
  abstract = {The methodological advancements made in the field of joint models are numerous. None the less, the case of competing risks joint models have largely been neglected, especially from a practitioner's point of view. In the relevant works on competing risks joint models, the assumptions of Gaussian linear longitudinal series and proportional cause-specific hazard functions, amongst others, have remained unchallenged. In this paper, we provide a framework based on R-INLA to apply competing risks joint models in a unifying way such that non-Gaussian longitudinal data, spatial structures, time dependent splines and various latent association structures, to mention a few, are all embraced in our approach. Our motivation stems from the SANAD trial which exhibits non-linear longitudinal trajectories and competing risks for failure of treatment. We also present a discrete competing risks joint model for longitudinal count data as well as a spatial competing risks joint model, as specific examples.},
  archivePrefix = {arXiv},
  eprint = {1909.01637},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WU762LAT\\van Niekerk et al. - 2019 - Competing risks joint models using R-INLA.pdf},
  journal = {arXiv:1909.01637 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{vanniekerkJointModelsLatent2019,
  title = {Joint Models as Latent {{Gaussian}} Models - Not Reinventing the Wheel},
  author = {{van Niekerk}, Janet and Bakka, Haakon and Rue, Haavard},
  year = {2019},
  month = jan,
  abstract = {Joint models have received increasing attention during recent years with extensions into various directions; numerous hazard functions, different association structures, linear and non-linear longitudinal trajectories amongst others. Many of these resulted in new R packages and new formulations of the joint model. However, a joint model with a linear bivariate Gaussian association structure is still a latent Gaussian model (LGM) and thus can be implemented using most existing packages for LGM's. In this paper, we will show that these joint models can be implemented from a LGM viewpoint using the R-INLA package. As a particular example, we will focus on the joint model with a non-linear longitudinal trajectory, recently developed and termed the partially linear joint model. Instead of the usual spline approach, we argue for using a Bayesian smoothing spline framework for the joint model that is stable with respect to knot selection and hence less cumbersome for practitioners.},
  archivePrefix = {arXiv},
  eprint = {1901.09365},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\Z68RZUVU\\Van Niekerk et al. - 2019 - Joint models as latent Gaussian models - not reinv.pdf},
  journal = {arXiv:1901.09365 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@book{vanwagnerDevelopmentStructureCanadian1987,
  title = {Development and Structure of the {{Canadian Forest Fire Weather Index System}}},
  author = {Van Wagner, C. E.},
  year = {1987},
  publisher = {{Canada Communication Group Publ}},
  address = {{Ottawa}},
  annotation = {OCLC: 246355770},
  file = {C\:\\Users\\devan\\Zotero\\storage\\U99BM6HA\\Wagner - 1987 - Development and structure of the Canadian Forest F.pdf},
  isbn = {978-0-662-15198-2},
  language = {en},
  number = {35},
  series = {Forestry Technical Report}
}

@book{vanwagnerDevelopmentStructureCanadian1987a,
  title = {Development and Structure of the {{Canadian Forest Fire Weather Index System}}},
  author = {Van Wagner, C. E.},
  year = {1987},
  volume = {35},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\K83GAT3L\\Van Wagner - 1987 - Development and structure of the Canadian Forest F.pdf;C\:\\Users\\devan\\Zotero\\storage\\Z79N66CD\\publications.html},
  isbn = {978-0-662-15198-2},
  language = {English}
}

@article{vavasisComplexityNonnegativeMatrix2007,
  title = {On the Complexity of Nonnegative Matrix Factorization},
  author = {Vavasis, Stephen A.},
  year = {2007},
  month = sep,
  abstract = {Nonnegative matrix factorization (NMF) has become a prominent technique for the analysis of image databases, text databases and other information retrieval and clustering applications. In this report, we define an exact version of NMF. Then we establish several results about exact NMF: (1) that it is equivalent to a problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a polynomial-time local search heuristic exists.},
  archivePrefix = {arXiv},
  eprint = {0708.4149},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CCKM7UVE\\Vavasis - 2007 - On the complexity of nonnegative matrix factorizat.pdf;C\:\\Users\\devan\\Zotero\\storage\\NKNFGXTA\\0708.html},
  journal = {arXiv:0708.4149 [cs]},
  keywords = {Computer Science - Information Retrieval,G.1.3,H.3.3,Mathematics - Numerical Analysis},
  primaryClass = {cs}
}

@article{vazquezPatternsLightningPeopleCaused1998,
  title = {Patterns of {{Lightning}}-, and {{People}}-{{Caused Fires}} in {{Peninsular Spain}}},
  author = {Vazquez, A. and Moreno, J. M.},
  year = {1998},
  volume = {8},
  pages = {103--115},
  issn = {1448-5516},
  doi = {10.1071/wf9980103},
  abstract = {A comparative study of lightning-, and people-caused fires is presented for peninsular Spain, for the period 1974-1994. Based on records of fire reports, yearly trends for fires started by the two causes were compared. Fire reports assign each fire to one 10 \texttimes{} 10 km grid-cell within the country. This information, together with data on the cause and date of fire, elevation, size of fire, type of vegetation burned, and meteorological conditions at the time of fire initiation, was incorporated to a raster-based geographic information system for further analysis and mapping. Additional information incorporated to the GIS for each grid-cell was the phytogeographic sector to which it belonged and the main land-use types. The study shows that the number of fires has increased recently and, particularly, that of lightning fires. Annual fire occurrence of the two causes was significantly correlated. People-caused fires were widespread throughout most of the country, whereas lightning-caused fires, although also widely dispersed throughout Spain, were more clustered together in certain areas, mainly in the eastern part of the country and along certain mountain ranges. The difference between the geographic distribution of the fires started by the two causes was statistically significant. Additionally, lightning-caused fires occurred at upper elevations and were more clustered towards the summer than people-caused fires. Furthermore, in those grid-cells where fires of both causes occurred, lightning fires tended to occur at upper elevations, affected more woodlands, produced smaller maximum fire-sizes, and were started under different meteorological conditions than people-caused fi-es. Fire frequencies were small, and fire rotation periods high, in most phytogeographic sectors of the country for fires caused by lightning, not so for fires caused by people. In general, fire temporal-, and geographic-patterns, and fire characteristics of lightning-caused fires were different from those of people-caused fires.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\DNLAB3VT\\Vazquez and Moreno - 1998 - Patterns of Lightning-, and People-Caused Fires in.pdf;C\:\\Users\\devan\\Zotero\\storage\\QBH9BP2X\\WF9980103.html},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {2}
}

@article{vehtariPracticalBayesianModel2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  month = sep,
  volume = {27},
  pages = {1413--1432},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H86YTT43\\s11222-016-9696-4.pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {5}
}

@article{vehtariWAICCrossvalidationStan,
  title = {{{WAIC}} and Cross-Validation in {{Stan}}},
  author = {Vehtari, Aki and Gelman, Andrew},
  pages = {15},
  abstract = {The Watanabe-Akaike information criterion (WAIC) and cross-validation are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model. WAIC is based on the series expansion of leave-one-out cross-validation (LOO), and asymptotically they are equal. With finite data, WAIC and cross-validation address different predictive questions and thus it is useful to be able to compute both. WAIC and an importance-sampling approximated LOO can be estimated directly using the log-likelihood evaluated at the posterior simulations of the parameter values. We show how to compute WAIC, IS-LOO, K-fold cross-validation, and related diagnostic quantities in the Bayesian inference package Stan as called from R.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EU2C6DR3\\Vehtari and Gelman - WAIC and cross-validation in Stan.pdf},
  language = {en}
}

@article{Verdinelli1995,
  title = {Computing {{Bayes Factors Using}} a {{Generalization}} of the {{Savage}}-{{Dickey Density Ratio}}},
  author = {Verdinelli, Isabella and Wasserman, Larry},
  year = {1995},
  volume = {90},
  pages = {614--618},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {01621459},
  abstract = {We present a simple method for computing Bayes factors. The method derives from observing that in general, a Bayes factor can be written as the product of a quantity called the Savage-Dickey density ratio and a correction factor; both terms are easily estimated from posterior simulation. In some cases it is possible to do these computations without ever evaluating the likelihood.},
  journal = {Journal of the American Statistical Association},
  number = {430}
}

@article{verdoyEnhancingSPDEModeling2019,
  title = {Enhancing the {{SPDE}} Modeling of Spatial Point Processes with {{INLA}}, Applied to Wildfires. {{Choosing}} the Best Mesh for Each Database},
  author = {Verdoy, Pablo Juan},
  year = {2019},
  month = may,
  volume = {0},
  pages = {1--34},
  issn = {0361-0918},
  doi = {10.1080/03610918.2019.1618473},
  abstract = {Wildfires play an important role in shaping landscapes and as a source of CO2 and particulate matter, and are a typical spatial point process studied in many papers. Modeling the spatial variability of a wildfire could be performed in different ways and an important issue is the computational facilities that the new techniques afford us. The most common approaches have been through point pattern analysis or by Markov random fields. These methods have made it possible to build risk maps, but for many forest managers it is very useful to know the size of the fire as well as its location. In this work, we use Stochastic Partial Differential Equation (SPDE) with Integrated Nested Laplace Approximation (INLA) to model the size of the forest fires observed in the Valencian Community, Spain. But the most important element in this paper is the process that needs to be carried out prior to simulating and analyzing the different point patterns, namely, the choice of the most suitable mesh for the database. We describe and take advantage of the Bayesian methodology by including INLA and SPDE in the modeling process in all the scenarios.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9WL32KA2\\Verdoy - 2019 - Enhancing the SPDE modeling of spatial point proce.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZMTJAAJS\\Verdoy - 2019 - Enhancing the SPDE modeling of spatial point proce.pdf;C\:\\Users\\devan\\Zotero\\storage\\U34F6G4C\\03610918.2019.html},
  journal = {Communications in Statistics - Simulation and Computation},
  keywords = {Bayesian inference,INLA,mesh,spatial point process,SPDE},
  number = {0}
}

@article{vere-jonesModelsProceduresSpacetime2009,
  title = {Some Models and Procedures for Space-Time Point Processes},
  author = {{Vere-Jones}, David},
  year = {2009},
  month = jun,
  volume = {16},
  pages = {173--195},
  issn = {1352-8505, 1573-3009},
  doi = {10.1007/s10651-007-0086-0},
  abstract = {A common element in modelling forest fires and earthquakes is the need to develop space-time point process models that can be used to quantify the evolving risk from forest fires (or earthquakes) as a function of time, location, and background factors. This paper is intended as an introduction to space-time point process modelling. It includes brief summaries of the most relevant point process properties, starting from the description and estimation of first and second order moment properties, proceeding to a description of conditional intensity or dynamic models, and ending with an introduction to some of the models and estimation procedures which are currently being used in seismology. A short final section contrasts the modelling problems for seismology and for forest fires.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YQI5W7DY\\Vere-Jones - 2009 - Some models and procedures for space-time point pr.pdf},
  journal = {Environmental and Ecological Statistics},
  language = {en},
  number = {2}
}

@article{vere-jonesRescalingMarkedPoint2004,
  title = {Rescaling {{Marked Point Processes}}},
  author = {{Vere-Jones}, David and Schoenberg, Frederic Paik},
  year = {2004},
  month = mar,
  volume = {46},
  pages = {133--143},
  issn = {1369-1473, 1467-842X},
  doi = {10.1111/j.1467-842X.2004.00319.x},
  abstract = {In 1971 Meyer showed how to use the compensator to rescale a multivariate point process, forming independent Poisson processes with intensity 1. Meyer's result has been generalized to multi-dimensional point processes. This paper explores generalization of Meyer's theorem to the case of marked point processes, where the mark space may be quite general. Assuming simplicity and the existence of a conditional intensity, it shows that a marked point process can be transformed into a compound Poisson process with unit total rate and a fixed mark distribution.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\23MNGDJY\\Vere-Jones and Schoenberg - 2004 - Rescaling Marked Point Processes.pdf},
  journal = {Australian {$<$}html\_ent glyph="@amp;" ascii="\&amp;"/{$>$} New Zealand Journal of Statistics},
  language = {en},
  number = {1}
}

@article{viitasaariIntegralRepresentationRandom2016,
  title = {Integral Representation of Random Variables with Respect to {{Gaussian}} Processes},
  author = {Viitasaari, Lauri},
  year = {2016},
  month = feb,
  volume = {22},
  pages = {376--395},
  issn = {1350-7265},
  doi = {10.3150/14-BEJ662},
  abstract = {It was shown in Mishura et al. (Stochastic Process. Appl. 123 (2013) 2353-2369), that any random variable can be represented as improper pathwise integral with respect to fractional Brownian motion. In this paper, we extend this result to cover a wide class of Gaussian processes. In particular, we consider a wide class of processes that are H\textbackslash "\{o\}lder continuous of order \$\textbackslash alpha{$>$}1/2\$ and show that only local properties of the covariance function play role for such results.},
  archivePrefix = {arXiv},
  eprint = {1307.7559},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZR63TE67\\Viitasaari - 2016 - Integral representation of random variables with r.pdf},
  journal = {Bernoulli},
  keywords = {Mathematics - Probability},
  language = {en},
  number = {1}
}

@article{vilarModelPredictingHumancaused2010,
  title = {A Model for Predicting Human-Caused Wildfire Occurrence in the Region of {{Madrid}}, {{Spain}}},
  author = {Vilar, Lara and Woolford, Douglas. G. and Martell, David L. and Mart{\'i}n, M. Pilar},
  year = {2010},
  volume = {19},
  pages = {325},
  issn = {1049-8001},
  doi = {10.1071/WF09030},
  abstract = {This paper describes the development and validation of a spatio-temporal model for human-caused wildfire occurrence prediction at a regional scale. The study area is the 8028-km2 region of Madrid, located in central Spain, where more than 90\% of wildfires are caused by humans. We construct a logistic generalised additive model to estimate daily fire ignition risk at a 1-km2 grid spatial resolution. Spatially referenced socioeconomic and weather variables appear as covariates in the model. Spatial and temporal effects are also included. The variables in the model were selected using an iterative approach, which we describe. We use the model to predict the expected number of fires in our study area during the 2002\textendash 05 period, by aggregating the estimated probabilities over space\textendash time scales of interest. The estimated partial effects of the presence of railways, roads, and wildland\textendash urban interface in forest areas were highly significant, as were the observed daily maximum temperature and precipitation.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IAEGL5BZ\\Vilar et al. - 2010 - A model for predicting human-caused wildfire occur.pdf},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {3}
}

@article{vockEstimatingEffectPlate2018,
  title = {Estimating the Effect of Plate Discipline Using a Causal Inference Framework: An Application of the {{G}}-Computation Algorithm},
  shorttitle = {Estimating the Effect of Plate Discipline Using a Causal Inference Framework},
  author = {Vock, David Michael and Vock, Laura Frances Boehm},
  year = {2018},
  month = jun,
  volume = {14},
  pages = {37--56},
  issn = {1559-0410, 2194-6388},
  doi = {10.1515/jqas-2016-0029},
  abstract = {Offensive performance in baseball depends on a number of correlated factors: the pitches the batter faces, the batter's choice to swing, and the batter's hitting ability. Recently a renewed focus on the effect of plate discipline on batter performance has emerged. Plate discipline has traditionally been summarized as the proportion of pitches inside and outside of the strike zone a player swings at; however, there have been few metrics proposed to assess the effect of plate discipline directly on batters' outcomes. In this paper, we focus on estimating a batter's performance if he were able to adopt a different plate discipline. Because we wish to assess the effect of a counterfactual plate discipline, we use a potential outcome framework and show how the G-computation algorithm can be used to isolate the effect of plate discipline separately from a batter's hitting ability or the types of pitches the batter faces. As an example, we implement our approach using data collected with the PITCHf/x system over the 2012\textendash 2014 seasons to identify the improvement Starlin Castro would expect to see in offensive performance were he able to adopt Andrew McCutchen's plate discipline. We estimate that had Castro adopted McCutchen's discipline his batting average, on-base percentage, and slugging percentage would have increased 0.017 (se = 0.004), 0.040 (se = 0.006), and 0.028 (se = 0.008), respectively.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\49F6MUP9\\Vock and Vock - 2018 - Estimating the effect of plate discipline using a .pdf},
  journal = {Journal of Quantitative Analysis in Sports},
  language = {en},
  number = {2}
}

@article{vogtProbabilisticClusteringTimeEvolving2015,
  title = {Probabilistic {{Clustering}} of {{Time}}-{{Evolving Distance Data}}},
  author = {Vogt, Julia E. and Kloft, Marius and Stark, Stefan and Raman, Sudhir S. and Prabhakaran, Sandhya and Roth, Volker and R{\"a}tsch, Gunnar},
  year = {2015},
  month = apr,
  abstract = {We present a novel probabilistic clustering model for objects that are represented via pairwise distances and observed at different time points. The proposed method utilizes the information given by adjacent time points to find the underlying cluster structure and obtain a smooth cluster evolution. This approach allows the number of objects and clusters to differ at every time point, and no identification on the identities of the objects is needed. Further, the model does not require the number of clusters being specified in advance \textendash{} they are instead determined automatically using a Dirichlet process prior. We validate our model on synthetic data showing that the proposed method is more accurate than state-of-the-art clustering methods. Finally, we use our dynamic clustering model to analyze and illustrate the evolution of brain cancer patients over time.},
  archivePrefix = {arXiv},
  eprint = {1504.03701},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\C2SVL83R\\Vogt et al. - 2015 - Probabilistic Clustering of Time-Evolving Distance.pdf},
  journal = {arXiv:1504.03701 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{vollmanHockeyProspectusHowe2017,
  title = {Hockey {{Prospectus}} | {{Howe}} and {{Why}}: {{Ten Ways}} to {{Measure Defensive Contr}}\ldots},
  shorttitle = {Hockey {{Prospectus}} | {{Howe}} and {{Why}}},
  author = {Vollman, Robert},
  year = {2017},
  month = aug,
  file = {C\:\\Users\\devan\\Zotero\\storage\\YEW8MVVA\\ASqKa.html},
  howpublished = {http://archive.is/ASqKa},
  journal = {archive.is}
}

@article{volzViralPhylodynamics2013,
  title = {Viral {{Phylodynamics}}},
  author = {Volz, Erik M. and Koelle, Katia and Bedford, Trevor},
  editor = {Wodak, Shoshana},
  year = {2013},
  month = mar,
  volume = {9},
  pages = {e1002947},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002947},
  abstract = {Viral phylodynamics is defined as the study of how epidemiological, immunological, and evolutionary processes act and potentially interact to shape viral phylogenies. Since the coining of the term in 2004, research on viral phylodynamics has focused on transmission dynamics in an effort to shed light on how these dynamics impact viral genetic variation. Transmission dynamics can be considered at the level of cells within an infected host, individual hosts within a population, or entire populations of hosts. Many viruses, especially RNA viruses, rapidly accumulate genetic variation because of short generation times and high mutation rates. Patterns of viral genetic variation are therefore heavily influenced by how quickly transmission occurs and by which entities transmit to one another. Patterns of viral genetic variation will also be affected by selection acting on viral phenotypes. Although viruses can differ with respect to many phenotypes, phylodynamic studies have to date tended to focus on a limited number of viral phenotypes. These include virulence phenotypes, phenotypes associated with viral transmissibility, cell or tissue tropism phenotypes, and antigenic phenotypes that can facilitate escape from host immunity. Due to the impact that transmission dynamics and selection can have on viral genetic variation, viral phylogenies can therefore be used to investigate important epidemiological, immunological, and evolutionary processes, such as epidemic spread [2], spatio-temporal dynamics including metapopulation dynamics [3], zoonotic transmission, tissue tropism [4], and antigenic drift [5]. The quantitative investigation of these processes through the consideration of viral phylogenies is the central aim of viral phylodynamics.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YTQKT97W\\Volz et al. - 2013 - Viral Phylodynamics.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {3}
}

@article{voneshSharedParameterModels2006,
  title = {Shared Parameter Models for the Joint Analysis of Longitudinal Data and Event Times},
  author = {Vonesh, Edward F. and Greene, Tom and Schluchter, Mark D.},
  year = {2006},
  month = jan,
  volume = {25},
  pages = {143--163},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.2249},
  abstract = {Longitudinal studies often gather joint information on time to some event (survival analysis, time to dropout) and serial outcome measures (repeated measures, growth curves). Depending on the purpose of the study, one may wish to estimate and compare serial trends over time while accounting for possibly non-ignorable dropout or one may wish to investigate any associations that may exist between the event time of interest and various longitudinal trends. In this paper, we consider a class of randome ects models known as shared parameter models that are particularly useful for jointly analysing such data; namely repeated measurements and event time data. Speci\"yc attention will be given to the longitudinal setting where the primary goal is to estimate and compare serial trends over time while adjusting for possible informative censoring due to patient dropout. Parametric and semi-parametric survival models for event times together with generalized linear or non-linear mixed-e ects models for repeated measurements are proposed for jointly modelling serial outcome measures and event times. Methods of estimation are based on a generalized non-linear mixed-e ects model that may be easily implemented using existing software. This approach allows for exible modelling of both the distribution of event times and of the relationship of the longitudinal response variable to the event time of interest. The model and methods are illustrated using data from a multi-centre study of the e ects of diet and blood pressure control on progression of renal disease, the modi\"ycation of diet in renal disease study. Copyright ? 2005 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\ZFYP79TZ\\Vonesh et al. - 2006 - Shared parameter models for the joint analysis of .pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {1}
}

@article{waagepetersenEstimatingFunctionApproach2007,
  title = {An {{Estimating Function Approach}} to {{Inference}} for {{Inhomogeneous Neyman}}-{{Scott Processes}}},
  author = {Waagepetersen, Rasmus Plenge},
  year = {2007},
  month = mar,
  volume = {63},
  pages = {252--258},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2006.00667.x},
  abstract = {This paper is concerned with inference for a certain class of inhomogeneous Neyman-Scott point processes depending on spatial covariates. Regression parameter estimates obtained from a simple estimating function are shown to be asymptotically normal when the ``mother'' intensity for the Neyman-Scott process tends to infinity. Clustering parameter estimates are obtained using minimum contrast estimation based on the K-function. The approach is motivated and illustrated by applications to point pattern data from a tropical rain forest plot.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\263UJJ48\\Waagepetersen - 2007 - An Estimating Function Approach to Inference for I.pdf;C\:\\Users\\devan\\Zotero\\storage\\33KNX69M\\Waagepetersen - 2007 - An Estimating Function Approach to Inference for I.pdf;C\:\\Users\\devan\\Zotero\\storage\\CG6IFFLL\\Waagepetersen - 2007 - An Estimating Function Approach to Inference for I.pdf;C\:\\Users\\devan\\Zotero\\storage\\FWCFIMJY\\Waagepetersen - 2007 - An Estimating Function Approach to Inference for I.pdf;C\:\\Users\\devan\\Zotero\\storage\\QQ3XH7XM\\Waagepetersen - 2007 - An Estimating Function Approach to Inference for I.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{waagepetersenESTIMATINGFUNCTIONSINHOMOGENEOUS,
  title = {{{ESTIMATING FUNCTIONS FOR INHOMOGENEOUS COX PROCESSES}}},
  author = {Waagepetersen, Rasmus},
  pages = {9},
  abstract = {Estimation methods are reviewed for inhomogeneous Cox processes with tractable first and second order properties. We illustrate the various suggestions by means of data examples.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\F7ACE9ZZ\\Waagepetersen - ESTIMATING FUNCTIONS FOR INHOMOGENEOUS COX PROCESS.pdf;C\:\\Users\\devan\\Zotero\\storage\\KR58QQ5T\\Waagepetersen - ESTIMATING FUNCTIONS FOR INHOMOGENEOUS COX PROCESS.pdf;C\:\\Users\\devan\\Zotero\\storage\\RGC4N88M\\Waagepetersen - ESTIMATING FUNCTIONS FOR INHOMOGENEOUS COX PROCESS.pdf;C\:\\Users\\devan\\Zotero\\storage\\YFR6KK2M\\Waagepetersen - ESTIMATING FUNCTIONS FOR INHOMOGENEOUS COX PROCESS.pdf},
  language = {en}
}

@article{waagepetersenEstimatingFunctionsInhomogeneous2008,
  title = {Estimating {{Functions}} for {{Inhomogeneous Spatial Point Processes}} with {{Incomplete Covariate Data}}},
  author = {Waagepetersen, Rasmus},
  year = {2008},
  volume = {95},
  pages = {351--363},
  abstract = {The R package spat stat provides a very flexible and useful framework for analy point patterns. A fundamental feature is a procedure for fitting spatial point proc depending on covariates. However, in practice one often faces incomplete observati covariates and this leads to parameter estimation error which is difficult to quant paper, we introduce a Monte Carlo version of the estimating function used in fitting inhomogeneous Poisson processes and certain inhomogeneous cluster proces modified estimating function, it is feasible to obtain the asymptotic distribution of t estimators in the case of incomplete covariate information. This allows a study of efficiency due to the missing covariate data.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6WRVFWUI\\Waagepetersen - 2008 - Estimating Functions for Inhomogeneous Spatial Poi.pdf},
  journal = {Biometrika},
  language = {en},
  number = {2}
}

@article{waagepetersenTwoStepEstimationInhomogeneous2009,
  title = {Two-{{Step Estimation}} for {{Inhomogeneous Spatial Point Processes}}},
  author = {Waagepetersen, Rasmus and Guan, Yongtao},
  year = {2009},
  volume = {71},
  pages = {685--702},
  abstract = {The paper is concerned with parameter estimation for inhomogeneous spatial point processes with a regression model for the intensity function and tractable second-order properties (K-function). Regression parameters are estimated by using a Poisson likelihood score estimating function and in the second step minimum contrast estimation is applied for the residual clustering parameters. Asymptotic normality of parameter estimates is established under certain mixing conditions and we exemplify how the results may be applied in ecological studies of rainforests.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CKYVPIQE\\Waagepetersen and Guan - 2009 - Two-Step Estimation for Inhomogeneous Spatial Poin.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  language = {en},
  number = {3}
}

@article{Wagenmakers2010,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}\textendash{{Dickey}} Method},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  year = {2010},
  volume = {60},
  pages = {158--189},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2009.12.001},
  journal = {Cognitive Psychology},
  keywords = {Bayes factor,Hierarchical modeling,Model selection,Order-restrictions,Random effects,Statistical evidence},
  number = {3}
}

@article{wagenmakers2010,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}\textendash{{Dickey}} Method},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  year = {2010},
  volume = {60},
  pages = {158--189},
  doi = {10.1016/j.cogpsych.2009.12.001},
  annotation = {Exported from https://app.dimensions.ai on 2018/09/18},
  keywords = {Statistics and Psychology},
  number = {3}
}

@misc{wagnerWhyQualityCompetition2018,
  title = {Why {{Quality}} of {{Competition}} Doesn't Matter to Analytics Experts Anymore},
  author = {Wagner, Daniel},
  year = {2018},
  abstract = {It's every coach's nightmare. In the middle of a game, your line-matching goes awry and your fourth-line forwards and third-pairing defencemen wind up on the ice at the same time as the other team's first line and top pairing.  Suddenly your grinders and pylons are staring down a rush from Sidney Crosby, Nathan Mackinnon, and Connor McDavid, with Brent Burns and Erik Karlsson jumping up in the play. One of your forwards gets turnstiled, a defenceman falls flat on his keister trying to keep up with a quick pivot, and a couple passes later your poor goaltender is turned into a pretzel with the puck in the back of your net.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SU2ZC92Y\\why-quality-of-competition-doesn-t-matter-to-analytics-experts-anymore-1.html},
  howpublished = {https://www.vancourier.com/pass-it-to-bulis/why-quality-of-competition-doesn-t-matter-to-analytics-experts-anymore-1.23414544?utm\_source=dlvr.it\&utm\_medium=twitter},
  journal = {Vancouver Courier}
}

@article{wangCffdrsPackageCanadian2017,
  title = {Cffdrs: An {{R}} Package for the {{Canadian Forest Fire Danger Rating System}}.},
  shorttitle = {Cffdrs},
  author = {Wang, X. and Wotton, B. M. and Cantin, A. and Parisien, M.-A. and Anderson, K. R. and Moore, B. and Flannigan, M. D.},
  year = {2017},
  volume = {6},
  doi = {10.1186/s13717-017-0070-z},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\XLUCUK3P\\Wang et al. - 2017 - cffdrs an R package for the Canadian Forest Fire .pdf;C\:\\Users\\devan\\Zotero\\storage\\RFN93AYH\\publications.html},
  language = {English},
  number = {5}
}

@article{wangGeneralizedCommonSpatial2003,
  title = {Generalized Common Spatial Factor Model},
  author = {Wang, F.},
  year = {2003},
  month = oct,
  volume = {4},
  pages = {569--582},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/4.4.569},
  abstract = {There are often two types of correlations in multivariate spatial data: correlations between variables measured at the same locations, and correlations of each variable across the locations. We hypothesize that these two types of correlations are caused by a common spatially correlated underlying factor. Under this hypothesis, we propose a generalized common spatial factor model. The parameters are estimated using the Bayesian method and a Markov chain Monte Carlo computing technique. Our main goals are to determine which observed variables share a common underlying spatial factor and also to predict the common spatial factor. The model is applied to county-level cancer mortality data in Minnesota to find whether there exists a common spatial factor underlying the cancer mortality throughout the state.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BSNQMJWC\\Wang - 2003 - Generalized common spatial factor model.pdf},
  journal = {Biostatistics},
  language = {en},
  number = {4}
}

@article{wangIncorporatingParameterUncertainty2003,
  title = {Incorporating Parameter Uncertainty into Prediction Intervals for Spatial Data Modeled via a Parametric Variogram},
  author = {Wang, Fujun and Wall, Melanie M.},
  year = {2003},
  month = sep,
  volume = {8},
  pages = {296--309},
  issn = {1085-7117, 1537-2693},
  doi = {10.1198/1085711031661},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IFNCABLA\\Wang and Wall - 2003 - Incorporating parameter uncertainty into predictio.pdf},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  language = {en},
  number = {3}
}

@article{wangJointModelingBinary,
  title = {Joint {{Modeling}} of {{Binary Response}} and {{Survival Data}} in {{Clinical Trials}}},
  author = {Wang, Jia},
  pages = {76},
  file = {C\:\\Users\\devan\\Zotero\\storage\\B3RHSK48\\Wang - Joint Modeling of Binary Response and Survival Dat.pdf},
  language = {en}
}

@article{wangModelingHeapingSelf2008,
  title = {Modeling Heaping in Self-reported Cigarette Counts},
  author = {Wang, Hao and Heitjan, Daniel F.},
  year = {2008},
  month = aug,
  volume = {27},
  pages = {3789--3804},
  issn = {02776715, 10970258},
  doi = {10.1002/sim.3281},
  abstract = {In studies of smoking behavior, some subjects report exact cigarette counts, whereas others report roundedoff counts, particularly multiples of 20, 10 or 5. This form of data reporting error, known as heaping, can bias the estimation of parameters of interest such as mean cigarette consumption. We present a model to describe heaped count data from a randomized trial of bupropion treatment for smoking cessation. The model posits that the reported cigarette count is a deterministic function of an underlying precise cigarette count variable and a heaping behavior variable, both of which are at best partially observed. To account for an excess of zeros, as would likely occur in a smoking cessation study where some subjects successfully quit, we model the underlying count variable with zero-inflated count distributions. We study the sensitivity of the inference on smoking cessation by fitting various models that either do or do not account for heaping and zero inflation, comparing the models by means of Bayes factors. Our results suggest that sufficiently rich models for both the underlying distribution and the heaping behavior are indispensable to obtaining a good fit with heaped smoking data. The analyses moreover reveal that bupropion has a significant effect on the fraction abstinent, but not on mean cigarette consumption among the non-abstinent. Copyright q 2008 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4TS36UTV\\Wang and Heitjan - 2008 - Modeling heaping in self‐reported cigarette counts.pdf;C\:\\Users\\devan\\Zotero\\storage\\7C3QFBQZ\\Wang and Heitjan - 2008 - Modeling heaping in self‐reported cigarette counts.pdf;C\:\\Users\\devan\\Zotero\\storage\\L6C4T6CG\\Wang and Heitjan - 2008 - Modeling heaping in self‐reported cigarette counts.pdf;C\:\\Users\\devan\\Zotero\\storage\\S4EKZ9KH\\Wang and Heitjan - 2008 - Modeling heaping in self‐reported cigarette counts.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {19}
}

@article{wangNonnegativeMatrixFactorization2013,
  title = {Nonnegative {{Matrix Factorization}}: {{A Comprehensive Review}}},
  shorttitle = {Nonnegative {{Matrix Factorization}}},
  author = {Wang, Yu-Xiong and Zhang, Yu-Jin},
  year = {2013},
  month = jun,
  volume = {25},
  pages = {1336--1353},
  issn = {2326-3865},
  doi = {10.1109/TKDE.2012.51},
  abstract = {Nonnegative Matrix Factorization (NMF), a relatively novel paradigm for dimensionality reduction, has been in the ascendant since its inception. It incorporates the nonnegativity constraint and thus obtains the parts-based representation as well as enhancing the interpretability of the issue correspondingly. This survey paper mainly focuses on the theoretical research into NMF over the last 5 years, where the principles, basic models, properties, and algorithms of NMF along with its various modifications, extensions, and generalizations are summarized systematically. The existing NMF algorithms are divided into four categories: Basic NMF (BNMF), Constrained NMF (CNMF), Structured NMF (SNMF), and Generalized NMF (GNMF), upon which the design principles, characteristics, problems, relationships, and evolution of these algorithms are presented and analyzed comprehensively. Some related work not on NMF that NMF should learn from or has connections with is involved too. Moreover, some open issues remained to be solved are discussed. Several relevant application areas of NMF are also briefly described. This survey aims to construct an integrated, state-of-the-art framework for NMF concept, from which the follow-up research may benefit.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9I82AR5R\\Wang and Zhang - 2013 - Nonnegative Matrix Factorization A Comprehensive .pdf;C\:\\Users\\devan\\Zotero\\storage\\CYNS5CRN\\6165290.html},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {Algorithm design and analysis,basic NMF,BNMF,CNMF,constrained NMF,Data analysis,Data mining,dimensionality reduction,generalized NMF,GNMF,matrix decomposition,Matrix decomposition,multivariate data analysis,nonnegative matrix factorization,nonnegative matrix factorization (NMF),nonnegativity constraint,Optimization,parts-based representation,Semantics,Signal processing algorithms,SNMF,structured NMF,Vectors},
  number = {6}
}

@article{watanabeAsymptoticEquivalenceBayes2010,
  title = {Asymptotic {{Equivalence}} of {{Bayes Cross Validation}} and {{Widely Applicable Information Criterion}} in {{Singular Learning Theory}}},
  author = {Watanabe, Sumio},
  year = {2010},
  month = dec,
  volume = {11},
  pages = {3571--3594},
  issn = {1532-4435},
  abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2{$\lambda$}/n, where {$\lambda$} is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AK6G3DS3\\Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf},
  journal = {J. Mach. Learn. Res.}
}

@article{watanabeEquationsStatesSingular2010,
  title = {Equations of {{States}} in {{Singular Statistical Estimation}}},
  author = {Watanabe, Sumio},
  year = {2010},
  month = jan,
  volume = {23},
  pages = {20--34},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.08.002},
  abstract = {Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VEH6ZXIC\\Watanabe - 2010 - Equations of States in Singular Statistical Estima.pdf},
  journal = {Neural Netw.},
  number = {1}
}

@article{watson1986univariate,
  title = {Univariate Detrending Methods with Stochastic Trends},
  author = {Watson, Mark W},
  year = {1986},
  volume = {18},
  pages = {49--75},
  publisher = {{Elsevier}},
  journal = {Journal of monetary economics},
  number = {1}
}

@article{watsonEstimatingAnimalUtilization2020,
  title = {Estimating Animal Utilization Distributions from Multiple Data Types: A Joint Spatio-Temporal Point Process Framework},
  shorttitle = {Estimating Animal Utilization Distributions from Multiple Data Types},
  author = {Watson, Joe and Joy, Ruth and Tollit, Dominic and Thornton, Sheila J. and {Auger-M{\'e}th{\'e}}, Marie},
  year = {2020},
  month = sep,
  abstract = {Models of the spatial distribution of animals provide useful tools to help ecologists quantify species-environment relationships, and they are increasingly being used to help determine the impacts of climate and habitat changes on species. While high-quality survey-style data with known effort are sometimes available, often researchers have multiple datasets of varying quality and type. In particular, collections of sightings made by citizen scientists are becoming increasingly common, with no information typically provided on their observer effort. Many standard modelling approaches ignore observer effort completely, which can severely bias estimates of an animal's distribution. Combining sightings data from observers who followed different protocols is challenging. Any differences in observer skill, spatial effort, and the detectability of the animals across space all need to be accounted for. To achieve this, we build upon the recent advancements made in integrative species distribution models and present a novel marked spatio-temporal point process framework for estimating the utilization distribution (UD) of the individuals of a highly mobile species. We show that in certain settings, we can also use the framework to combine the UDs from the sampled individuals to estimate the species' distribution. We combine the empirical results from a simulation study with the implications outlined in a causal directed acyclic graph to identify the necessary assumptions required for our framework to control for observer effort when it is unknown. We then apply our framework to combine multiple datasets collected on the endangered Southern Resident Killer Whales, to estimate their monthly effort-corrected space-use.},
  archivePrefix = {arXiv},
  eprint = {1911.00151},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BT4CHIKR\\Watson et al. - 2020 - Estimating animal utilization distributions from m.pdf},
  journal = {arXiv:1911.00151 [stat]},
  keywords = {Statistics - Applications,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{watsonUnivariateDetrendingMethods1986,
  title = {Univariate Detrending Methods with Stochastic Trends},
  author = {Watson, Mark W.},
  year = {1986},
  month = jul,
  volume = {18},
  pages = {49--75},
  issn = {03043932},
  doi = {10.1016/0304-3932(86)90054-1},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4BK6CJWS\\Watson - 1986 - Univariate detrending methods with stochastic tren.pdf;C\:\\Users\\devan\\Zotero\\storage\\FZ5AIFIW\\Watson - 1986 - Univariate detrending methods with stochastic tren.pdf},
  journal = {Journal of Monetary Economics},
  language = {en},
  number = {1}
}

@misc{weissbockMachineLearningHockey2019,
  title = {Machine {{Learning}} and {{Hockey}}: {{Is}} There a Theoretical Limit on Predicti\ldots},
  shorttitle = {Machine {{Learning}} and {{Hockey}}},
  author = {Weissbock, Josh},
  year = {2019},
  month = apr,
  file = {C\:\\Users\\devan\\Zotero\\storage\\X45YISMT\\B3vgj.html},
  howpublished = {http://archive.is/B3vgj},
  journal = {NHL Numbers}
}

@misc{wendorfAnotherShotQuality2015,
  title = {Another {{Shot Quality Quandary}}: {{League Variance}}, {{Evolution}}, {{Error}}},
  shorttitle = {Another {{Shot Quality Quandary}}},
  author = {Wendorf, Benjamin},
  year = {2015},
  month = may,
  abstract = {Hockey statistical analysis isn't really capturing all of hockey, or seeking to package it; it's about getting as close we can to the essence of the thing. All the ideas, conclusions, b\ldots},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YU47FCGW\\nhl-shot-quality-league-variance-error-shot-distance.html},
  journal = {Hockey Graphs},
  language = {en}
}

@article{westcottProbabilityGeneratingFunctional1972,
  title = {The Probability Generating Functional},
  author = {Westcott, M.},
  year = {1972},
  month = dec,
  volume = {14},
  pages = {448--466},
  issn = {0004-9735},
  doi = {10.1017/S1446788700011095},
  abstract = {This paper is concerned with certain aspects of the theory and application of the probability generating functional (p.g.fl) of a point process on the real line. Interest in point processes has increased rapidly during the last decade and a number of different approaches to the subject have been expounded (see for example [6], [11], [15], [17], [20], [25], [27], [28]). It is hoped that the present development using the p.g.ff will calrify and unite some of these viewpoints and provide a useful tool for solution of particular problems.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\TULVICZB\\Westcott - 1972 - The probability generating functional.pdf},
  journal = {Journal of the Australian Mathematical Society},
  language = {en},
  number = {4}
}

@article{westPracticalGuideUsing2007,
  title = {A {{Practical Guide Using Statistical Software}}},
  author = {West, Brady T and Welch, Kathleen B and Ga, Andrzej T},
  year = {2007},
  pages = {348},
  file = {C\:\\Users\\devan\\Zotero\\storage\\2F6NL83U\\West et al. - 2007 - A Practical Guide Using Statistical Software.pdf;C\:\\Users\\devan\\Zotero\\storage\\8IHPFI96\\West et al. - 2007 - A Practical Guide Using Statistical Software.pdf},
  language = {en}
}

@article{whittleFittingMultivariateAutoregressions1963,
  title = {On the Fitting of Multivariate Autoregressions, and the Approximate Canonical Factorization of a Spectral Density Matrix},
  author = {Whittle, P.},
  year = {1963},
  month = jun,
  volume = {50},
  pages = {129--134},
  issn = {0006-3444},
  doi = {10.1093/biomet/50.1-2.129},
  abstract = {Abstract.  The recursive method proposed by Durbin (1960) for the fitting of autoregreseive schemes of successively increasing order is generalized to the fitti},
  file = {C\:\\Users\\devan\\Zotero\\storage\\6MZXVHUL\\Whittle - 1963 - On the fitting of multivariate autoregressions, an.pdf;C\:\\Users\\devan\\Zotero\\storage\\8JG83ZKN\\285713.html},
  journal = {Biometrika},
  language = {en},
  number = {1-2}
}

@article{whittleStationaryProcessesPlane1954,
  title = {On {{Stationary Processes}} in the {{Plane}}},
  author = {Whittle, P.},
  year = {1954},
  volume = {41},
  pages = {434--449},
  issn = {0006-3444},
  doi = {10.2307/2332724},
  abstract = {The sampling theory of stationary processes in space is not completely analogous to that of stationary time series, due to the fact that the variate of a time series is influenced only by past values, while for a spatial process dependence extends in all directions. This point is elaborated in \textsection\textsection{} 2-4. The estimation and test theory developed in \textsection{} 7 is applied in \textsection{} 8 to uniformity data for wheat and oranges. The final section is devoted to an examination of some particular two-dimensional processes.},
  journal = {Biometrika},
  number = {3/4}
}

@article{Wickham,
  title = {Dplyr: {{A Grammar}} of {{Data Manipulation}}},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill},
  year = {2018},
  journal = {R package version 1.0.2}
}

@book{Wickham2016,
  title = {\{g\}gplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@article{Wickham2018,
  title = {Tidyr: {{Easily Tidy Data}} with 'spread()' and 'Gather()' {{Functions}}},
  author = {Wickham, Hadley and Henry, Lionel},
  year = {2018},
  journal = {R package version 1.0.2}
}

@techreport{wildfiremanagementbranchWildfireManagementBranch2017,
  title = {Wildfire {{Management Branch Strategic Plan}} 2012-2017},
  author = {{Wildfire Management Branch}},
  year = {2017},
  institution = {{BC Ministry of Forests, Lands and Natural Resource Operations}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\W2KD94TS\\Wildfire Management Branch Strategic Plan 2012-201.pdf},
  language = {en}
}

@misc{Wilke2018,
  title = {Cowplot: {{Streamlined Plot Theme}} and {{Plot Annotations}} for 'Ggplot2'},
  author = {Wilke, Claus O.},
  year = {2018}
}

@misc{wilsonLimitsObservationNHLNumbers2017,
  title = {The {{Limits}} of {{Observation}} | {{NHLNumbers}}.Com},
  author = {Wilson, Kent},
  year = {2017},
  month = aug,
  howpublished = {http://archive.is/qN4DM},
  journal = {NHL Numbers}
}

@article{woodallCurrentResearchProfile2007,
  title = {Current Research on Profile Monitoring},
  author = {Woodall, William H.},
  year = {2007},
  month = dec,
  volume = {17},
  pages = {420--425},
  issn = {0103-6513},
  doi = {10.1590/S0103-65132007000300002},
  abstract = {In many applications the quality of a process or product is best characterized and summarized by a functional relationship between a response variable and one or more explanatory variables. Profile monitoring is used to understand and to check the stability of this relationship over time. At each sampling stage one observes a collection of data points that can be represented by a curve (or profile). In some calibration applications, the profile can be represented adequately by a simple linear regression model, while in other applications more complicated models are needed. The purposes of this paper are to review recent research on the use of control charts to monitor process and product quality profiles and to encourage further research in this area.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\PSA8MFLL\\Woodall - 2007 - Current research on profile monitoring.pdf},
  journal = {Production},
  language = {en},
  number = {3}
}

@book{woodGeneralizedAdditiveModels2017,
  title = {Generalized {{Additive Models}} : {{An Introduction}} with {{R}}, {{Second Edition}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Wood, Simon N.},
  year = {2017},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315370279},
  abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CR7Y824K\\Wood - 2017 - Generalized Additive Models  An Introduction with.pdf;C\:\\Users\\devan\\Zotero\\storage\\IK8CGGTB\\9781315370279.html},
  isbn = {978-1-315-37027-9},
  language = {en}
}

@article{wooForestFireRisk2017,
  title = {Forest Fire Risk Assessment Using Point Process Modelling of Fire Occurrence and {{Monte Carlo}} Fire Simulation},
  author = {Woo, Hyeyoung and Chung, Woodam and Graham, Jonathan M. and Lee, Byungdoo},
  year = {2017},
  month = oct,
  volume = {26},
  pages = {789--805},
  issn = {1448-5516},
  doi = {10.1071/WF17021},
  abstract = {Risk assessment of forest fires requires an integrated estimation of fire occurrence probability and burn probability because fire spread is largely influenced by ignition locations as well as fuels, weather, topography and other environmental factors. This study aims to assess forest fire risk over a large forested landscape using both fire occurrence and burn probabilities. First, we use a spatial point processing method to generate a fire occurrence probability surface. We then perform a Monte Carlo fire spread simulation using multiple fire ignition points generated from the fire occurrence surface to compute burn probability across the landscape. Potential loss per land parcel due to forest fire is assessed as the combination of burn probability and government-appraised property values. We applied our methodology to the municipal boundary of Gyeongju in the Republic of Korea. The results show that the density of fire occurrence is positively associated with low elevation, moderate slope, coniferous land cover, distance to roads, high density of tombs and interaction among fire ignition locations. A correlation analysis among fire occurrence probability, burn probability, land property value and potential value loss indicates that fire risk in the study landscape is largely associated with the spatial pattern of burn probability.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UVF7M4QZ\\Woo - Forest Fire Risk Assessment Using Point Process Mo.pdf;C\:\\Users\\devan\\Zotero\\storage\\VPT3P3E4\\wf17021.html},
  journal = {International Journal of Wildland Fire},
  language = {en},
  number = {9}
}

@article{woolfordCharacterizingTemporalChanges2010,
  title = {Characterizing Temporal Changes in Forest Fire Ignitions: Looking for Climate Change Signals in a Region of the {{Canadian}} Boreal Forest},
  shorttitle = {Characterizing Temporal Changes in Forest Fire Ignitions},
  author = {Woolford, Douglas G. and Cao, Jiguo and Dean, Charmaine B. and Martell, David L.},
  year = {2010},
  volume = {21},
  pages = {789--800},
  issn = {1099-095X},
  doi = {10.1002/env.1067},
  abstract = {The potential impact of climate change on forest fire risk is of significant concern. Postulated climate change effects on wildfires include increasing annual trends in ignitions and a lengthening of the fire season. We propose to use logistic generalized additive mixed models to investigate these characteristics. We present the modelling framework and outline a set of candidate models that are nested in terms of their fixed effects components. Model selection via likelihood ratio testing is discussed and connected to an entropy-based scoring rule for Bernoulli responses. We illustrate its application using data for lightning-caused forest fire ignitions over a period of 42 years in a 9 884 943 hectare region of boreal forest of northwestern Ontario, Canada. Seasonal and annual changes in ignition risk are observed and discussed, but we identify significant outstanding confounding factors that need to be addressed before one can assess the extent to which those changes can or cannot be attributed to climate change. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\T2CSTWWM\\Woolford et al. - 2010 - Characterizing temporal changes in forest fire ign.pdf;C\:\\Users\\devan\\Zotero\\storage\\MY2FDS73\\env.html},
  journal = {Environmetrics},
  keywords = {entropy score,generalized additive mixed model,nonparametric smoothing,wildfire ignitions},
  language = {en},
  number = {7-8}
}

@article{woolfordLightningcausedForestFire2014,
  title = {Lightning-Caused Forest Fire Risk in {{Northwestern Ontario}}, {{Canada}}, Is Increasing and Associated with Anomalies in Fire Weather},
  author = {Woolford, Douglas G. and Dean, C. B. and Martell, David L. and Cao, Jiguo and Wotton, B. M.},
  year = {2014},
  volume = {25},
  pages = {406--416},
  issn = {1099-095X},
  doi = {10.1002/env.2278},
  abstract = {Results from studies of climate model scenarios suggest that forest fire ignitions will increase in Canada in the future because of climate change. Yet, there have been few studies that monitor long-term trends in Canadian historical fire records. Although there are seasonal trends to historically reported fires within a fire season, there are also periods of zero-heavy behaviour as well as periods during which more fires are reported than usual. We develop a flexible mixture-modelling framework that permits the joint assessment of temporal trends in these dominant characteristics in terms of fire risk, defined as the daily probability that one or more fires are reported. The statistical power of such trend tests are also evaluated. We identify statistically significant increases in lightning-caused fire risk between 1963 and 2009 in the boreal forest regions of the Rainy River and Lake of the Woods ecoregions in Northwestern Ontario, Canada. These observed changes in lightning-caused fire risk were found to be associated with temperature and fire danger rating index anomalies. If such trends continue into the future, the duration of elevated periods of lightning-caused forest fire risk is forecasted to increase by over 50\% by the middle of this century. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JQCQDVY9\\Woolford et al. - 2014 - Lightning-caused forest fire risk in Northwestern .pdf;C\:\\Users\\devan\\Zotero\\storage\\EHHMUHI4\\env.html},
  journal = {Environmetrics},
  keywords = {climate change,ignition,logistic GAM,power,wildfire,wildland fire,zero heavy},
  language = {en},
  number = {6}
}

@article{woolfordSitespecificSeasonalBaselines2009,
  title = {Site-Specific Seasonal Baselines for Fire Risk in {{Ontario}}},
  author = {Woolford, D G and Braun, W J},
  year = {2009},
  volume = {63},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YKCYVA35\\Woolford and Braun - SITE-SPECIFIC SEASONAL BASELINES FOR FIRE RISK IN .pdf},
  journal = {Geomatica},
  language = {en},
  number = {4}
}

@article{woolfordSpatiotemporalModelPeopleCaused2011,
  title = {A {{Spatio}}-Temporal {{Model}} for {{People}}-{{Caused Forest Fire Occurrence}} in the {{Romeo Malette Forest}}},
  author = {Woolford, Douglas G and Bellhouse, David R and Braun, W John and Dean, Charmaine B and Martell, David L and Sun, Jessica},
  year = {2011},
  volume = {2},
  pages = {26},
  abstract = {This paper describes the development and an assessment of a spatio-temporal model for people-caused forest fires in a portion of boreal forest in northeastern Ontario, a central province in Canada. Space and time along with location-specific weather-based fire danger rating indices and anthropogenic effects are included in the modelling we present, which parallels the structure of recent methodology for assessing fire risk using logistic generalized additive models (GAMs) introduced in Brillinger et al. (Institute of Mathematical Statistics Lecture Notes, 2003). In these models, the data consist of observations on a very fine set of space-time cells, where fires are rare and the complete data set is too large to analyze. Consequently, the non-fire observations are sampled. This induces an offset in the additive structure, which we connect to the analysis of case-control studies. The model's fit and estimated partial effects are shown to be sensitive to large reductions in this inclusion probability. We also make comparisons between a model with an additive decomposition of spatial and temporal effects to one with a spatio-temporal interaction, and we investigate the impact of restricting fire-weather and anthropogenic effects to be linear. Our results suggest that, when using logistic GAMs to model our wildland fire occurrence data on this scale, there is no advantage to including space and time interaction effects, and that models with linear terms, which have dominated the fire risk literature, are inadequate.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\AMY52UPF\\Woolford et al. - A Spatio-temporal Model for People-Caused Forest F.pdf},
  journal = {Journal of Environmental Statistics},
  language = {en},
  number = {1}
}

@misc{WorkshopEvolvingMarked2016,
  title = {Workshop on {{Evolving Marked Point Processes}} with {{Application}} to {{Wildland Fire Regime Modeling}}},
  year = {2016},
  month = jan,
  abstract = {High-profile wildland urban interface wildfires in Canada have heightened awareness and concern surrounding wildland fires on populated landscapes. For example, the 2003 Kelowna and McLure fires in BC led to approximately \textbackslash\$200 million in insurance claims and \textbackslash\$400 million in suppression costs, while the 2011 fire in the town of Slave Lake, AB led to approximately \textbackslash\$700 million in insurance claims. The economic and social impacts of wildfire in Canada raise important statistical questions.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\YR28LE7C\\marked-point.html},
  howpublished = {http://www.fields.utoronto.ca/activities/15-16/marked-point},
  journal = {Fields Institute for Research in Mathematical Sciences},
  language = {en}
}

@book{workshopontheuseofspeciesdistributionmodelinginthedeepseaUseSpeciesDistribution2019,
  title = {Use of Species Distribution Modeling in the Deep Sea},
  author = {{Workshop on the Use of Species Distribution Modeling in the Deep Sea} and Kenchington, E and {Canada} and {Department of Fisheries and Oceans} and {Maritimes Region}},
  year = {2019},
  abstract = {"In the last two decades the use of species distribution modeling (SDM) for the study and management of marine species has increased dramatically. The availability of predictor variables on a global scale and the ease of use of SDM techniques have resulted in a proliferation of research on the topic of species distribution in the deep sea. Translation of research projects into management tools that can be used to make decisions in the face of changing climate and increasing exploitation of deep-sea resources has been less rapid but necessary. The goal of this workshop was to discuss methods and variables for modeling species distributions in deep-sea habitats and produce standards that can be used to judge SDMs that may be useful to meet management and conservation goals"--Abstract, page vii.},
  annotation = {OCLC: 1101650208},
  file = {C\:\\Users\\devan\\Zotero\\storage\\9DIZII65\\Workshop on the Use of Species Distribution Modeling in the Deep Sea et al. - 2019 - Use of species distribution modeling in the deep s.pdf},
  isbn = {978-0-660-29721-7},
  language = {en}
}

@article{worobeyEmergenceSARSCoV2Europe2020,
  title = {The Emergence of {{SARS}}-{{CoV}}-2 in {{Europe}} and {{North America}}},
  author = {Worobey, Michael and Pekar, Jonathan and Larsen, Brendan B. and Nelson, Martha I. and Hill, Verity and Joy, Jeffrey B. and Rambaut, Andrew and Suchard, Marc A. and Wertheim, Joel O. and Lemey, Philippe},
  year = {2020},
  month = sep,
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abc8169},
  abstract = {Accurate understanding of the global spread of emerging viruses is critically important for public health responses and for anticipating and preventing future outbreaks. Here, we elucidate when, where and how the earliest sustained SARS-CoV-2 transmission networks became established in Europe and North America. Our results suggest that rapid early interventions successfully prevented early introductions of the virus into Germany and the US from taking hold. Other, later introductions of the virus from China to both Italy and to Washington State founded the earliest sustained European and North America transmission networks. Our analyses demonstrate the effectiveness of public health measures in preventing onward transmission and show that intensive testing and contact tracing could have prevented SARS-CoV-2 from becoming established.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).. https://creativecommons.org/licenses/by/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\UKYYP5CH\\Worobey et al. - 2020 - The emergence of SARS-CoV-2 in Europe and North Am.pdf;C\:\\Users\\devan\\Zotero\\storage\\AU3BFQAD\\science.abc8169.html},
  journal = {Science},
  language = {en},
  pmid = {32912998}
}

@article{wottonInterpretingUsingOutputs2009,
  title = {Interpreting and Using Outputs from the {{Canadian Forest Fire Danger Rating System}} in Research Applications},
  author = {Wotton, Mike},
  year = {2009},
  month = jun,
  volume = {16},
  pages = {107--131},
  doi = {10.1007/s10651-007-0084-2},
  abstract = {Understanding and being able to predict forest fire occurrence, fire growth and fire intensity are important aspects of forest fire management. In Canada fire management agencies use the Canadian Forest Fire Danger Rating System (CFFDRS) to help predict these elements of forest fire activity. In this paper a review of the CFFDRS is presented with the main focus on understanding and interpreting Canadian Fire Weather Index (FWI) System outputs. The need to interpret the outputs of the FWI System with consideration to regional differences is emphasized and examples are shown of how the relationship between actual fuel moisture and the FWI System's moisture codes vary from region to region. Examples are then shown of the relationship between fuel moisture and fire occurrence for both human- and lightning-caused fire for regions with different forest composition. The relationship between rate of spread, fuel consumption and the relative fire behaviour indices of the FWI System for different forest types is also discussed. The outputs of the CFFDRS are used every day across Canada by fire managers in every district, regional and provincial fire management office. The purpose of this review is to provide modellers with an understanding of this system and how its outputs can be interpreted. It is hoped that this review will expose statistical modellers and other researchers to some of the models used currently in forest fire management and encourage further research and development of models useful for understanding and managing forest fire activity.},
  journal = {Environmental and Ecological Statistics}
}

@article{wottonLightningFireOccurrence2005,
  title = {A Lightning Fire Occurrence Model for {{Ontario}}},
  author = {Wotton, B M and Martell, David L},
  year = {2005},
  month = jun,
  volume = {35},
  pages = {1389--1401},
  issn = {0045-5067, 1208-6037},
  doi = {10.1139/x05-071},
  abstract = {Lightning strike, fire weather, and fire occurrence data were used to model (i) the probability that a lightning strike causes a sustainable ignition on the forest floor and (ii) the probability of an ignition being detected and reported to the fire management agency for each ecoregion in the province of Ontario. An index that tracks duff moisture content in very sheltered areas of a forest stand (near the tree boles) was the most significant predictor in each ignition model. The presence of positive cloud-to-ground lightning strikes was also found to have a significant and positive influence on the probability of ignition in most areas of the province with the exception of the far northwest. Weather conditions following a lightning storm influence the probability that a lightning strike causes a sustainable ignition. Models of the probability of detecting a fire ignited by lightning were also created for each of the ecoregions across Ontario. The form of these models varied somewhat among ecoregions, but contained an indicator of receptive surface fire spread conditions and an indicator of the dryness of the heavier fuels (the organic layer) in the forest floor.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IWY9N3C3\\Wotton and Martell - 2005 - A lightning fire occurrence model for Ontario.pdf;C\:\\Users\\devan\\Zotero\\storage\\QE7FIA5Z\\Wotton and Martell - 2005 - A lightning fire occurrence model for Ontario.pdf},
  journal = {Canadian Journal of Forest Research},
  keywords = {Count,Ignition Probability},
  language = {en},
  number = {6}
}

@article{wottonLightningFireOccurrence2005a,
  title = {A Lightning Fire Occurrence Model for {{Ontario}}},
  author = {Wotton, B M and Martell, David L},
  year = {2005},
  month = jun,
  volume = {35},
  pages = {1389--1401},
  issn = {0045-5067},
  doi = {10.1139/x05-071},
  abstract = {Lightning strike, fire weather, and fire occurrence data were used to model (i) the probability that a lightning strike causes a sustainable ignition on the forest floor and (ii) the probability of..., Les donn\'ees sur la foudre, les conditions m\'et\'eorologiques et l'occurrence des feux ont \'et\'e utilis\'ees pour mod\'eliser (i) la probabilit\'e que la foudre r\'eussisse \`a allumer un feu dans la couverture mo...},
  file = {C\:\\Users\\devan\\Zotero\\storage\\974ED83E\\Wotton and Martell - 2005 - A lightning fire occurrence model for Ontario.pdf;C\:\\Users\\devan\\Zotero\\storage\\3TLBRPNS\\x05-071.html},
  journal = {Canadian Journal of Forest Research},
  number = {6}
}

@article{wottonLightningFirePrediction2012,
  title = {A Lightning Fire Prediction System.},
  author = {Wotton, B. M.},
  year = {2012},
  abstract = {The Canadian Forest Service promotes the sustainable development of Canada's forests and the competitiveness of the Canadian forest sector, Site menu for NRCan internet website.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\X4K6PRBS\\Wotton - 2012 - A lightning fire prediction system. Frontline Expr.pdf;C\:\\Users\\devan\\Zotero\\storage\\ZCP8T9D2\\publications.html},
  language = {English}
}

@article{wrightMixtureModelRounded2003,
  title = {A Mixture Model for Rounded Data},
  author = {Wright, David E. and Bray, Isabelle},
  year = {2003},
  month = mar,
  volume = {52},
  pages = {3--13},
  issn = {0039-0526, 1467-9884},
  doi = {10.1111/1467-9884.00338},
  abstract = {The paper focuses on the problem of data heaping that arises when measurements are recorded to varying degrees of precision. The work is motivated by an application in foetal medicine where measurements obtained from ultrasound images are rounded to varying numbers of decimal places causing heaping at integer values. We demonstrate the dangers of ignoring heaping before presenting a case-study of the ultrasound measurements. A mixture model, in which the different components represent different levels of rounding, is used for the heaping process. We illustrate a range of graphical posterior predictive checks to assess the fit of the model and we explore some extensions of the model. We adopt a Bayesian approach implemented by using the Gibbs sampler.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5US26YM6\\Wright and Bray - 2003 - A mixture model for rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\BA4RAXDD\\Wright and Bray - 2003 - A mixture model for rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\CY7PCK2D\\Wright and Bray - 2003 - A mixture model for rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\JVZAIAJQ\\Wright and Bray - 2003 - A mixture model for rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\P9WC4R87\\Wright and Bray - 2003 - A mixture model for rounded data.pdf;C\:\\Users\\devan\\Zotero\\storage\\RT9TLEKI\\Wright and Bray - 2003 - A mixture model for rounded data.pdf},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  language = {en},
  number = {1}
}

@article{wulfsohnJointModelSurvival1997,
  title = {A {{Joint Model}} for {{Survival}} and {{Longitudinal Data Measured}} with {{Error}}},
  author = {Wulfsohn, Michael S. and Tsiatis, Anastasios A.},
  year = {1997},
  month = mar,
  volume = {53},
  pages = {330},
  issn = {0006341X},
  doi = {10.2307/2533118},
  abstract = {The relationship between a longitudinal covariate and a failure time process can be assessed using the Cox proportional hazards regression model. We consider the problem of estimating the parameters in the Cox model when the longitudinal covariate is measured infrequently and with measurement error. We assume a repeated measures random effects model for the covariate process. Estimates of the parameters are obtained by maximizing the joint likelihood for the covariate process and the failure time process. This approach uses the available information optimally because we use both the covariate and survival data simultaneously. Parameters are estimated using the expectation-maximization algorithm. We argue that such a method is superior to naive methods where one maximizes the partial likelihood of the Cox model using the observed covariate values. It also improves on two-stage methods where, in the first stage, empirical Bayes estimates of the covariate process are computed and then used as time-dependent covariates in a second stage to find the parameters in the Cox model that maximize the partial likelihood.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\SD4QZUL2\\Wulfsohn and Tsiatis - 1997 - A Joint Model for Survival and Longitudinal Data M.pdf;C\:\\Users\\devan\\Zotero\\storage\\WTXR2VJZ\\Wulfsohn and Tsiatis - 1997 - A Joint Model for Survival and Longitudinal Data M.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{wulfsohnJointModelSurvival1997a,
  title = {A {{Joint Model}} for {{Survival}} and {{Longitudinal Data Measured}} with {{Error}}},
  author = {Wulfsohn, Michael S. and Tsiatis, Anastasios A.},
  year = {1997},
  month = mar,
  volume = {53},
  pages = {330},
  issn = {0006341X},
  doi = {10.2307/2533118},
  abstract = {The relationship between a longitudinal covariate and a failure time process can be assessed using the Cox proportional hazards regression model. We consider the problem of estimating the parameters in the Cox model when the longitudinal covariate is measured infrequently and with measurement error. We assume a repeated measures random effects model for the covariate process. Estimates of the parameters are obtained by maximizing the joint likelihood for the covariate process and the failure time process. This approach uses the available information optimally because we use both the covariate and survival data simultaneously. Parameters are estimated using the expectation-maximization algorithm. We argue that such a method is superior to naive methods where one maximizes the partial likelihood of the Cox model using the observed covariate values. It also improves on two-stage methods where, in the first stage, empirical Bayes estimates of the covariate process are computed and then used as time-dependent covariates in a second stage to find the parameters in the Cox model that maximize the partial likelihood.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CF7UBJIQ\\Wulfsohn and Tsiatis - 1997 - A Joint Model for Survival and Longitudinal Data M.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{xiStatisticalModelsKey2019,
  title = {Statistical {{Models}} of {{Key Components}} of {{Wildfire Risk}}},
  author = {Xi, Dexen D.Z. and Taylor, Stephen W. and Woolford, Douglas G. and Dean, C.B.},
  year = {2019},
  volume = {6},
  pages = {197--222},
  doi = {10.1146/annurev-statistics-031017-100450},
  abstract = {Fire danger systems have evolved from qualitative indices, to process-driven deterministic models of fire behavior and growth, to data-driven stochastic models of fire occurrence and simulation systems. However, there has often been little overlap or connectivity in these frameworks, and validation has not been common in deterministic models. Yet, marked increases in annual fire costs, losses, and fatality costs over the past decade draw attention to the need for better understanding of fire risk to support fire management decision making through the use of science-backed, data-driven tools. Contemporary risk modeling systems provide a useful integrative framework. This article discusses a variety of important contributions for modeling fire risk components over recent decades, certain key fire characteristics that have been overlooked, and areas of recent research that may enhance risk models.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LBNUH6A3\\Xi et al. - 2019 - Statistical Models of Key Components of Wildfire R.pdf},
  journal = {Annual Review of Statistics and Its Application},
  number = {1}
}

@article{xuEvaluationMultipleSurrogate2001,
  title = {The {{Evaluation}} of {{Multiple Surrogate Endpoints}}},
  author = {Xu, Jane and Zeger, Scott L.},
  year = {2001},
  volume = {57},
  pages = {81--87},
  abstract = {Surrogate endpoints are desirable because they typically result in smaller, faster efficacy studies compared with the ones using the clinical endpoints. Research on surrogate endpoints has received substantial attention lately, but most investigations have focused on the validity of using a single biomarker as a surrogate. Our paper studies whether the use of multiple markers can improve inferences about a treatment's effects on a clinical endpoint. We propose a joint model for a time to clinical event and for repeated measures over time on multiple biomarkers that are potential surrogates. This model extends the formulation of Xu and Zeger (2001, in press) and Fawcett and Thomas (1996, Statistics in Medicine 15, 1663-1685). We propose two complementary measures of the relative benefit of multiple surrogates as opposed to a single one. Markov chain Monte Carlo is implemented to estimate model parameters. The methodology is illustrated with an analysis of data from a schizophrenia clinical trial.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JGB82GXQ\\Xu and Zeger - 2001 - The Evaluation of Multiple Surrogate Endpoints.pdf;C\:\\Users\\devan\\Zotero\\storage\\T9XQM6BB\\Xu and Zeger - 2001 - The Evaluation of Multiple Surrogate Endpoints.pdf},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{xuJointAnalysisLongitudinal2001,
  title = {Joint {{Analysis}} of {{Longitudinal Data Comprising Repeated Measures}} and {{Times}} to {{Events}}},
  author = {Xu, Jane and Zeger, Scott L.},
  year = {2001},
  volume = {50},
  pages = {375--387},
  abstract = {In biomedical and public health research, both repeated measures of biomarkers Y as well as times Tto key clinical events are often collected for a subject. The scientific question is how the distribution of the responses [T, YIX] changes with covariates X [TIX] may be the focus of the estimation where Ycan be used as a surrogate for T. Alternatively, Tmay be the time to drop-out in a study in which [YIX] is the target for estimation. Also, the focus of a study might be on the effects of covariates X on both T and Y or on some underlying latent variable which is thought to be manifested in the observable outcomes. In this paper, we present a general model for the joint analysis of [T, YIX] and apply the model to estimate [TIX] and other related functionals by using the relevant information in both Tand Y We adopt a latent variable formulation like that of Fawcett and Thomas and use it to estimate several quantities of clinical relevance to determine the efficacy of a treatment in a clinical trial setting. We use a Markov chain Monte Carlo algorithm to estimate the model's parameters. We illustrate the methodology with an analysis of data from a clinical trial comparing risperidone with a placebo for the treatment of schizophrenia.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\IYN5X3N2\\Xu and Zeger - 2001 - Joint Analysis of Longitudinal Data Comprising Rep.pdf;C\:\\Users\\devan\\Zotero\\storage\\LGE6JUXV\\Xu and Zeger - 2001 - Joint Analysis of Longitudinal Data Comprising Rep.pdf},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  language = {en},
  number = {3}
}

@article{xuPointProcessModeling2011,
  title = {Point Process Modeling of Wildfire Hazard in {{Los Angeles County}}, {{California}}},
  author = {Xu, Haiyong and Schoenberg, Frederic Paik},
  year = {2011},
  month = jun,
  volume = {5},
  pages = {684--704},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS401},
  abstract = {The Burning Index (BI) produced daily by the United States govern- ment's National Fire Danger Rating System is commonly used in forecasting the hazard of wildfire activity in the United States. However, recent evalua- tions have shown the BI to be less effective at predicting wildfires in Los An- geles County, compared to simple point process models incorporating similar meteorological information. Here, we explore the forecasting power of a suite of more complex point process models that use seasonal wildfire trends, daily and lagged weather variables, and historical spatial burn patterns as covari- ates, and that interpolate the records from different weather stations. Results are compared with models using only the BI. The performance of each model is compared by Akaike Information Criterion (AIC), as well as by the power in predicting wildfires in the historical data set and residual analysis. We find that multiplicative models that directly use weather variables offer substantial improvement in fit compared to models using only the BI, and, in particular, models where a distinct spatial bandwidth parameter is estimated for each weather station appear to offer substantially improved fit.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\852QAAMR\\Xu and Schoenberg - 2011 - Point process modeling of wildfire hazard in Los A.pdf},
  journal = {The Annals of Applied Statistics},
  keywords = {Ignition Probability,Spatial},
  language = {en},
  number = {2A}
}

@article{xuPointProcessModeling2011a,
  title = {Point Process Modeling of Wildfire Hazard in {{Los Angeles County}}, {{California}}},
  author = {Xu, Haiyong and Schoenberg, Frederic Paik},
  year = {2011},
  month = jun,
  volume = {5},
  pages = {684--704},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS401},
  abstract = {The Burning Index (BI) produced daily by the United States government's National Fire Danger Rating System is commonly used in forecasting the hazard of wildfire activity in the United States. However, recent evaluations have shown the BI to be less effective at predicting wildfires in Los Angeles County, compared to simple point process models incorporating similar meteorological information. Here, we explore the forecasting power of a suite of more complex point process models that use seasonal wildfire trends, daily and lagged weather variables, and historical spatial burn patterns as covariates, and that interpolate the records from different weather stations. Results are compared with models using only the BI. The performance of each model is compared by Akaike Information Criterion (AIC), as well as by the power in predicting wildfires in the historical data set and residual analysis. We find that multiplicative models that directly use weather variables offer substantial improvement in fit compared to models using only the BI, and, in particular, models where a distinct spatial bandwidth parameter is estimated for each weather station appear to offer substantially improved fit.},
  archivePrefix = {arXiv},
  eprint = {1108.0754},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MU63338U\\Xu and Schoenberg - 2011 - Point process modeling of wildfire hazard in Los A.pdf},
  journal = {The Annals of Applied Statistics},
  keywords = {Statistics - Applications},
  language = {en},
  number = {2A}
}

@article{yangMolecularPhylogeneticsPrinciples2012,
  title = {Molecular Phylogenetics: Principles and Practice},
  shorttitle = {Molecular Phylogenetics},
  author = {Yang, Ziheng and Rannala, Bruce},
  year = {2012},
  month = may,
  volume = {13},
  pages = {303--314},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg3186},
  abstract = {Phylogenies are important for addressing various biological questions such as relationships among species or genes, the origin and spread of viral infection and the demographic changes and migration patterns of species. The advancement of sequencing technologies has taken phylogenetic analysis to a new height. Phylogenies have permeated nearly every branch of biology, and the plethora of phylogenetic methods and software packages that are now available may seem daunting to an experimental biologist. Here, we review the major methods of phylogenetic analysis, including parsimony, distance, likelihood and Bayesian methods. We discuss their strengths and weaknesses and provide guidance for their use.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\WMNXB6PX\\Yang and Rannala - 2012 - Molecular phylogenetics principles and practice.pdf},
  journal = {Nature Reviews Genetics},
  language = {en},
  number = {5}
}

@article{yangSpatialPatternsModern,
  title = {Spatial {{Patterns}} of {{Modern Period Human}}-{{Caused Fire Occurrence}} in the {{Missouri Ozark Highlands}}},
  author = {Yang, Jian and He, Hong S and Shifley, Stephen R and Gustafson, Eric J},
  pages = {15},
  abstract = {The spatial pattern of forest fire locations is important in the study of the dynamics of fire disturbance. In this article we used a spatial point process modeling approach to quantitatively study the effects of land cover, topography, roads, municipalities, ownership, and population density on fire occurrence reported between 1970 and 2002 in the Missouri Ozark Highland forests, where more than 90\% of fires are humancaused. We used the AIC (Akaike information criterion) method to select an appropriate inhomogeneous Poisson process model to best fit to the data. The fitted model was diagnosed using residual analysis as well. Our results showed that fire locations were spatially clustered, and high fire occurrence probability was found in areas that (1) were public land, (2) within 6 km to 17 km of municipalities, and (3) Ͻ500 m from roads where forests are accessible to humans. In addition, fire occurrence probability was higher in pine-oak forests on moderate (Ͻ25 degree) slopes and xeric aspects and at higher (Ͼ270 m) elevations, reflecting the effects of natural factors on fire occurrence. The results serve as a provisional hypothesis for expanding fire risk estimation to surrounding areas. The spatial scale of analysis (approximately 1 ha) provides new information to guide planning and risk reduction efforts. FOR. SCI. 53(1):1\textendash 15.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H9QZ8CWG\\Yang et al. - Spatial Patterns of Modern Period Human-Caused Fir.pdf},
  language = {en}
}

@article{yuanPointProcessModels2016,
  title = {Point Process Models for Spatio-Temporal Distance Sampling Data from a Large-Scale Survey of Blue Whales},
  author = {Yuan, Y. and Bachl, F. E. and Lindgren, F. and Brochers, D. L. and Illian, J. B. and Buckland, S. T. and Rue, H. and Gerrodette, T.},
  year = {2016},
  month = apr,
  abstract = {Distance sampling is a widely used method for estimating wildlife population abundance. The fact that conventional distance sampling methods are partly design-based constrains the spatial resolution at which animal density can be estimated using these methods. Estimates are usually obtained at survey stratum level. For an endangered species such as the blue whale, it is desirable to estimate density and abundance at a finer spatial scale than stratum. Temporal variation in the spatial structure is also important. We formulate the process generating distance sampling data as a thinned spatial point process and propose model-based inference using a spatial log-Gaussian Cox process. The method adopts a flexible stochastic partial differential equation (SPDE) approach to model spatial structure in density that is not accounted for by explanatory variables, and integrated nested Laplace approximation (INLA) for Bayesian inference. It allows simultaneous fitting of detection and density models and permits prediction of density at an arbitrarily fine scale. We estimate blue whale density in the Eastern Tropical Pacific Ocean from thirteen shipboard surveys conducted over 22 years. We find that higher blue whale density is associated with colder sea surface temperatures in space, and although there is some positive association between density and mean annual temperature, our estimates are consitent with no trend in density across years. Our analysis also indicates that there is substantial spatially structured variation in density that is not explained by available covariates.},
  archivePrefix = {arXiv},
  eprint = {1604.06013},
  eprinttype = {arxiv},
  file = {C\:\\Users\\devan\\Zotero\\storage\\GKFXRFW8\\Yuan et al. - 2016 - Point process models for spatio-temporal distance .pdf},
  journal = {arXiv:1604.06013 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{yuOutlierDetectionFunctional2012,
  title = {Outlier {{Detection}} in {{Functional Observations With Applications}} to {{Profile Monitoring}}},
  author = {Yu, Guan and Zou, Changliang and Wang, Zhaojun},
  year = {2012},
  month = aug,
  volume = {54},
  pages = {308--318},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2012.694781},
  file = {C\:\\Users\\devan\\Zotero\\storage\\E9B9WGAT\\Yu et al. - 2012 - Outlier Detection in Functional Observations With .pdf;C\:\\Users\\devan\\Zotero\\storage\\NRKW3QX5\\Yu et al. - 2012 - Outlier Detection in Functional Observations With .pdf},
  journal = {Technometrics},
  language = {en},
  number = {3}
}

@article{zegerGeneralizedLinearModels1991,
  title = {Generalized {{Linear Models}} with {{Random Effects}}; a {{Gibbs Sampling Approach}}},
  author = {Zeger, Scott L and Karim, M. Rezaul},
  year = {1991},
  month = mar,
  volume = {86},
  pages = {79--86},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1991.10475006},
  file = {C\:\\Users\\devan\\Zotero\\storage\\MSM88999\\Zeger and Karim - 1991 - Generalized Linear Models with Random Effects\; a G.pdf;C\:\\Users\\devan\\Zotero\\storage\\XNXCVP6Z\\Zeger and Karim - 1991 - Generalized Linear Models with Random Effects\; a G.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {413}
}

@article{zengSimultaneousModellingSurvival2005,
  title = {Simultaneous {{Modelling}} of {{Survival}} and {{Longitudinal Data}} with an {{Application}} to {{Repeated Quality}} of {{Life Measures}}},
  author = {{zeng}, Donglin and {cai}, Jianwen},
  year = {2005},
  month = jun,
  volume = {11},
  pages = {151--174},
  issn = {1380-7870, 1572-9249},
  doi = {10.1007/s10985-004-0381-0},
  abstract = {In biomedical studies, interest often focuses on the relationship between patient's characteristics or some risk factors and both quality of life and survival time of subjects under study. In this paper, we propose a simultaneous modelling of both quality of life and survival time using the observed covariates. Moreover, random effects are introduced into the simultaneous models to account for dependence between quality of life and survival time due to unobserved factors. EM algorithms are used to derive the point estimates for the parameters in the proposed model and profile likelihood function is used to estimate their variances. The asymptotic properties are established for our proposed estimators. Finally, simulation studies are conducted to examine the finite-sample properties of the proposed estimators and a liver transplantation data set is analyzed to illustrate our approaches.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\36DCAVVQ\\zeng and cai - 2005 - Simultaneous Modelling of Survival and Longitudina.pdf;C\:\\Users\\devan\\Zotero\\storage\\3L377NMQ\\zeng and cai - 2005 - Simultaneous Modelling of Survival and Longitudina.pdf},
  journal = {Lifetime Data Analysis},
  language = {en},
  number = {2}
}

@article{zhang2009detecting,
  title = {Detecting Outliers in Complex Profiles Using a {$X^2$} Control Chart Method},
  author = {Zhang, Hang and Albin, Susan},
  year = {2009},
  volume = {41},
  pages = {335--345},
  publisher = {{Taylor {{\&}} Francis}},
  journal = {IIE Transactions},
  number = {4}
}

@article{zhangBayesianModelAssessment2017,
  title = {Bayesian {{Model Assessment}} in {{Joint Modeling}} of {{Longitudinal}} and {{Survival Data With Applications}} to {{Cancer Clinical Trials}}},
  author = {Zhang, Danjie and Chen, Ming-Hui and Ibrahim, Joseph G. and Boye, Mark E. and Shen, Wei},
  year = {2017},
  month = jan,
  volume = {26},
  pages = {121--133},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2015.1117472},
  abstract = {Joint models for longitudinal and survival data are routinely used in clinical trials or other studies to assess a treatment effect while accounting for longitudinal measures such as patient-reported outcomes (PROs). In the Bayesian framework, the deviance information criterion (DIC) and the logarithm of the pseudo marginal likelihood (LPML) are two well-known Bayesian criteria for comparing joint models. However, these criteria do not provide separate assessments of each component of the joint model. In this paper, we develop a novel decomposition of DIC and LPML to assess the fit of the longitudinal and survival components of the joint model, separately. Based on this decomposition, we then propose new Bayesian model assessment criteria, namely, {$\Delta$}DIC and {$\Delta$}LPML, to determine the importance and contribution of the longitudinal (survival) data to the model fit of the survival (longitudinal) data. Moreover, we develop an efficient Monte Carlo method for computing the Conditional Predictive Ordinate (CPO) statistics in the joint modeling setting. A simulation study is conducted to examine the empirical performance of the proposed criteria and the proposed methodology is further applied to a case study in mesothelioma.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BHE2WSSK\\Zhang et al. - 2017 - Bayesian Model Assessment in Joint Modeling of Lon.pdf;C\:\\Users\\devan\\Zotero\\storage\\TJJV82BW\\Zhang et al. - 2017 - Bayesian Model Assessment in Joint Modeling of Lon.pdf;C\:\\Users\\devan\\Zotero\\storage\\VAG4NWS2\\Zhang et al. - 2017 - Bayesian Model Assessment in Joint Modeling of Lon.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {1}
}

@article{zhangControlChartsMonitoring2014,
  title = {Control {{Charts}} for {{Monitoring Linear Profiles}} with {{Within}}-{{Profile Correlation Using Gaussian Process Models}}: {{Monitoring Linear Profiles}} with {{Within}}-{{Profile Correlation}}},
  shorttitle = {Control {{Charts}} for {{Monitoring Linear Profiles}} with {{Within}}-{{Profile Correlation Using Gaussian Process Models}}},
  author = {Zhang, Yang and He, Zhen and Zhang, Chi and Woodall, William H.},
  year = {2014},
  month = jun,
  volume = {30},
  pages = {487--501},
  issn = {07488017},
  doi = {10.1002/qre.1502},
  file = {C\:\\Users\\devan\\Zotero\\storage\\7GHJJ5IQ\\Zhang et al. - 2014 - Control Charts for Monitoring Linear Profiles with.pdf;C\:\\Users\\devan\\Zotero\\storage\\AFTUNAH4\\Zhang et al. - 2014 - Control Charts for Monitoring Linear Profiles with.pdf},
  journal = {Quality and Reliability Engineering International},
  language = {en},
  number = {4}
}

@article{zhangDescriptionDefinitionResearch,
  title = {Description and {{Definition Research Problems Testing Independence A Few Famous Models First}}-{{Order}} and {{Second}}-{{Order Analyses Asymptotic Frameworks}}},
  author = {Zhang, Tonglin},
  pages = {25},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KJHX7PXD\\Zhang - Description and Deﬁnition Research Problems Testin.pdf},
  language = {en}
}

@article{zhangDetectingOutliersComplex2009,
  title = {Detecting Outliers in Complex Profiles Using a {$\chi$} {\textsuperscript{2}} Control Chart Method},
  author = {Zhang, Hang and Albin, Susan},
  year = {2009},
  month = feb,
  volume = {41},
  pages = {335--345},
  issn = {0740-817X, 1545-8830},
  doi = {10.1080/07408170802323000},
  file = {C\:\\Users\\devan\\Zotero\\storage\\77S3GW6B\\Zhang and Albin - 2009 - Detecting outliers in complex profiles using a χ .pdf;C\:\\Users\\devan\\Zotero\\storage\\SZFTCHVB\\Zhang and Albin - 2009 - Detecting outliers in complex profiles using a χ .pdf},
  journal = {IIE Transactions},
  language = {en},
  number = {4}
}

@article{zhangIndependenceSeparabilityPoints2017,
  title = {On {{Independence}} and {{Separability}} between {{Points}} and {{Marks}} of {{Marked Point Processes}}},
  author = {Zhang, Tonglin},
  year = {2017},
  issn = {10170405},
  doi = {10.5705/ss.202015.0232},
  abstract = {An important problem in statistical methods for marked point processes (MPPs) is to evaluate the relationship between points and marks, which can be developed under either the concept of independence or the concept of separability. Although both have been used, the connection between these two concepts is still unclear in the literature. The present article provides a way to evaluate such a connection, concluding that the concept of independence and the concept of separability are equivalent if the Kolmogorov consistency condition is satisfied, but not otherwise. We also provide a testing method to assess first-order independence between points and marks, where first-order independence is concluded if the test statistic is insignificant and first-order dependence is concluded if the test statistic is significant. The performance of the testing method is evaluated under simulation and case studies.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\3HHG4IDJ\\Zhang - 2017 - On Independence and Separability between Points an.pdf},
  journal = {Statistica Sinica},
  language = {en}
}

@article{zhangKolmogorovSmirnovTypeTest2014,
  title = {A {{Kolmogorov}}-{{Smirnov}} Type Test for Independence between Marks and Points of Marked Point Processes},
  author = {Zhang, Tonglin},
  year = {2014},
  volume = {8},
  pages = {2557--2584},
  issn = {1935-7524},
  doi = {10.1214/14-EJS961},
  abstract = {Marked point processes are commonly used stochastic models for representing a finite number of natural hazard events located in space and time, because these kinds of data often associate measurements (i.e. marks) with locations (i.e. points) of events. Methods of marked point processes when marks and points are interacting have been proposed, but it is still necessary to know whether the interaction must be considered. This article presents a Kolmogorov-Smirnov type method to test the independence between points and marks of marked point processes. The asymptotic distribution of the test statistic under a few weak regularity conditions is derived. According to the asymptotic result, a specific way to construct the test statistic is recommended as its null distribution can be approximated by the absolute maximum of the two-dimensional standard Brownian pillow. The simulation results and real data analyses demonstrated that the proposed method is powerful in detecting weak dependence between marks and points and performs well with a moderate sample size.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\JU35C6ZN\\Zhang - 2014 - A Kolmogorov-Smirnov type test for independence be.pdf},
  journal = {Electronic Journal of Statistics},
  language = {en},
  number = {2}
}

@article{zhangSecondOrderAnalysisSpatial,
  title = {Second-{{Order Analysis}} of {{Spatial Point Process}}},
  author = {Zhang, Tonglin},
  pages = {24},
  file = {C\:\\Users\\devan\\Zotero\\storage\\5CZYZMG4\\Zhang - Second-Order Analysis of Spatial Point Process.pdf},
  language = {en}
}

@article{zhangTestingProportionalityFirstorder2017,
  title = {Testing Proportionality between the First-Order Intensity Functions of Spatial Point Processes},
  author = {Zhang, Tonglin and Zhuang, Run},
  year = {2017},
  month = mar,
  volume = {155},
  pages = {72--82},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2016.11.013},
  abstract = {This article proposes a Kolmogorov\textendash Smirnov type test for proportionality between the first-order intensity functions of two independent spatial point processes. After appropriate scaling, the test statistic is constructed by maximizing the absolute difference between their point densities over a {$\pi$} -system. By treating non-stationary point processes as transformed from stationary point processes such that all questions of asymptotics related to the tightness can be answered, the article shows that the resulting test statistic converges weakly to the absolute maximum of a pinned Brownian sheet. This may be reduced to the standard Brownian bridge in a special case. A simulation study shows that the type I error probability of the test is close to the significance level and the power function increases to 1 as the magnitude of non-proportionality increases. In applications to two typical natural hazard data, the article concludes that the first-order intensity functions might be proportional in one case and not in the other.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\LZVXURAQ\\Zhang and Zhuang - 2017 - Testing proportionality between the first-order in.pdf},
  journal = {Journal of Multivariate Analysis},
  language = {en}
}

@article{zhengEDGECORRECTIONSPATIAL,
  title = {{{EDGE}}\textendash{{CORRECTION FOR SPATIAL KERNEL SMOOTHING METHODS}}?{{WHEN IS IT NECESSARY}}?},
  author = {Zheng, P and Durr, P A and Diggle, P J},
  pages = {3},
  abstract = {A limitation to the practical implementation of some kernel smoothing methods for spatial data is the need for edge correction. This applies particularly to kernel density estimation. Here we demonstrate by simulation the extent of the bias introduced when edge correction is not applied for realisations of both homogeneous and inhomogeneous Poisson processes. This shows the overwhelming importance of edge-correction. We also argue that edge correction is usually not necessary for kernel regression.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\EPG9ZBFU\\Zheng et al. - EDGE–CORRECTION FOR SPATIAL KERNEL SMOOTHING METHO.pdf},
  language = {en}
}

@article{zhigangzhangIntervalCensoring2010,
  title = {Interval Censoring},
  author = {{Zhigang Zhang} and {Jianguo Sun}},
  year = {2010},
  month = feb,
  volume = {19},
  pages = {53--70},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280209105023},
  file = {C\:\\Users\\devan\\Zotero\\storage\\BRRFL2HV\\Zhigang Zhang and Jianguo Sun - 2010 - Interval censoring.pdf;C\:\\Users\\devan\\Zotero\\storage\\FGQSDJS7\\Zhigang Zhang and Jianguo Sun - 2010 - Interval censoring.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {1}
}

@article{zhouSpatioTemporalPointProcess2015,
  title = {A {{Spatio}}-{{Temporal Point Process Model}} for {{Ambulance Demand}}},
  author = {Zhou, Zhengyi and Matteson, David S. and Woodard, Dawn B. and Henderson, Shane G. and Micheas, Athanasios C.},
  year = {2015},
  month = jan,
  volume = {110},
  pages = {6--15},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2014.941466},
  file = {C\:\\Users\\devan\\Zotero\\storage\\H2PEGWHJ\\Zhou et al. - 2015 - A Spatio-Temporal Point Process Model for Ambulanc.pdf;C\:\\Users\\devan\\Zotero\\storage\\M48S282K\\Zhou et al. - 2015 - A Spatio-Temporal Point Process Model for Ambulanc.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {509}
}

@article{zhuangStochasticDeclusteringSpaceTime2002,
  title = {Stochastic {{Declustering}} of {{Space}}-{{Time Earthquake Occurrences}}},
  author = {Zhuang, Jiancang and Ogata, Yosihiko and {Vere-Jones}, David},
  year = {2002},
  month = jun,
  volume = {97},
  pages = {369--380},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214502760046925},
  file = {C\:\\Users\\devan\\Zotero\\storage\\VK6IUA85\\Zhuang et al. - 2002 - Stochastic Declustering of Space-Time Earthquake O.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {458}
}

@article{zimmermanBivariateCramervonMises1993,
  title = {A {{Bivariate Cramer}}-von {{Mises Type}} of {{Test}} for {{Spatial Randomness}}},
  author = {Zimmerman, Dale L.},
  year = {1993},
  volume = {42},
  pages = {43},
  issn = {00359254},
  doi = {10.2307/2347408},
  abstract = {A test for the randomness of a mapped spatial pattern of events in a rectangle D in R2 is examined that is based on the 'distance' between the bivariate empirical distribution function of the events' Cartesian co-ordinates and the uniform distribution function. The distance between distribution functions is measured by a modification of the bivariate Cram6r-von Mises statistic that is invariant to the corner of D identified as the origin. This invariance property, which is essential, is lacking in the standard bivariate Cram6r-von Mises statistic. Several real examples and results from simulation indicate that the test proposed is superior to existing tests for detecting heterogeneous alternatives to spatial randomness but inferior for detecting regular or aggregated alternatives. Some additional features of the test are its computational simplicity, the non-necessity of adjusting for edge effects and the ease with which it can be extended to test for certain types of heterogeneity.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\CBNYWAX2\\Zimmerman - 1993 - A Bivariate Cramer-von Mises Type of Test for Spat.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {1}
}

@article{zinnSTATISTICALAPPROACHACCOUNT2014,
  title = {A {{STATISTICAL APPROACH TO ACCOUNT FOR HEAPING PATTERNS}}: {{AN APPLICATION TO}}},
  author = {Zinn, Sabine and W{\"u}rbach, Ariane},
  year = {2014},
  pages = {26},
  abstract = {Self-reported income information particularly su ers from misreporting due to the sensitivity of the issue and the error-proneness of the memory. This leads to an intentional coarsening of the data, which is called heaping or rounding. If it does not occur completely at random which is usually not the case heaping and rounding has detrimental e ects on the results of statistical analysis. For instance, it has an e ect on empirical statistics (e.g., percentiles) as well as on inferences from multivariate analyses. Conventional statistical methods do not consider this kind of reporting bias, and thus might produce invalid inference. In this paper, we describe a novel statistical modeling approach that allows us to deal with self-reported heaped income data in an adequate way. We suggest modeling heaping mechanisms and the true underlying model in combination. This way we are able to simultaneously estimate the parameters of the true distribution and to determine the heaping pattern present in the data. To describe the true net income distribution, we use the 3-parametric Dagum distribution. Heaping points are identi ed from the data by applying a heuristic procedure comparing a hypothetical income distribution and the empirical one. To determine heaping behavior, we employ two distinct models: On the one hand, we assume piecewise constant heaping probabilities, and on the other hand, heaping probabilities are considered to increase steadily with proximity to a heaping point. We validate our novel approach by a range of simulation studies. To illustrate the capacity of the novel approach, we conduct a case study using income data from the adult cohort of the German National Educational Panel Study.},
  file = {C\:\\Users\\devan\\Zotero\\storage\\KQJGKTN9\\Zinn and Würbach - 2014 - A STATISTICAL APPROACH TO ACCOUNT FOR HEAPING PATT.pdf},
  language = {en},
  number = {40}
}

@misc{ZoteroStaticScholarsArchive,
  title = {Zotero {{Static}} // {{ScholarsArchive}}@{{OSU}}},
  file = {C\:\\Users\\devan\\Zotero\\storage\\4IHREQB3\\zotero.html},
  howpublished = {https://ir.library.oregonstate.edu/zotero?locale=en}
}



@article{alexandridisWildlandFireSpread2011,
  title = {Wildland Fire Spread Modelling Using Cellular Automata: Evolution in Large-Scale Spatially Heterogeneous Environments under Fire Suppression Tactics},
  shorttitle = {Wildland Fire Spread Modelling Using Cellular Automata},
  author = {Alexandridis, A. and Russo, L. and Vakalis, D. and Bafas, G. V. and Siettos, C. I.},
  year = {2011},
  journal = {International Journal of Wildland Fire},
  volume = {20},
  number = {5},
  pages = {633},
  issn = {1049-8001},
  doi = {10.1071/WF09119},
  abstract = {We show how microscopic modelling techniques such as Cellular Automata linked with detailed geographical information systems (GIS) and meteorological data can be used to efficiently predict the evolution of fire fronts on mountainous and heterogeneous wild forest landscapes. In particular, we present a lattice-based dynamic model that includes various factors, ranging from landscape and earth statistics, attributes of vegetation and wind field data to the humidity of the fuel and the spotting transfer mechanism. We also attempt to model specific fire suppression tactics based on air tanker attacks utilising technical specifications as well as operational capabilities of the aircrafts. We use the detailed model to approximate the dynamics of a large-scale fire that broke out in a region on the west flank of the Greek National Park of Parnitha Mountain in June of 2007. The comparison between the simulation and the actual results showed that the proposed model predicts the fire-spread characteristics in an adequate manner. Finally, we discuss how such a detailed model can be exploited in order to design and develop, in a systematic way, fire risk management policies.},
  language = {en},
  file = {/home/devan/Zotero/storage/DU69L6ZV/Alexandridis et al. - 2011 - Wildland fire spread modelling using cellular auto.pdf}
}

@article{almeidaStochasticCellularAutomata2011,
  title = {Stochastic Cellular Automata Model for Wildland Fire Spread Dynamics},
  author = {Almeida, Rodolfo Maduro and Macau, Elbert E N},
  year = {2011},
  month = mar,
  journal = {Journal of Physics: Conference Series},
  volume = {285},
  pages = {012038},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/285/1/012038},
  abstract = {A stochastic cellular automata model for wildland fire spread under flat terrain and no-wind conditions is proposed and its dynamics is characterized and analyzed. One of three possible states characterizes each cell: vegetation cell, burning cell and burnt cell. The dynamics of fire spread is modeled as a stochastic event with an effective fire spread probability S which is a function of three probabilities that characterize: the proportion of vegetation cells across the lattice, the probability of a burning cell becomes burnt, and the probability of the fire spread from a burning cell to a neighboring vegetation cell. A set of simulation experiments is performed to analyze the effects of different values of the three probabilities in the fire pattern. Monte-Carlo simulations indicate that there is a critical line in the model parameter space that separates the set of parameters which a fire can propagate from those for which it cannot propagate. Finally, the relevance of the model is discussed under the light of computational experiments that illustrate the capability of the model catches both the dynamical and static qualitative properties of fire propagation.},
  language = {en},
  file = {/home/devan/Zotero/storage/B6HFR9UU/Almeida and Macau - 2011 - Stochastic cellular automata model for wildland fi.pdf}
}

@inproceedings{couceStatisticalParameterEstimation2010,
  title = {Statistical Parameter Estimation for a Cellular Automata Wildfire Model Based on Satellite Observations},
  booktitle = {{{FOREST FIRES}} 2010},
  author = {Couce, E. and Knorr, W.},
  year = {2010},
  month = may,
  pages = {47--55},
  address = {{Kos, Greece}},
  doi = {10.2495/FIVA100051},
  abstract = {The importance of understanding the impact of wildfires on natural ecosystems has given rise to the development of realistic computer models for the simulation of wildfires. Stochastic models based on simplified equations and local interactions, such as Cellular Automata (CA) models, are particularly popular as an alternative to more computationally demanding deterministic models. However, the challenges associated with observing wildfires under natural conditions, and the highly non-linear nature of fire spread makes it extremely difficult to parameterize them. In this work we present a method for adjusting the behaviour of one such CA model from the statistical analysis of satellite data of more than 750,000 African wildfires detected in 2003. Statistical metrics are developed to characterize agreement between model and satellite observations. The average probability of fire transmission amongst cells and the spatial scale of the model are adjusted so that maximum agreement is found between model output and the observed extension and statistical distribution of the real fires. While the results obtained are only valid for the particular CA model used and within the geographical limits of the region studied, we believe the process could be adapted to fine-tune and validate other CA models in regions where enough fire observations are available.},
  language = {en},
  file = {/home/devan/Zotero/storage/U7XIEVTH/Couce and Knorr - 2010 - Statistical parameter estimation for a cellular au.pdf}
}

@article{jiangModellingWildlandurbanInterface2021,
  title = {Modelling of Wildland-Urban Interface Fire Spread with the Heterogeneous Cellular Automata Model},
  author = {Jiang, Wenyu and Wang, Fei and Fang, Linghang and Zheng, Xiaocui and Qiao, Xiaohui and Li, Zhanghua and Meng, Qingxiang},
  year = {2021},
  month = jan,
  journal = {Environmental Modelling \& Software},
  volume = {135},
  pages = {104895},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2020.104895},
  abstract = {Fire safety in urban areas is gaining more attention along with the rapid development of urbanization and the increasing complexity of urban structures. Many fire spread models have been developed to support emergency decision making. However, most of them only investigate on single scenarios (forest fire or house fire) and usually have difficulty in balancing model accuracy and timeliness, which makes it hard to apply these models to large-scale heterogeneous scenarios. In this paper, we developed a fire spread model based on the heterogeneous Cellular Automata model for large-scale complex wildland-urban interface (WUI) areas. The model flexibly integrates a forest fire model and a house fire model based on thermal principles and empirical statistics. A software platform is developed based on the fire spread model correspondingly to validate its performance. Taking a real WUI fire occurred in California as a case study, we conducted comparative simulation experiments based on our software platform and widely used forest fire simulation software FlamMap6. The experimental results show that our model can simulate fire spread process with high efficiency, strong timeliness, and competitive accuracy, providing support on emergency decision-making of large-scale complex fire scenarios.},
  language = {en},
  keywords = {Emergency decision making,Fire spread model,Heterogeneous cellular automata model,WUI},
  file = {/home/devan/Zotero/storage/XJRKDLN2/Jiang et al_2021_Modelling of wildland-urban interface fire spread with the heterogeneous.pdf;/home/devan/Zotero/storage/BWS6GHQI/S136481522030952X.html}
}

@article{luciarussoDetectingWeakPoints2014,
  title = {Detecting Weak Points of Wildland Fire Spread: A Cellular Atomata Model Risk Assesemnt Simulation Approach},
  shorttitle = {Detecting Weak Points of Wildland Fire Spread},
  author = {{Lucia Russo} and {Paola Russo} and {Dimitris Vakalis} and {Constantinos Siettos}},
  year = {2014},
  journal = {Chemical Engineering Transactions},
  volume = {36},
  pages = {253--258},
  doi = {10.3303/CET1436043},
  language = {en},
  file = {/home/devan/Zotero/storage/78ADJFIP/Lucia Russo et al. - 2014 - Detecting weak points of wildland fire spread a c.pdf}
}

@article{quartieriCellularAutomataModel,
  title = {A {{Cellular Automata Model}} for {{Fire Spreading Prediction}}},
  author = {Quartieri, Joseph and Mastorakis, Nikos E and Iannone, Gerardo and Guarnaccia, Claudio},
  pages = {7},
  abstract = {The modeling of fire spread in a flammable area is an interesting and challenging issue, widely studied in literature. Some of the models used for fire front evolution prediction are based on Cellular Automata (CA) approach, and they have shown a good agreement with other models and with experimental data. In this paper, the authors propose a new approach to this kind of models, introducing an ``ignition probability'' in the transition rules. This probability depends on several parameters, such as number of burning neighbors, wind direction and speed, land slope, fuel typology, etc.. As a starting approach, the number of neighbors that already burn seems to be the more interesting parameter to be considered in the ignition probability. Thus, the evolution of each single cell is governed by the presence of burning cells in its neighbourhood and, according to reality, the simulated fire spread does not present a regular shape of the fire front.},
  language = {en},
  file = {/home/devan/Zotero/storage/PT8R3A8N/Quartieri et al. - A Cellular Automata Model for Fire Spreading Predi.pdf}
}

@article{sullivanReviewWildlandFire2009,
  title = {A Review of Wildland Fire Spread Modelling, 1990-Present 3: {{Mathematical}} Analogues and Simulation Models},
  shorttitle = {A Review of Wildland Fire Spread Modelling, 1990-Present 3},
  author = {Sullivan, A. L.},
  year = {2009},
  journal = {International Journal of Wildland Fire},
  volume = {18},
  number = {4},
  eprint = {0706.4130},
  eprinttype = {arxiv},
  pages = {387},
  issn = {1049-8001},
  doi = {10.1071/WF06144},
  abstract = {In recent years, advances in computational power and spatial data analysis (GIS, remote sensing, etc) have led to an increase in attempts to model the spread and behvaiour of wildland fires across the landscape. This series of review papers endeavours to critically and comprehensively review all types of surface fire spread models developed since 1990. This paper reviews models of a simulation or mathematical analogue nature. Most simulation models are implementations of existing empirical or quasi-empirical models and their primary function is to convert these generally one dimensional models to two dimensions and then propagate a fire perimeter across a modelled landscape. Mathematical analogue models are those that are based on some mathematical conceit (rather than a physical representation of fire spread) that coincidentally simulates the spread of fire. Other papers in the series review models of an physical or quasi-physical nature and empirical or quasiempirical nature. Many models are extensions or refinements of models developed before 1990. Where this is the case, these models are also discussed but much less comprehensively.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Physics - Atmospheric and Oceanic Physics,Physics - Geophysics},
  file = {/home/devan/Zotero/storage/MIZ7XMIN/Sullivan - 2009 - A review of wildland fire spread modelling, 1990-p.pdf}
}

@article{sullivanWildlandSurfaceFire2009,
  title = {Wildland Surface Fire Spread Modelling, 1990\textendash 2007. 3: {{Simulation}} and Mathematical Analogue Models},
  shorttitle = {Wildland Surface Fire Spread Modelling, 1990\textendash 2007. 3},
  author = {Sullivan, Andrew L. and Sullivan, Andrew L.},
  year = {2009},
  month = jun,
  journal = {International Journal of Wildland Fire},
  volume = {18},
  number = {4},
  pages = {387--403},
  publisher = {{CSIRO PUBLISHING}},
  issn = {1448-5516, 1448-5516},
  doi = {10.1071/WF06144},
  abstract = {In recent years, advances in computational power have led to an increase in attempts to model the behaviour of wildland fires and to simulate their spread across landscape. The present series of articles endeavours to comprehensively survey and pr\'ecis all types of surface fire spread models developed during the period 1990\textendash 2007. The present paper surveys models of a simulation or mathematical analogue nature. Most simulation models are implementations of existing empirical or quasi-empirical models and their primary function is to convert these generally one-dimensional models to two dimensions and then simulate the propagation of a fire perimeter across a modelled landscape. Mathematical analogue models are those that are based on some mathematical concept (rather than a physical representation of fire spread) that coincidentally represents the spread of fire. Other papers in the series survey models of a physical or quasi-physical nature, and empirical or quasi-empirical nature. Many models are extensions or refinements of models developed before 1990. Where this is the case, these models are also discussed but much less comprehensively.},
  language = {en},
  file = {/home/devan/Zotero/storage/5XWU5XID/wf06144.html}
}

@article{taylorWildfirePredictionInform2013,
  title = {Wildfire {{Prediction}} to {{Inform Fire Management}}: {{Statistical Science Challenges}}},
  shorttitle = {Wildfire {{Prediction}} to {{Inform Fire Management}}},
  author = {Taylor, S. W. and Woolford, Douglas G. and Dean, C. B. and Martell, David L.},
  year = {2013},
  month = nov,
  journal = {Statistical Science},
  volume = {28},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/13-STS451},
  abstract = {Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales.},
  language = {en},
  file = {/home/devan/Zotero/storage/9HGIHFFI/Taylor et al. - 2013 - Wildfire Prediction to Inform Fire Management Sta.pdf}
}

@book{tymstraDevelopmentStructurePrometheus2010,
  title = {Development and Structure of {{Prometheus}}: The {{Canadian Wildland Fire Growth Simulation Model}}.},
  shorttitle = {Development and Structure of {{Prometheus}}},
  author = {Tymstra, C. and Bryce, R. W. and Wotton, B. M. and Taylor, S. W. and Armitage, O. B.},
  year = {2010},
  volume = {417},
  issn = {0831-8247},
  isbn = {978-1-100-14674-4},
  language = {English},
  file = {/home/devan/Zotero/storage/YLQPY2ZQ/publications.html}
}

@article{wolframCellularAutomataModels1984,
  title = {Cellular Automata as Models of Complexity},
  author = {Wolfram, Stephen},
  year = {1984},
  month = oct,
  journal = {Nature},
  volume = {311},
  number = {5985},
  pages = {419--424},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/311419a0},
  language = {en},
  file = {/home/devan/Zotero/storage/VQQXM6DF/Wolfram - 1984 - Cellular automata as models of complexity.pdf}
}


